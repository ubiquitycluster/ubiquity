{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ubiquity","text":"<p>Welcome to the Ubiquity documentation. Ubiquity is a cloud-native platform for high-performance computing and data science workloads.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide - Start with our sandbox environment</li> <li>Architecture Overview - Learn about Ubiquity's design</li> <li>User Guide - Get started as a user</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>GitHub Repository</li> <li>Contributing Guide</li> <li>License</li> </ul>"},{"location":"#features","title":"Features","text":"<p>Ubiquity provides:</p> <ul> <li>Kubernetes-native: Built on modern cloud-native technologies</li> <li>Multi-cloud: Deploy on AWS, Azure, GCP, OpenStack, and more</li> <li>HPC ready: Optimized for high-performance computing workloads</li> <li>Data science: Integrated tools for data science and machine learning</li> <li>GitOps: Automated deployment and configuration management</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001","title":"v0.0.1","text":"<p>Notable changes:</p> <ul> <li>build: run post install scripts by default</li> <li>build: set <code>KUBECONFIG</code> from global Makefile</li> <li>feat(external-dns)!: add cluster name as owner ID</li> <li>feat(tools): install <code>yamllint</code>, <code>ansible-lint</code> and <code>k9s</code></li> <li>feat(tools): set <code>KUBECONFIG</code> by default</li> <li>feat: add pre-commit hooks</li> <li>feat: add script to setup Gitea tokens and OAuth apps</li> <li>perf(argocd): turning on selective sync</li> <li>refactor(docs): migrate to mkdocs</li> <li>refactor(metal): migrate to Fedora 36 for newer packages</li> <li>refactor(pxe)!: combine dhcpd and tftpd to dnsmasq</li> <li>Replace standard Vault with Vault Operator</li> <li>Automatically initialize and unseal Vault</li> <li>Declarative secret generation and management</li> <li>Declarative Gitea configuration with YAML</li> <li>Automatic OS rolling upgrade</li> <li>Automatic Kubernetes rolling upgrade</li> <li>Automatic application updates using Renovate (still require manual token generation)</li> <li>Add script to wait for essential services after deployment</li> <li>Add icons and bookmarks to the home page</li> <li>Deploy Matrix chat</li> <li>Replace Authentik with Dex for SSO (still require manual token generation)</li> <li>Switch to Mermaid for diagrams in documentation</li> <li>Replace Vagrant with k3d for development environment</li> <li>Use nip.io domain for development environment</li> <li>Remove Backblaze (S3 Glacier and/or Minio will be added in future version)</li> <li>Enable monitor for the majority of applications</li> <li>Many code refactorings and bug fixes</li> <li>Upgrade to Kubernetes 1.23</li> <li>Support external resources:<ul> <li>Cloudflare DNS and Tunnel</li> <li>Backblaze for backup</li> <li>Auto inject secrets to required namespaces</li> </ul> </li> <li>Replace self-signed certificates with Let's Encrypt production (with API token injected from the <code>external</code> layer)</li> <li>Add DNS records automatically using external-dns</li> <li>Easy Cloudflare Tunnel configuration with annotations</li> <li>Offsite backup to Backblaze B2 bucket using k8up-operator</li> <li>Add private container registry</li> <li>Remove Knative to save resources (temporarily)</li> <li>Enable encryption at rest for Kubernetes Secrets</li> <li>Add more Tekton tasks and pipelines</li> <li>Initialize GitOps repository on Gitea automatically after install</li> <li>Generate MetalLB address pool automatically (default to the last <code>/27</code> subnet)</li> <li>Some bug fixes</li> <li>Add convenience scripts</li> <li>Add Loki for logging</li> <li>Add custom health check for Application and ApplicationSet</li> <li>Use Vault with dev mode on (temporarily until we hit beta)</li> <li>Replace Authelia with Authentik</li> <li>Upgrade to Kubernetes 1.22</li> <li>Upgrade most services to the latest version</li> <li>Set ingress class and storage class explicitly</li> <li>Initial Linkerd and Knative setup (not working yet)</li> <li>Set up Hajimari for home page with automatic ingress discovery</li> <li>Add dev VM for local development or evaluation</li> <li>Optimize bare metal provisioning performance</li> <li>Replace Syncthing with Seafile (may use both in the feature)</li> <li>Enable Gitea SSH cloning via Ingress</li> <li>Various code clean up</li> <li>Add more documents</li> <li>Switch to Rocky Linux</li> <li>Some optimization for bare metal provisioning</li> <li>Switch to k3s and combine Kubernetes cluster config in <code>./infra</code> layer to <code>./metal</code> layer (because k3s is also configured using Ansible)</li> <li>Split boostrap Helm charts in <code>./infra</code> layer to <code>./bootstrap</code> layer (with new ArgoCD pattern) and <code>./system</code> layer</li> <li>Add <code>./platform</code> layer for some applications like Gitea, Tekton...</li> <li>User only need to provision <code>./metal</code> and <code>bootstrap</code> layer, the <code>./bootstrap</code> layer will deploy the remaining layers</li> <li>Provisioning time from empty disk to running services is significantly reduced (thanks to k3s and new bootstrap pattern)</li> <li>Use mdBook for documents</li> <li>Replace Drone CI with Tekton</li> <li>Enable TLS on all Ingresses (using cert-manager)</li> <li>Add some new applications</li> <li>Generate Terraform backend config automatically</li> <li>Switch to CoreOS</li> <li>Better PXE boot setup</li> <li>Diagrams as code</li> <li>Ensure idempotency for bare metal provisioning</li> <li>Extract instead of mounting the OS ISO file</li> <li>Easy initial controller setup (with only Docker)</li> <li>Switch to Fedora</li> <li>Remove LXD</li> <li>Move etcd (Terraform state backend) back to Docker</li> <li>Bare metal provisioning with PXE</li> <li>LXD cluster</li> <li>Terraform state backend (etcd)</li> <li>RKE cluster</li> <li>Core services (Vault, Gitea, ArgoCD,...)</li> <li>Public services to the internet (via port forwarding or Cloudflare Tunnel)</li> </ul>"},{"location":"checklist-for-production/","title":"Checklist For Production","text":""},{"location":"checklist-for-production/#introduction","title":"Introduction","text":"<p>This checklist is intended to be used as a guide for deploying Ubiquity in production. It is not intended to be exhaustive, but rather to provide a starting point for the most common deployment scenarios.</p>"},{"location":"checklist-for-production/#ubiquity","title":"Ubiquity","text":""},{"location":"checklist-for-production/#general","title":"General","text":"<ul> <li> Ubiquity is installed on a supported platform</li> <li> Ubiquity is installed using a supported method</li> <li> Ubiquity is installed using a supported version</li> <li> Ubiquity is installed using a supported configuration</li> <li> Ubiquity is installed using a supported storage provider</li> <li> Ubiquity is installed using a supported identity provider</li> <li> Ubiquity is installed using a supported platform provider</li> <li> Ubiquity is installed using a supported database</li> <li> Ubiquity is installed using a supported ingress controller</li> <li> Ubiquity is installed using a supported load balancer</li> <li> Ubiquity is installed using a supported DNS provider</li> <li> Ubiquity is installed using a supported SMTP provider</li> <li> Ubiquity is installed using a supported monitoring provider</li> <li> Ubiquity is installed using a supported logging provider</li> <li> Ubiquity is installed using a supported metrics provider</li> <li> Ubiquity is installed using a supported alerting provider</li> <li> Ubiquity is installed using a supported backup provider</li> </ul>"},{"location":"checklist-for-production/#security","title":"Security","text":"<ul> <li> Ubiquity is installed using a supported TLS configuration</li> <li> Ubiquity is installed using a supported TLS certificate</li> <li> Ubiquity is installed using a supported TLS key</li> <li> Ubiquity is installed using a supported TLS CA certificate</li> <li> Ubiquity is installed using a supported TLS CA key</li> <li> Ubiquity is installed using a supported TLS CA certificate bundle</li> </ul>"},{"location":"checklist-for-production/#identity","title":"Identity","text":"<ul> <li> Ubiquity is configured to use a supported identity provider</li> <li> Ubiquity is configured to use a supported identity provider configuration</li> <li> Ubiquity is configured to use a supported identity provider client ID</li> <li> Ubiquity is configured to use a supported identity provider client secret</li> </ul>"},{"location":"checklist-for-production/#storage","title":"Storage","text":"<ul> <li> Ubiquity is configured to use a supported storage provider</li> <li> Ubiquity is configured to use a supported storage provider configuration</li> <li> Ubiquity is configured to use a supported storage provider client ID</li> </ul>"},{"location":"checklist-for-production/#platform","title":"Platform","text":"<ul> <li> Ubiquity is configured to use a supported platform provider</li> <li> Ubiquity is configured to use a supported platform provider configuration</li> </ul>"},{"location":"checklist-for-production/#database","title":"Database","text":"<ul> <li> Ubiquity is configured to use a supported database</li> </ul>"},{"location":"checklist-for-production/#ingress","title":"Ingress","text":"<ul> <li> Ubiquity is configured to use a supported ingress controller</li> </ul>"},{"location":"checklist-for-production/#load-balancer","title":"Load Balancer","text":"<ul> <li> Ubiquity is configured to use a supported load balancer</li> </ul>"},{"location":"checklist-for-production/#dns","title":"DNS","text":"<ul> <li> Ubiquity is configured to use a supported DNS provider</li> </ul>"},{"location":"checklist-for-production/#smtp","title":"SMTP","text":"<ul> <li> Ubiquity is configured to use a supported SMTP provider</li> </ul>"},{"location":"checklist-for-production/#monitoring","title":"Monitoring","text":"<ul> <li> Ubiquity is configured to use a supported monitoring provider</li> </ul>"},{"location":"checklist-for-production/#logging","title":"Logging","text":"<ul> <li> Ubiquity is configured to use a supported logging provider</li> </ul>"},{"location":"checklist-for-production/#metrics","title":"Metrics","text":"<ul> <li> Ubiquity is configured to use a supported metrics provider</li> </ul>"},{"location":"checklist-for-production/#alerting","title":"Alerting","text":"<ul> <li> Ubiquity is configured to use a supported alerting provider</li> </ul>"},{"location":"checklist-for-production/#backup","title":"Backup","text":"<ul> <li> Ubiquity is configured to use a supported backup provider</li> </ul>"},{"location":"checklist-for-production/#kubernetes","title":"Kubernetes","text":""},{"location":"checklist-for-production/#general_1","title":"General","text":"<ul> <li> Kubernetes is installed on a supported platform</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":"<p>Installing Ubiquity is a relatively simple and straightforward process. That is the entire goal - To make it easy to install and use.</p>"},{"location":"getting-started/#pick-method-of-installation","title":"Pick method of installation","text":"<p>There are 3 supported methods to install and run Ubiquity:</p> <ul> <li>Using laptop/workstation in a standalone sandbox mode. Fastest stand-up time, but runs on a single system. Should be considered \"for evaluation only\". Lacks longhorn storage.</li> <li>Using a cloud provider multi-node. For deploying on a cloud provider.</li> <li>Using on-premises hardware, multi-node. For deploying on-premises.</li> </ul>"},{"location":"getting-started/#configure-ubiquity","title":"Configure Ubiquity","text":"<p>You can tune Ubiquity to your needs after initial deployment. These include:</p> <ul> <li>Configuring identity providers. Ubiquity supports a range of OIDC and SAML based IdPs.</li> <li>Configuring storage providers. Ubiquity supports a range of storage providers.</li> <li>Configuring platform providers. Ubiquity supports a range of platform providers.</li> <li>Adding catalogue items for sharing amongst Ubiquity users as a self-serve function.</li> </ul>"},{"location":"getting-started/#access-ubiquity","title":"Access Ubiquity","text":"<p>You are done! Simply access using the user-guide! </p> <p>If you are happy and want to support the project, make sure you check the support page.</p> <p>Danger</p> <p>Before going into production, make sure you have completed the go-live checklist.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#pxe-server-logs","title":"PXE server logs","text":"<p>To view PXE server (includes DHCP, TFTP and HTTP server) logs:</p> <pre><code>./scripts/pxe-logs\n</code></pre> <p>Tip</p> <p>You can view the logs of one or more containers selectively, for example:</p> <pre><code>./scripts/pxe-logs dnsmasq\n./scripts/pxe-logs http\n</code></pre>"},{"location":"troubleshooting/#nodes-not-booting-from-the-network","title":"Nodes not booting from the network","text":"<ul> <li>Plug a monitor and a keyboard to one of the bare metal node if possible to make the debugging process easier</li> <li>Check if the controller (PXE server) is on the same subnet with bare metal nodes (sometimes Wifi will not work or conflict with wired Ethernet, try to turn it off)</li> <li>Check if bare metal nodes are configured to boot from the network</li> <li>Check if Wake-on-LAN is enabled or IPMI is working correctly</li> <li>Check if the operating system ISO file is mounted</li> <li>Check the controller's firewall config</li> <li>Check PXE server Docker logs</li> <li>Check if the servers are booting to the correct OS (OS installer instead of the previously installed OS), if not try to select it manually or remove the previous OS boot entry</li> <li>Check that the bare metal node is booting off the correct interface</li> <li>Check that the VLAN the node is sat on is the same as your controller</li> </ul>"},{"location":"about/screenshots/","title":"Screenshots","text":"<p>Here are some screenshots of the environment in action:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"about/support/","title":"Support","text":"<p>Ubiquity is an open-source project developed by The Ubiquity Authors and the broader community including research institutions. We are committed to providing a high-quality, open-source HPC platform for the research community.</p>"},{"location":"about/support/#support-and-sponsored-development","title":"Support and sponsored development","text":"<p>For commercial support options and hosted services, please reach out to the community through GitHub Issues or GitHub Discussions.</p> <p>To contact the project maintainers directly, please use GitHub Discussions.</p>"},{"location":"about/support/#managed-hosting-for-research-institutions","title":"Managed hosting for research institutions","text":"<p>Please reach out through GitHub Discussions if you represent an R&amp;D organisation and are interested in collaborating on Ubiquity or discussing hosted HPC services.</p>"},{"location":"admin-guide/","title":"Admin Guide","text":""},{"location":"admin-guide/#acronyms","title":"Acronyms","text":"<ul> <li>HPC - High-Performance Computing</li> <li>MPI - Message Passing Interface</li> <li>GPU - Graphics Processing Unit</li> <li>CPU - Central Processing Unit</li> <li>FPGA - Field-Programmable Gate Array</li> <li>TPU - Tensor Processing Unit</li> <li>CUDA - Compute Unified Device Architecture</li> <li>OpenCL - Open Computing Language</li> <li>GPGPU - General-Purpose Computing on Graphics Processing Units</li> <li>FFT - Fast Fourier Transform</li> <li>LINPACK - Linear Algebra Package</li> <li>PDE - Partial Differential Equation</li> <li>CFD - Computational Fluid Dynamics</li> <li>IO - Input/Output</li> <li>RAM - Random Access Memory</li> <li>ROM - Read-Only Memory</li> <li>SSD - Solid-State Drive</li> <li>HDFS - Hadoop Distributed File System</li> <li>NFS - Network File System</li> <li>HDF5 - Hierarchical Data Format version 5</li> <li>SSH - Secure Shell</li> <li>LAN - Local Area Network</li> <li>WAN - Wide Area Network</li> <li>SAN - Storage Area Network</li> <li>NUMA - Non-Uniform Memory Access</li> <li>SMP - Symmetric Multiprocessing</li> <li>HT - Hyper-Threading</li> <li>OS - Operating System</li> <li>BIOS - Basic Input/Output System</li> <li>UEFI - Unified Extensible Firmware Interface</li> <li>PBS - Portable Batch System</li> <li>SLURM - Simple Linux Utility for Resource Management</li> <li>TORQUE - Terascale Open-source Resource and QUEue manager</li> <li>PBS - Portable Batch System</li> <li>DRAM - Dynamic Random Access Memory</li> <li>ECC - Error-Correcting Code</li> <li>RAID - Redundant Array of Independent Disks</li> <li>SSD - Solid State Drive</li> <li>NVMe - Non-Volatile Memory Express</li> <li>IB - InfiniBand</li> <li>NIC - Network Interface Card</li> <li>FP64 - Double Precision Floating Point</li> <li>FP32 - Single Precision Floating Point</li> <li>FP16 - Half Precision Floating Point</li> <li>FP8/FP4 - AI Floating point</li> <li>AI - Artificial Intelligence</li> <li>ML - Machine Learning</li> <li>DL - Deep Learning</li> <li>VLSI - Very-Large-Scale Integration</li> <li>ASIC - Application-Specific Integrated Circuit</li> <li>CSI - Container Storage Interface</li> <li>ASU - Lenovo Advanced Systems Utility</li> <li>XCC - XClarity Controller</li> <li>BMO - Bare Metal Operator</li> </ul>"},{"location":"admin-guide/#introduction","title":"Introduction","text":""},{"location":"admin-guide/#document-format","title":"Document Format","text":"<p>The documentation for Ubiquity is in markdown format - Embedded within the git repository and as such can be served as github pages or equivalent.</p>"},{"location":"admin-guide/#system-introduction","title":"System Introduction","text":"<p>Ubiquity is deployed as a high-availability control plane, using keepalived for a floating IP address.</p> <p>The kubernetes service underpinning it then manages all other services as Kubernetes PODS. This includes: - DNS - Ansible via AWX - KeyCloak - Monitoring - Workload Managers (where necessary) - Vault - Onyxia (self-serve Kubernetes provisioning layer) - ArgoCD - BareMetalOperator - ..and so on.</p> <p>All services are broken down into functions, the functions of which are within subdirectories: - Bootstrap - For installing ArgoCD and its app-of-apps functionality - System - For system-underpinning services core to the cluster - Platform - For user-accessible platforms - Apps - For end-user applications</p>"},{"location":"admin-guide/#administrator-users","title":"Administrator users","text":"<p>All access to Ubiquity platforms is provided via a toolbox container called OPUS (also the tool used to deploy the cluster) - No administrative tooling is generally installed on the system unless site-specfics dictate that it's required. Therefore there are no specific users to mention other than the user who is entitled to spin up OPUS which is controlled by an AzureCR key.</p> <p>Access directly to nodes can be achieved using SSH-key only - The root passwords on every node are locked. The key is created on installation and is an ED25519 key.</p>"},{"location":"admin-guide/#the-nfs-storage-subsystem","title":"The NFS Storage Subsystem","text":"<p>NFS is available within Ubiquity - It can be provided using an exisitng NFS share and provisioned out using the NFS CSI - There are also playbooks to provision an NFS server as a 3-way installation using Pacemaker (if you do not have an NFS share).</p>"},{"location":"admin-guide/#the-lustre-storage-subsystem","title":"The Lustre Storage Subsystem","text":"<p>Lustre is available within Ubiquity - It can be provided using an existing Lustre filesystem and provisioned out using the Lustre CSI or passing through the node as a directory.</p>"},{"location":"admin-guide/#the-storage-scale-storage-subsystem","title":"The Storage Scale Storage Subsystem","text":"<p>TBC</p>"},{"location":"admin-guide/#ip-numbering-scheme","title":"IP Numbering Scheme","text":"<p>Generally a default install of a cluster follows the following convention, using an RFC1918-compliant 10.0.0.0/8 network.</p> <p>Networking within Kubernetes uses the Kubernetes flannel network as a software-defined networking scheme, with IP addresses that are auto-allocated and can continually change - These never route out of the network and generate lots of firewall rules, operating via NFTables (as it's faster than IPTables).</p> <p>Site-specific configurations will be in the site-specific folder.</p>"},{"location":"admin-guide/#variable-subnet-definitions","title":"Variable Subnet Definitions:","text":"<ul> <li>10.0.0.x/22 - Vlan102 - OOB/IPMI</li> <li>10.46.0.0/16 - Kubernetes flannel network</li> <li>10.48.0.0/16 - Kubernetes service network</li> <li>10.1.0.x/22 - Vlan103 - MGMT</li> <li>10.8.0.x/22 - N/A - InfiniBand IPoIB</li> </ul>"},{"location":"admin-guide/#ip-address-assignment","title":"IP Address Assignment","text":"<p>The general configuration is as follows: - .1-3 - OOB for control plane nodes 1-3 - ..251-253 - OOB interface on control plane nodes to control OOB devices - .1-3 - Control plane nodes 1-3 - .1-3 - IB for control plane nodes 1-3 - .11-?? - OOB for compute nodes - .11-?? - MGMT for compute nodes - .11-?? - IB for compute nodes <p>Site-specfic configurations will be in the site-specfic folder</p>"},{"location":"admin-guide/#ethernet-port-allocation","title":"Ethernet port Allocation","text":"<p>Good practice would be to label all ports and shut all unused ports for physical security - This means that the port label on switch configs matches your running cluster and is merely a case of good housekeeping to keep on top of.</p>"},{"location":"admin-guide/#ethernet-port-connections-on-physical-nodes","title":"Ethernet Port connections on physical nodes","text":"<p>This should be addressed via port labelling on switches which should say nodename.portnum</p>"},{"location":"admin-guide/#initial-system-build","title":"Initial System Build","text":""},{"location":"admin-guide/#hardware-state","title":"Hardware state","text":""},{"location":"admin-guide/#bios-settings","title":"BIOS Settings","text":"<p>BIOS settings can be retrieved using the ASU tool within the XCC. You can SSH to the XCC address and use credentials from the BareMetalHost definition for the node to gain access.</p>"},{"location":"admin-guide/#retrieve-bios-settings-from-asu","title":"Retrieve BIOS Settings from ASU","text":"<p>To retrieve BIOS Settings from ASU, you can use SSH to SSH to the OOB address for the XCC which then presents you with a <code>system&gt;</code> prompt.</p> <p>At this prompt a question mark at any point followed by enter will show you context and syntax help for any command.</p> <p>From there, run the <code>asu show</code> command, this dumps out all current configuration for the node.</p>"},{"location":"admin-guide/#clone-bios-settings-from-one-node-to-another-post-install-addition-to-smg","title":"Clone BIOS Settings from One Node to Another (post-install addition to SMG)","text":"<p>Using the same process for XCC, use ASU to dump out all the configuration, then:</p> <ul> <li>Sed replace the '=' with a space</li> <li>Sed insert the first words on each line of output with a 'asu set '</li> <li>In the XCC terminal, paste.</li> </ul> <p>This could be automated using a very simple container.</p>"},{"location":"admin-guide/#hardware-raid-card-configuration","title":"Hardware RAID Card configuration","text":"<p>RAID configuration can be set or shown via the XCC using ASU, or via the web interface for the node in question on the same IP address.</p>"},{"location":"admin-guide/#build-sequence-for-hawk-systems-from-management-node","title":"Build sequence for HAWK systems from management node.","text":""},{"location":"admin-guide/#install-basic-image","title":"Install basic image","text":"<p>Images are created and deployed via the Bare Metal Operator. The BMO uses openstack ironic to deploy bare metal images, pre-built in qcow2 format.</p> <p>These images are created using the diskimage-creator tool (please see training slides on usage, however these will be copied into the administration folder under disk image build, then uploaded/copied into the ironic HTTP server (in the <code>/shared/html/images</code> folder), and then referenced in the BareMetalHost definition/manifest.</p>"},{"location":"admin-guide/#system-management","title":"System management","text":""},{"location":"admin-guide/#access-methods","title":"Access methods","text":"<p>The general access method for administrators is via OPUS, however if you have access to the ED25519 key and the kubeconfig.yaml then you can access from anywhere that has SSH access and network access to the in-band/out-of-band addresses, such as a bastion node. This simplifies access, and a bastion node can have all the required tooling necessary to run OPUS as a container to continue to avoid installing specific tools on a node (which becomes a security risk).</p> <p>You can do this via SSH-tunnelling accordingly and defining inside your kubeconfig the <code>proxy:</code> setting to allow your kubernetes client to talk to the main cluster control plane.</p>"},{"location":"admin-guide/#testdev","title":"Test/Dev","text":"<p>You can test/dev changes to kubernetes environments via using the \"sandbox\" mode that is defined inside the deployment folder . This means that you can deploy thea copy of your environment settings as a k3d (or kubernetes in docker) image which duplicates your kubernetes environment inside a docker container that you can tear down once wfinished.</p>"},{"location":"admin-guide/#maintenance-procedures","title":"MAINTENANCE PROCEDURES","text":""},{"location":"admin-guide/#getting-support","title":"Getting Support","text":"<p>For community support, please visit our GitHub repository and open an issue.</p>"},{"location":"admin-guide/#general-maintenance-procedures","title":"General Maintenance Procedures","text":""},{"location":"admin-guide/#power-up-procedures","title":"Power Up Procedures","text":"<p>To power up a Ubiquity cluster, the following steps should be taken: - Power Up switches - Power Up Storage shelves and wait to settle - Power Up Control Planes - Once this comes online, all functions will be attempted to be restored to previous known state, including power status. - Power Up Storage Nodes (if not controlled by BareMetalHost) - This may occur irrespective of manual intervention in the event of an unscheduled power outage. - Power Up Compute Nodes - Again, controlled by BareMetalHost - Confirm Service Restoration - Again, Kubernetes will endeavour to restore service if they are Kubernetes Pods - Else manual steps to restore services such as Workload Managers etc.</p>"},{"location":"admin-guide/#power-down-procedures","title":"Power Down Procedures","text":"<ul> <li> <p>Start with workers:</p> </li> <li> <p>list worker nodes in kubetctl or k9s  - <code>kubectl get nodes</code></p> </li> <li>cordon (drain+unschedule) worker nodes - <code>kubectl cordon &lt;node&gt;</code></li> <li>wait for worker nodes to be clean</li> <li>BareMetalHost definition online to false (See below) &lt;&lt; only use ipmitool if absolutely necessary (from opus)</li> </ul>"},{"location":"admin-guide/#control-plane","title":"Control plane:","text":"<p>Make sure no workers are running</p> <p>ssh to each control plane node and issue init 0</p> <p>To check status of workers</p> <pre><code>kubectl get nodes | egrep -v '(control-plane|etcd|master|^NAME)'| awk -F '.' '{ print $1}' | while read line; doecho $line; kubectl -n metal-nodes get bmh $line -o custom-columns=ONLINE:.spec.online;echo \"\"; done\n</code></pre>"},{"location":"admin-guide/#cordon-workers","title":"Cordon workers","text":"<pre><code>kubectl get nodes | egrep -v '(control-plane|etcd|master|^NAME)'| awk '{ print $1 }' | while read line; dokubectl cordon $line; done\n</code></pre> <p>Get bmh_host kubectl get bmh -n metal-nodes...</p>"},{"location":"admin-guide/#get-bmh_host","title":"Get bmh_host","text":"<pre><code>kubectl get bmh -n metal-nodes\n</code></pre>"},{"location":"admin-guide/#poweroff","title":"poweroff","text":"<pre><code>kubectl -n metal-nodes annotate bmh &lt;bmh_host&gt; --overwrite reboot.metal3.io/poweroff=\"\"\n</code></pre>"},{"location":"admin-guide/#reboot","title":"reboot","text":"<pre><code>kubectl -n metal-nodes annotate bmh &lt;bmh_host&gt; --overwrite reboot.metal3.io='true'\n</code></pre>"},{"location":"admin-guide/#remove-poweroff-flag-add-a-dash-at-the-end-of-the-annotation-for-poweroff","title":"Remove Poweroff flag: Add a dash at the end of the annotation for poweroff","text":"<pre><code>kubectl -n metal-nodes annotate bmh &lt;bmh_host&gt; --overwrite reboot.metal3.io/poweroff-\n</code></pre>"},{"location":"admin-guide/#powering-on-a-compute-node","title":"Powering on a Compute Node","text":"<p>A node defined inside the BareMetalHost definition as long as the online: true state is defined will automatically be powered on. This is the \"default state\" for a BareMetalHost.</p>"},{"location":"admin-guide/#marking-a-compute-node-down-for-maintenance","title":"Marking a Compute Node Down for maintenance","text":"<p>See [##### poweroff] for details</p>"},{"location":"admin-guide/#resuming-a-compute-node","title":"Resuming a Compute Node","text":"<p>See [##### Remove Poweroff flag: Add a dash at the end of the annotation for poweroff] for details</p>"},{"location":"admin-guide/#viewing-downed-nodes","title":"Viewing Downed Nodes","text":"<p>Using OPUS, you can see node status by:</p> <p><pre><code>kubectl get nodes\n</code></pre> Or by running k9s and going to the <code>:nodes</code> view.</p>"},{"location":"admin-guide/#replacing-a-compute-node-system","title":"Replacing a Compute Node System","text":"<p>To replace a compute node, simply change the MAC address inside the BareMetalHost definition, and configure the BMC with the same address (and password). If necessary, clone the BIOS settings as per [##### Clone BIOS Settings from One Node to Another (post-install addition to SMG)]</p>"},{"location":"admin-guide/#business-as-usual-management","title":"Business as Usual Management","text":""},{"location":"admin-guide/#unbanning-login-node-ips-fail2ban","title":"Unbanning login node IPs (fail2ban)","text":"<p>Dependent on system configuration specifics, however if fail2ban is installed please follow these steps to unban an IP:"},{"location":"admin-guide/#editing-slurmconf","title":"Editing slurm.conf","text":"<p>There are 3 modes of operation for Ubiquity that can be chosen: - Baremetal - Effectively has monitoring and general services on Kubernetes, but all workload manager functions and compute functions are baremetal. - Hybrid - Workload manager control daemons are inside Kubernetes and can communicate with baremetal workers. - Native - Workload manager control daemons are inside Kubernetes and worker nodes are PODS within Kubernetes.</p> <p>To edit in baremetal mode: Edit the ansible playbook in ubiq-playbooks and edit the vars for it, located in <code>ubiq-playbooks/workload-managers/&lt;workload manager&gt;/vars/main.yml</code> - Then call the workload-managers role within AWX on the nodes required.</p> <p>To edit in Hybrid or Native mode: Edit the values.yaml inside platform/hpc-ubiq/slurm/base/values.yaml - Add required configs and git commit, git push.</p> <p>Wait for undrain-nodes-hook to complete.</p>"},{"location":"admin-guide/#work-load-management","title":"Work Load Management","text":""},{"location":"admin-guide/#slurm-accounting","title":"Slurm Accounting","text":"<p>To inspect slurm accounting, login to a slurm node that has slurm control rights and run sacctmgr.  In addition, a slurm-exporter is included in all installation modes and can be access by adding a slurm dashboard for monitoring purposes.</p>"},{"location":"admin-guide/#monitoring","title":"Monitoring","text":"<p>Monitoring for Ubiquity is provided by Prometheus, and visualised with Grafana. There are alerts that can be configured using AlertManager. To login to grafana, using a web browser go to [grafana][https://grafana.] You can also log into the prometheus instance, however all configuration for prometheus is inside git and we don't provision an ingress for prometheus for this reason. For more detail on prometheus and grafana, please see administration/tutorials/kube-prometheus-stack.md <p>For logging purposes, Loki is used - which aggregates logging information to Grafana to be able to be searched interactively. For more details please see administration/tutorials/loki-promtail.md</p>"},{"location":"admin-guide/#customising-monitoring","title":"Customising Monitoring","text":"<p>All configuration for monitoring is inside git and controlled by values.yaml for monitoring-system. Edit the values.yaml for the monitoring platform, git commit, git push and wait for ArgoCD to implement monitoring changes.</p>"},{"location":"admin-guide/#monitoring-users","title":"Monitoring users","text":"<p>Monitoring within Ubiquity defines a default admin user, of which you can get the admin user password from a secret within Kubernetes. There is also a helpful convenience script called <code>grafana-admin-password.sh</code> which when ran from OPUS gives you the admin password.</p> <p>This can be changed inside the values.yaml - As explained in [### Customising Monitoring].</p> <p>You can add extra users either inside values.yaml or you can attach external authentication methods such as KeyCloak.</p>"},{"location":"admin-guide/#nfslustre-storage-administration","title":"NFS/Lustre Storage Administration","text":""},{"location":"admin-guide/#nfs-quota","title":"NFS Quota","text":""},{"location":"admin-guide/#nfs-faults","title":"NFS Faults","text":""},{"location":"admin-guide/#lustre-quota","title":"Lustre Quota","text":""},{"location":"admin-guide/#lustre-faults","title":"Lustre Faults","text":""},{"location":"admin-guide/#lustre-performance-tracking","title":"Lustre Performance Tracking","text":""},{"location":"admin-guide/#disk-failure","title":"Disk Failure","text":""},{"location":"admin-guide/#pacemakercorosync-troubleshooting","title":"Pacemaker/Corosync troubleshooting","text":"<p>pcs status etc</p>"},{"location":"admin-guide/#log-files","title":"Log files","text":""},{"location":"admin-guide/#cluster-shell","title":"Cluster Shell","text":"<p>To run commands om multiple nodes, please see run-commands-on-multiple-nodes</p>"},{"location":"admin-guide/#serial-console","title":"Serial Console","text":"<p>Attach via XCC or IPMITool</p>"},{"location":"admin-guide/#compiling-software-adding-modules","title":"Compiling Software, Adding Modules","text":"<p>Adding a new Environment Module</p>"},{"location":"admin-guide/#load-balancing-users-across-the-login-nodes","title":"Load balancing users across the login nodes","text":"<p>DNS round robin?</p>"},{"location":"admin-guide/#updating-infiniband-hca-firmware","title":"Updating InfiniBand HCA firmware","text":""},{"location":"admin-guide/#using-mlxfw","title":"Using mlxfw","text":""},{"location":"admin-guide/#infiniband-troubleshooting","title":"InfiniBand Troubleshooting","text":""},{"location":"admin-guide/#ldap-openldap-based","title":"LDAP (openLDAP-based)","text":"<p>Ubiquity can run a 3-way openLDAP instance that can import LDIFs and manage appropriately.</p>"},{"location":"admin-guide/#compute-node-frequency-control-turbo","title":"Compute Node Frequency Control &amp; Turbo","text":"<p>This can be controlled via scripts that are present inside the ubiq-playbooks directory inside the HPCTuning role.</p>"},{"location":"admin-guide/#additional-package-installation","title":"Additional Package Installation","text":""},{"location":"admin-guide/#firewalld-configuration","title":"Firewalld configuration","text":""},{"location":"admin-guide/#temperature-monitor","title":"Temperature monitor","text":"<p>Ubiquity can shutdown on thermal events using the IPMI exporter within Grafana</p>"},{"location":"admin-guide/#backups-and-dr","title":"Backups and DR","text":"<p>For more information, please see administration/tutorials/backup-cluster.md</p>"},{"location":"admin-guide/administration/tutorials/add-or-remove-nodes/","title":"Add or remove nodes","text":"<p>Or how to scale vertically. To replace the same node with a clean OS, remove it and add it again.</p>"},{"location":"admin-guide/administration/tutorials/add-or-remove-nodes/#add-new-nodes-to-control-plane","title":"Add new nodes to control plane","text":"<p>Tip</p> <p>You can add multiple nodes at the same time</p> <p>Add its details to the inventory at the end of the group (masters or workers):</p> metal/inventories/prod.yml<pre><code>diff --git a/metal/inventories/prod.yml b/metal/inventories/prod.yml\nindex 7f6474a..1bb2cbc 100644\n--- a/metal/inventories/prod.yml\n+++ b/metal/inventories/prod.yml\n@@ -8,3 +8,4 @@ metal:\n     workers:\n       hosts:\n         metal3: {ansible_host: 192.168.1.113, mac: '00:23:24:d1:f5:69', disk: sda, network_interface: eno1}\n+        metal4: {ansible_host: 192.168.1.114, mac: '00:11:22:33:44:55', disk: sda, network_interface: eno1}\n</code></pre> <p>Install the OS and join the cluster:</p> <pre><code>make metal\n</code></pre> <p>That's it!</p>"},{"location":"admin-guide/administration/tutorials/add-or-remove-nodes/#remove-a-node","title":"Remove a node","text":"<p>Danger</p> <p>It is recommended to remove nodes one at a time</p> <p>Remove it from the inventory:</p> metal/inventories/prod.yml<pre><code>diff --git a/metal/inventories/prod.yml b/metal/inventories/prod.yml\nindex 7f6474a..d12b50a 100644\n--- a/metal/inventories/prod.yml\n+++ b/metal/inventories/prod.yml\n@@ -4,7 +4,6 @@ metal:\n       hosts:\n         metal0: {ansible_host: 192.168.1.110, mac: '00:23:24:d1:f3:f0', disk: sda, network_interface: eno1}\n         metal1: {ansible_host: 192.168.1.111, mac: '00:23:24:d1:f4:d6', disk: sda, network_interface: eno1}\n-        metal2: {ansible_host: 192.168.1.112, mac: '00:23:24:e7:04:60', disk: sda, network_interface: eno1}\n     workers:\n       hosts:\n         metal3: {ansible_host: 192.168.1.113, mac: '00:23:24:d1:f5:69', disk: sda, network_interface: eno1}\n</code></pre> <p>Drain the node:</p> <pre><code>kubectl drain ${NODE_NAME} --delete-emptydir-data --ignore-daemonsets --force\n</code></pre> <p>Remove the node from the cluster</p> <pre><code>kubectl delete node ${NODE_NAME}\n</code></pre> <p>Shutdown the node:</p> <pre><code>ssh root@${NODE_IP} poweroff\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/","title":"Back up and restore K8s objects in an on-premises environment","text":"<p>This tutorial describes how you can use Velero, an open-source tool, to back up and restore Kubernetes cluster resources and persistent volumes in an on-premises environment. This is helpful when you destroyed some Kubernetes resources for whatever reason, for example, when you delete the suite namespace accidentally. </p> <p>Note This tool doesn't back up database data and NFS data. </p>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#export-an-nfs-directory","title":"Export an NFS directory","text":"<p>On your NFS server, export one NFS directory.</p> <p>Create one directory under the base directory. For example, if your existing NFS directories share the base directory /var/vols/itom, run the following command:</p> <pre><code>mkdir -p  /var/vols/itom/minio\n</code></pre> <p>Change the permission of the directory:</p> <pre><code>chmod -R 755 /var/vols/itom/minio\n</code></pre> <p>Change the ownership of the directory (change 1999:1999 to your own UID:GID if you use custom values):</p> <pre><code>chown -R 1999:1999 /var/vols/itom/minio\n</code></pre> <p>In the /etc/exports file, export the NFS directory by adding one line (change 1999 to your own UID or GID if you use a custom value for them): </p> <p><code>/var/vols/itom/minio *(rw,sync,anonuid=1999,anongid=1999,root_squash)</code></p> <p>Run the following command:</p> <pre><code>exportfs -ra\n</code></pre> <p>Run the following command to check that the directory is exported:</p> <pre><code>showmount -e | grep minio\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#download-the-minio-images","title":"Download the minio images","text":"<p>If your control plane nodes (formerly known as \"master nodes\") have Internet access, download the image from a control plane node; otherwise, download the image from another Linux machine that has Internet access and then transfer the image to the control plane node.</p> <p>On the download machine, navigate to a directory where you want to download the images, and then run the following commands :</p> <pre><code>docker pull minio/minio:latest \ndocker pull  minio/mc:latest\n</code></pre> <p>If the control plane node has no Internet access, transfer the images to the control plane node. </p>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#obtain-the-image-ids","title":"Obtain the image IDs.","text":"<p>Run the following command: </p> <pre><code>docker images |grep minio\n</code></pre> <p>In the output, find the IDs of the images. In the following example, it's <code>8dbf9ff992d5</code>.</p> <pre><code>docker.io/minio/minio latest 8dbf9ff992d5 30 hours ago 183 MB\n</code></pre> <p>Run the following command to tag one image:</p> <pre><code>docker tag &lt;image ID&gt; &lt;image registry URL&gt;/&lt;organization name&gt;/minio:&lt;tag&gt;\n</code></pre> <p>The following are two examples:</p> <pre><code>docker tag 8dbf9ff992d5 myregistry.azurecr.io/sandbox/minio:test\ndocker tag 8dbf9ff992d5 localhost:5000/hpeswitom/minio:test\n</code></pre> <pre><code>&lt;image ID&gt;: the image ID you obtained in the previous step.\n\n&lt;image registry URL&gt;/&lt;organizaition name&gt;: your image registry URL/organization name. If using the local registry, it's localhost:5000/hpeswitom; if using an external registry, ask your registry administrator for it.\n\n&lt;tag&gt;: specify any value you like. \n</code></pre> <p>Repeat the step above to tag the other image (minio/mc:latest) into your image registry. </p>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#push-the-images-into-your-image-registry","title":"Push the images into your image registry:","text":"<pre><code>docker push &lt;image registry URL&gt;/&lt;organization name&gt;/minio:&lt;tag&gt;\ndocker push &lt;image registry URL&gt;/&lt;organization name&gt;/mc:&lt;tag&gt;\n\n# Example 1:\n\ndocker push myregistry.azurecr.io/sandbox/minio:test\ndocker push myregistry.azurecr.io/sandbox/mc:test\n\n# Example 2:\n\ndocker push localhost:5000/hpeswitom/minio:test\ndocker push localhost:5000/hpeswitom/mc:test\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#install-velero-on-a-control-plane-node","title":"Install Velero on a control plane node","text":"<p>Perform the following steps to install Velero on the control plane node.</p> <p>Download the tarball of the latest Velero release to a temporary directory on the control plane node. The download URL is https://github.com/vmware-tanzu/velero/releases/. Extract the package:</p> <pre><code>tar -xvf &lt;release-tarball-name&gt;.tar.gz\n</code></pre> <p>The directory you extracted is called the \u201cVelero directory\u201d in subsequent steps. </p> <p>Move the Velero binary from the Velero directory to somewhere in your PATH. For example:</p> <pre><code>cp velero /usr/local/bin/\n</code></pre> <p>Create a Velero-specific credentials file in your local directory. For example, in the Velero directory:</p> <pre><code>cat &lt;&lt;ENDFILE &gt; ./credentials-velero\n[default]\naws_access_key_id = minio\naws_secret_access_key = minio123\nENDFILE\n</code></pre> <p>Navigate to the Velero directory, and create a backup copy of the examples/minio/00-minio-deployment.yaml file. This is because in the next steps, you will need to edit this file.  Edit the <code>00-minio-deployment.yaml</code> file as follows:</p> <p>Add PVs/PVCs to the <code>00-minio-deployment.yaml</code> file by appending the following code lines to the end of the file (replace $minio-NFS-path and $NFS-server-FQDN with the exported NFS folder path and hostname of the NFS server):</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: minio-pv-claim\n  namespace: velero\nspec:\n  capacity:\n    storage: 10Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: $minio-NFS-path\n    server: $NFS-server-FQDN\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minio-pv-claim\n  namespace: velero\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n  volumeName: minio-pv-claim\n</code></pre> <p>In the Deployment section, make the following change:</p> <p>From:</p> <pre><code>volumes:\n      - name: storage\n        emptyDir: {}\n      - name: config\n        emptyDir: {}\n</code></pre> <p>To:</p> <pre><code>volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: minio-pv-claim\n</code></pre> <p>Remove the last two lines in the Deployment section below:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: velero\n  name: minio\n  labels:\n    component: minio\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      component: minio\n  template:\n    metadata:\n      labels:\n        component: minio\n    spec:\n      volumes:\n      - name: storage\n        emptyDir: {}\n      - name: config\n        emptyDir: {}\n      containers:\n      - name: minio\n        image: minio/minio:latest\n        imagePullPolicy: IfNotPresent\n        args:\n        - server\n        - /storage\n        - --config-dir=/config\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: \"minio\"\n        - name: MINIO_SECRET_KEY\n          value: \"minio123\"\n        ports:\n        - containerPort: 9000\n        volumeMounts:\n        - name: storage\n          mountPath: \"/storage\"\n        - name: config\n          mountPath: \"/config\"\n</code></pre> <p>Replace the image values in the following lines with the images in your image registry:</p> <pre><code>image: minio/minio:latest \nimage: minio/mc:latest \n</code></pre> <p>For example, change them as shown below (this example uses an external registry):</p> <pre><code>image: myregistry.azurecr.io/sandbox/minio:test \nimage: myregistry.azurecr.io/sandbox/mc:test\n</code></pre> <p>Run the following commands:</p> <pre><code>kubectl apply -f examples/minio/00-minio-deployment.yaml\nvelero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.0.0 \\\n    --bucket velero \\\n    --secret-file ./credentials-velero \\\n    --use-volume-snapshots=false \\\n    --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://minio.velero.svc:9000\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#back-up-and-restore-k8s-objects","title":"Back up and restore k8s objects","text":"<p>Perform the following steps by using Velero.</p>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#backup","title":"Backup","text":"<p>To back up objects, run the following command on the control plane node on which Velero is installed:</p> <pre><code>velero backup create &lt;backup name&gt; --include-namespaces &lt;namespace&gt; --wait\n</code></pre> <p>The following are examples to back up objects for the suite and CDF, respectively:</p> <pre><code>velero backup create itsma-backup --include-namespaces itsma-fghnd --wait\nvelero backup create core-backup --include-namespaces core --wait\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#restore","title":"Restore","text":"<p>The following procedures assume that you have removed the suite namespace or the CDF core namespace. To test the procedure, you can run kubectl delete ns  to delete a namespace."},{"location":"admin-guide/administration/tutorials/backup-cluster/#restore-the-objects-for-the-suite","title":"Restore the objects for the suite","text":"<p>Perform the following steps:</p> <p>Once the suite namespace is deleted, the associated PVs are released. Before the restore, delete the released PVs by running the following command on the control plane node:</p> <pre><code>kubectl get pv |grep -i release | awk '{print $1}' | xargs kubectl delete pv\n</code></pre> <p>Run the following command to restore:</p> <pre><code>velero restore create --from-backup &lt;suite backup name&gt; --wait\n</code></pre> <p>For example:</p> <pre><code>velero restore create --from-backup itsma-backup --wait\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#change-the-nodeport-in-itom-nginx-ingress-svc-to-443","title":"Change the nodePort in itom-nginx-ingress-svc to 443:","text":"<p>Run the following command:</p> <pre><code>kubectl edit svc itom-nginx-ingress-svc -n &lt;suite namespace&gt;\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#change-nodeport-to-443-as-shown-below","title":"Change nodePort to 443 as shown below:","text":"<pre><code>- name: https-port\n    nodePort: 443\n    port: 443\n    protocol: TCP\n    targetPort: 443\n</code></pre>"},{"location":"admin-guide/administration/tutorials/backup-cluster/#restore-the-objects-for-cdf","title":"Restore the objects for CDF","text":"<p>Delete the PVs associated with the core namespace:</p> <pre><code>kubectl delete pv db-single itom-logging itom-vol\n</code></pre> <p>Restore with Velero:</p> <pre><code>velero restore create --from-backup &lt;core backup&gt; \u2013wait\n</code></pre> <p>For example:</p> <pre><code>velero restore create --from-backup core-backup \u2013wait\n</code></pre>"},{"location":"admin-guide/administration/tutorials/expose-services-to-the-internet/","title":"Expose services to the internet","text":"<p>Info</p> <p>This tutorial is for Cloudflare Tunnel users, please skip if you use port-forwarding.</p> <p>Apply the <code>./external</code> layer to create a tunnel if you haven't already, then add the following annotations to your <code>Ingress</code> object (replace <code>example.com</code> with your domain):</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/target: \"ubiquity-tunnel.example.com\"\n    external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\"\n# ...\n</code></pre>"},{"location":"admin-guide/administration/tutorials/install-pre-commit-hooks/","title":"Install pre-commit hooks","text":"<p>TODO: organize developer-focused documentation</p> <p>Git hook scripts are useful for identifying simple issues before commiting changes.</p> <p>Install pre-commit first, one-liner for Arch users:</p> <pre><code>sudo pacman -S python-pre-commit\n</code></pre> <p>Then install git hook scripts:</p> <pre><code>make git-hooks\n</code></pre>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/","title":"Kube-Prometheus","text":"<p>Kube-Prometheus is the CNCF monitoring system and time series database. It comes with an Alertmanager and Grafana as a WebUI component to visualise collected metrics.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#recommended-setup","title":"Recommended setup","text":"<p>Before you deploy the Kube-Prometheus, you should consider the following recommendations.</p> <p>cpu (vCPU)  Memory 6 CPU + 0.5 per node    6320MiB + 50MiB per node No further activities need to be carried out in advance.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#adding-kube-prometheus-stack-to-your-cluster","title":"Adding Kube Prometheus Stack to your cluster","text":"<p>Add the directory monitoring-system to your master ubiquity repository. This directory contains the building block for the Kube-Prometheus stack.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#configuration","title":"Configuration","text":""},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#required-configuration","title":"Required configuration","text":"<p>You have to set grafana.adminPassword. If you don\u2019t, the Grafana admin password changes on each CI run.</p> <p>Configure it in values.yaml:</p> <pre><code>grafana:\n  adminPassword: highly-secure-production-password\n</code></pre>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#configuring-alertmanager","title":"Configuring alertmanager","text":"<p>When adding a receiver, you need to copy the null receiver config into your own cluster configuration as well. If you customise the configuration of a BB with values.yaml files, you have to be careful that you cannot simply extend lists. Helm cannot merge lists. Therefore, the existing list plus the new entry must be added to the customised configuration.</p> <pre><code>alertmanager:\n  config:\n    receivers:\n      - name: \"null\"  # Add this to your config as well\n      - name: myotherreceiver\n        webhook_configs:\n          - send_resolved: true\n            url: https://myurl\n</code></pre> <p>With kube-prometheus-stack we already deploy an Alertmanager for you. In combination with Prometheus and the default rules a lot of base metrics are monitored and alerts for them are created when something goes wrong. In the default settings alerts are only visible in the webinterface of the alertmanager. Most of the time it is desirable to send those alerts to your operations team, your on-call engineer or someone else. To achieve that you can configure the alertmanager in values.yaml. There you can use the alertmanager.config.receivers setting to set all available options supported by alertmanager.</p> <p>Normally no alert should be triggered, but there is an exception! In the default configuration the Prometheus operator creates a watchdog alarm which is always triggered. This alarm can be used to check if your monitoring is working. If it stops triggering, either Prometheus or the alert manager is not working as expected. You can set up an external alerting provider (or a webhook hosted by you) to notify you when the alert stops triggering.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#forward-alertmanager-to-localhost","title":"Forward Alertmanager to localhost","text":"<pre><code>kubectl port-forward -n monitoring-system alertmanager-kube-prometheus-stack-alertmanager-0 9093 \n</code></pre>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#send-test-alerts","title":"Send test alerts","text":"<p>You can send test alerts to an Alertmanager instance to test alerting.</p> <pre><code>kubectl port-forward -n monitoring-system alertmanager-kube-prometheus-stack-alertmanager-0 9093 &amp;\n\ncurl -H \"Content-Type: application/json\" -d '[{\"labels\":{\"alertname\":\"myalert\"}}]' localhost:9093/api/v1/alerts\n</code></pre>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#adding-alert-rules","title":"Adding alert rules","text":"<p>If you want to configure additional alerts, you need to add PrometheusRules resources. The example below will generate an alert if the ServiceMonitor with the name your-service-monitor-name has less than one target up.</p> <p>Deploy those resources together with your application in e.g. a Helm chart so that they are bundled together and nicely versioned.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    prometheus: kube-prometheus-stack-prometheus\n    role: alert-rules\n  name: your-application-name\nspec:\n  groups:\n    - name: \"your-application-name.rules\"\n      rules:\n        - alert: PodDown\n          for: 1m\n          expr: sum(up{job=\"your-service-monitor-name\"}) &lt; 1 or absent(up{job=\"your-service-monitor-name\"})\n          annotations:\n            message: The deployment has less than 1 pod running.\n</code></pre>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#adding-grafana-dashboards","title":"Adding Grafana dashboards","text":"<p>If you want to deploy additional Grafana dashboards, we recommend adding a ConfigMap or Secret with the label grafana_dashboard=1. The ConfigMap or Secret does not have to be in the same namespace as Grafana and can be deployed together with your application or service.</p> <p>Although you can create dashboards in the Grafana user interface, they are not persistent in the default configuration. You can enable persistence, but then you also have to set the number of replicas to 1, which has the disadvantage of losing availability. We strongly recommend not to do this or to set up an external database for storing dashboards as described in the Grafana documentation.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    grafana_dashboard: \"1\"\n  name: new-dashboard-configmap\ndata:\n  new-dashboard.json: |-\n    {\n      \"id\": null,\n      \"uid\": \"cLV5GDCkz\",\n      \"title\": \"New dashboard\",\n      \"tags\": [],\n      \"style\": \"dark\",\n      \"timezone\": \"browser\",\n      \"editable\": true,\n      \"hideControls\": false,\n      \"graphTooltip\": 1,\n      \"panels\": [],\n      \"time\": {\n        \"from\": \"now-6h\",\n        \"to\": \"now\"\n      },\n      \"timepicker\": {\n        \"time_options\": [],\n        \"refresh_intervals\": []\n      },\n      \"templating\": {\n        \"list\": []\n      },\n      \"annotations\": {\n        \"list\": []\n      },\n      \"refresh\": \"5s\",\n      \"schemaVersion\": 17,\n      \"version\": 0,\n      \"links\": []\n    }\n</code></pre> <p>For more possibilities to deploy Grafana dashboards, have a look at the upstream helm chart repo.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#make-grafana-available-via-an-ingress","title":"Make Grafana available via an ingress","text":"<p>Requirements: - Ingress Controller, for example with the SysEleven Building Block - Optional: cert-manager and external-dns</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#configuring-ingress","title":"Configuring Ingress","text":"<p>Add to the matching values file:</p> <pre><code>grafana:\n  ingress:\n    enabled: true\n    annotations:\n      cert-manager.io/cluster-issuer: \"letsencrypt-production\"\n    hosts:\n      - grafana.example.com\n    tls:\n      - secretName: grafana.example.com-tls\n        hosts:\n          - grafana.example.com\n  grafana.ini:\n    server:\n      root_url: https://grafana.example.com\n</code></pre>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#monitoring","title":"Monitoring","text":"<p>The building blocks (in this case kube-prometheus-stack itself) come with a set of predefined alert rules and Grafana dashboards. Alert rules and dashboards are synchronized by kubernetes-mixin. This includes basic monitoring of the local Kubernetes cluster itself (e.g. resource limits/requirements, pod crash loops, API errors, ...). The Ubiquity community may add additional alertrules and dashboards to each component.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#additional-alertrules","title":"Additional alertrules","text":"<p>As an example, alert rules have been created in the directory kube-prometheus-stack-extension/templates/alerts. Add your own alert rules to the existing files or create your own files in the directory with your alert rules.</p> <p>Each alert rule should include a meaningful description as an annotation.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#additional-grafana-dashboards","title":"Additional Grafana dashboards","text":""},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#alerts","title":"Alerts","text":"<p>An overview of firing prometheus alerts</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#cluster-capacity","title":"Cluster Capacity","text":"<p>An overview of the capacity of the local kubernetes cluster</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#scale-prometheus-persistent-volumes","title":"Scale prometheus persistent volumes","text":"<pre><code># Set replicas to 0 and PVC template to new value\nkubectl edit prometheuses\n\n# Patch PVC (e.g. 100Gi)\nkubectl patch pvc prometheus-kube-prometheus-stack-prometheus-db-prometheus-kube-prometheus-stack-prometheus-0 --namespace syseleven-managed-kube-prometheus-stack -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"100Gi\"}}}}' --type=merge\nkubectl patch pvc prometheus-kube-prometheus-stack-prometheus-db-prometheus-kube-prometheus-stack-prometheus-1 --namespace syseleven-managed-kube-prometheus-stack -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"100Gi\"}}}}' --type=merge\n\n# Verify pending resize\nkubectl describe pvc prometheus-kube-prometheus-stack-prometheus-db-prometheus-kube-prometheus-stack-prometheus-0\n\n# Scale replicas back\nkubectl edit prometheuses\n\n# Commit the changes to the values*.yaml files.\nScaling setup\nThis building block consists of multiple components. Each of the components can and must be scaled individually.\n</code></pre>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#scaling-prometheus","title":"Scaling prometheus","text":""},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#prometheus-operator","title":"prometheus-operator","text":"<p>Usually should only be run with replicas=1 Requests/limits for CPU/memory can be adjusted</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#prometheus-node-exporter","title":"prometheus-node-exporter","text":"<p>Runs as DaemonSet on each node, so no further replica scaling needed Requests/limits for CPU/memory can be adjusted</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#kube-state-metrics","title":"kube-state-metrics","text":"<p>Usually should only be run with replicas=1 Requests/limits for CPU/memory can be adjusted</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#prometheus","title":"prometheus","text":"<p>Also see Prometheus High Availability for upstream documentation Replicas can be increased, though each replica will be a dedicated prometheus that scrapes everything Requests/limits for CPU/memory can be adjusted</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#scaling-alertmanager","title":"Scaling alertmanager","text":"<p>Also see Alertmanager High Availability for upstream documentation.</p> <p>Replicas can be increased to achieve higher availability New replicas will automatically join the alertmanager cluster Requests/limits for CPU/memory can be adjusted</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#scaling-grafana","title":"Scaling grafana","text":"<p>Also see Set up Grafana for high availability for upstream documentation.</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#replicas","title":"Replicas","text":"<p>Dashboards - when increasing replicas for grafana it is important to think about where you want the dashboards/user configuration to come from. By default, we run with replicas=2 but do not save dashboards locally. So after respawn the dashboards are gone if they are not automated by supplying them as ConfigMap.</p> <p>Sessions - when increasing replicas for grafana it is important to think about where you want to store user sessions. We configuresticky sessions by default, so each user is mapped to a specific replica until the replica is not available anymore.</p> <p>Requests/limits for CPU/memory can be adjusted</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#release-notes","title":"Release-Notes","text":"<p>Please find more infos on release notes and new features Release notes Prometheus-Stack</p>"},{"location":"admin-guide/administration/tutorials/kube-prometheus-stack/#known-issues","title":"Known Issues","text":"<p>For Kubernetes &lt;= 1.24, when upgrading  there will be errors in the diff stage of the CI pipeline. This is due to a bug in the helm chart. The upgrade will still work, but the CI pipeline will fail. This will be fixed in the next release.</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/","title":"Loki-Promtail","text":""},{"location":"admin-guide/administration/tutorials/loki-promtail/#prerequisites-on-loki-promtail","title":"Prerequisites on loki promtail","text":"<p>You need to provide a storage provider for this component. Proceed with the following prerequisite description to use the Velero Building Block out of the box.</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#recommended-setup","title":"Recommended setup","text":"<p>A recommended resource overview is listed in the table below.</p> <p>CPU/vCPU    Memory none    512MiB + 256 per node No further activities need to be carried out in advance.</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#adding-loki-promtail-to-your-cluster","title":"Adding Loki-Promtail to your cluster","text":"<p>Add the directory loki to your system directory within your repository. </p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#required-configuration","title":"Required configuration","text":"<p>No confguration is required.</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#what-logs-are-collected","title":"What logs are collected?","text":"<p>The building block consists of two parts: Loki and Promtail. Promtail runs as a DaemonSet on each node in the cluster and collects logs. Loki is running as a central instance in the cluster and stores the logs it receives from Promtail. When looking into the logs, you usually only interact with Loki. The building block collects stdout and stderr of all pods in the cluster. In addition to the pods deployed by the SysEleven provided building blocks, this also includes the logs of all application pods deployed by the user. In addition to the pod logs the systemd journals of all nodes in the cluster are collected. By default the logs are stored by Loki for one week. The retention period can be adjusted in values.yaml.</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#how-can-i-access-the-logs","title":"How can I access the logs?","text":"<p>The easiest way to access the logs is Grafana provided with the kube-stack-prometheus module. We automatically create a Loki data source for you in it, so you can skip this step in the linked upstream documentation. In Grafana, you can use the explore feature to take a look at your logs and refine your query. For something more permanent, you can add a log panel to one of your dashboards.</p> <p>If you prefer the command line you can use logcli.</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#monitoring","title":"Monitoring","text":""},{"location":"admin-guide/administration/tutorials/loki-promtail/#additional-alertrules","title":"Additional Alertrules","text":"<p>None</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#additional-grafana-dashboards","title":"Additional Grafana dashboards","text":"<p>Loki Top 10 producer An overview of the top 10 kubernetes namespaces that produce logs b</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#loki-promtail_1","title":"Loki &amp; Promtail","text":"<p>An overview of performance metrics from Loki and Promtail</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#scale-loki-volume","title":"Scale loki volume","text":"<p>If you need to scale the persistent volume Loki stores your data on, perform the following steps:</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#delete-statefulset-the-pvc-will-remain","title":"Delete StatefulSet (the PVC will remain)","text":"<p>kubectl delete sts loki</p>"},{"location":"admin-guide/administration/tutorials/loki-promtail/#patch-the-pvc-to-eg-10gi","title":"Patch the PVC to e.g. 10Gi","text":"<p>kubectl patch pvc storage-loki-0 -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"10Gi\"}}}}' --type=merge Then, adapt your adapt values.yaml, e.g. values-loki-stage.yaml for the same size as the patch above.</p> <p>To deploy the StatefulSet again, push your changes and merge them to the default branch - the CI will then deploy it again.</p> <p>Scale Setup This building block consists of multiple components. Each of the components can and must be scaled individually.</p> <p>Scaling Loki Also see Scaling with Loki for upstream documentation.</p> <p>Scaling replicas is not supported with this building block. See loki-distributed for a possible solution. Requests/limits for CPU/memory can be adjusted Scaling Promtail Runs as one DaemonSet on each node, so no further replica scaling is needed Requests/limits for CPU/memory can be adjusted Release-Notes Please find more infos on release notes and new features Release notes Loki-Promtail</p>"},{"location":"admin-guide/administration/tutorials/manual-dns-setup/","title":"Manual DNS setup","text":"<p>Info</p> <pre><code>Skip this step if you already use the included Cloudflare setup\n</code></pre> <p>Before you can access the home page at https://home.example.com, you'll need to update your DNS config.</p> <p>Some options for DNS config (choose one):</p> <ul> <li>Change the DNS config at your domain registrar (already included and automated)</li> <li>Change the DNS config in your router (also works if you don't own a domain)</li> <li>Use nip.io (suitable for a test environment)</li> </ul>"},{"location":"admin-guide/administration/tutorials/manual-dns-setup/#at-your-domain-registrar-recommended","title":"At your domain registrar (recommended)","text":"<p>The default configuration is for Cloudflare DNS, but you can change the code to use other providers.</p>"},{"location":"admin-guide/administration/tutorials/manual-dns-setup/#in-your-router","title":"In your router","text":"<p>Tip</p> <p>If you don't have a domain, you can use the <code>home.arpa</code> domain (according to RFC-8375).</p> <p>You can add each subdomain one by one, or use a wildcard <code>*.example.com</code> and point it to the IP address of the load balancer. To acquire a list of subdomains and their addresses, use this command:</p> <pre><code>./scripts/get-dns-config\n</code></pre>"},{"location":"admin-guide/administration/tutorials/manual-dns-setup/#use-nipio","title":"Use nip.io","text":"<p>Preconfigured in the <code>dev</code> branch.</p>"},{"location":"admin-guide/administration/tutorials/run-commands-on-multiple-nodes/","title":"Run commands on multiple nodes","text":"<p>Use ansible-console:</p> <pre><code>cd metal\nmake console\n</code></pre> <p>Then enter the command(s) you want to run.</p> <p>Example</p> <p><code>root@all (4)[f:5]$ uptime</code></p> <pre><code>metal0 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.17, 0.15, 0.06\nmetal1 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.14, 0.11, 0.04\nmetal3 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.03, 0.02, 0.00\nmetal2 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.06, 0.06, 0.02\n</code></pre>"},{"location":"admin-guide/administration/tutorials/shutdown/","title":"How to shutdown a Ubiquity cluster","text":""},{"location":"admin-guide/administration/tutorials/shutdown/#situation","title":"Situation","text":""},{"location":"admin-guide/administration/tutorials/shutdown/#task","title":"Task","text":"<p>This article provides instructions for safely shutting down a Ubiquity cluster provisioned.</p>"},{"location":"admin-guide/administration/tutorials/shutdown/#requirements","title":"Requirements","text":"<ul> <li>A Ubiquity cluster</li> </ul>"},{"location":"admin-guide/administration/tutorials/shutdown/#background","title":"Background","text":"<p>If you have a need to shut down the infrastructure running a Ubiquity cluster (datacentre maintenance, migration, etc.) this guide will provide steps in the proper order to ensure a safe cluster shutdown.</p> <p>Please ensure you complete an etcd backup before continuing this process. A guide regarding the backup and restore process can be found here.</p>"},{"location":"admin-guide/administration/tutorials/shutdown/#solution","title":"Solution","text":"<p>N.B. If you have nodes that share worker, control plane, or etcd roles, postpone the docker stop and shutdown operations until worker or control plane containers have been stopped.</p>"},{"location":"admin-guide/administration/tutorials/shutdown/#turning-off-any-bare-metal-services","title":"Turning off any bare-metal services","text":"<p>If you deployed your cluster using bare-metal services (i.e. not hybrid pod or k8s-native), then you are responsible for disabling and re-enabling those services. You can do that manually by shutting those respective services down cleanly.</p>"},{"location":"admin-guide/administration/tutorials/shutdown/#bare-metal-service-slurm","title":"Bare-Metal Service: Slurm","text":"<p>Shut down Slurm following best practices:</p> <ul> <li>Place a maintenance reservation on the cluster in question, to prevent any new jobs running. Change dates/times and duration where necessary for your outage window. The duration is in minutes: <pre><code>scontrol create reservation starttime=2023-09-15T10:30:00 duration=5760 user=root flags=maint,ignore_jobs nodes=ALL\n</code></pre></li> </ul> <p>Confirm the reservation is in-place: <pre><code>scontrol show reservations\n</code></pre> Ensure no jobs are running:</p> <pre><code>squeue\n</code></pre> <p>Then proceed to shut down the service accordingly:</p> <ul> <li>Shut down slurmctld on both nodes: <pre><code>systemctl stop slurmctld\n</code></pre></li> </ul> <p>Wait for a minute for any writes from slurmctld to have finished to the slurmdbd instance.</p> <ul> <li>Shut down slurmdbd on both nodes: <pre><code>systemctl stop slurmdbd\n</code></pre></li> </ul> <p>Again, wait for a minute for any writes to the MySQL database to be finished</p> <ul> <li> <p>Then shutdown MySQL on both nodes: <pre><code>systemctl stop mysqld\n</code></pre></p> </li> <li> <p>Confirm that mysqld is off:</p> </li> </ul> <pre><code>systemctl status mysqld\n</code></pre> <p>At which point you are ok to shutdown slurmd on the remaining compute nodes (but this step is not essential)</p> <pre><code>systemctl stop slurmd\n</code></pre>"},{"location":"admin-guide/administration/tutorials/shutdown/#bare-metal-service-htcondor","title":"Bare-Metal Service: HTCondor","text":""},{"location":"admin-guide/administration/tutorials/shutdown/#draining-worker-nodes","title":"Draining Worker Nodes","text":"<p>For all worker nodes, prior to stopping the containers, run:</p> <pre><code>kubectl get nodes\n</code></pre> <p>To identify the desired node, then run: <pre><code>kubectl drain &lt;node name&gt;\n</code></pre></p> <p>This will safely evict any pods, and you can proceed with the following steps to a shutdown.</p>"},{"location":"admin-guide/administration/tutorials/shutdown/#shutting-down-the-workers-nodes","title":"Shutting down the workers nodes","text":"<p>For each worker node:</p> <ul> <li>ssh into the worker node</li> <li>stop kubelet and kube-proxy by running <code>sudo docker stop kubelet kube-proxy</code></li> <li>stop docker by running <code>sudo service docker stop</code> or <code>sudo systemctl stop docker</code></li> <li>shutdown the system <code>sudo shutdown now</code></li> </ul>"},{"location":"admin-guide/administration/tutorials/shutdown/#shutting-down-the-control-plane-nodes","title":"Shutting down the control plane nodes","text":"<p>For each control plane node:</p> <ul> <li>ssh into the control plane node</li> <li>stop kubelet and kube-proxy by running sudo docker stop kubelet kube-proxy</li> <li>stop kube-scheduler and kube-controller-manager by running sudo docker stop kube-scheduler kube-controller-manager</li> <li>stop kube-apiserver by running sudo docker stop kube-apiserver</li> <li>stop docker by running sudo service docker stop or sudo systemctl stop docker</li> <li>shutdown the system sudo shutdown now</li> </ul>"},{"location":"admin-guide/administration/tutorials/shutdown/#shutting-down-the-etcd-nodes","title":"Shutting down the etcd nodes","text":"<p>For each etcd node:</p> <ul> <li>ssh into the etcd node</li> <li>stop kubelet and kube-proxy by running sudo docker stop kubelet kube-proxy</li> <li>stop etcd by running sudo docker stop etcd</li> <li>stop docker by running sudo service docker stop or sudo systemctl stop docker</li> <li>shutdown the system sudo shutdown now</li> </ul>"},{"location":"admin-guide/administration/tutorials/shutdown/#shutting-down-storage","title":"Shutting down storage","text":"<p>Shut down any persistent storage devices that you might have in your datacenter (such as NAS storage devices) if applicable. It iss important that you do this after shutting everything else down to prevent data loss/corruption for containers requiring persistency.</p> <p>N.B. If you are running a cluster that was not deployed through RKE then the order of the process is still the same, however the commands may vary. For instance, some distributions run kubelet and other control plane items as a service on the node rather than in docker. Check documentation for the specific Kubernetes distribution for information as to how to stop these services.</p>"},{"location":"admin-guide/administration/tutorials/shutdown/#starting-a-kubernetes-cluster-up-after-shutdown","title":"Starting a Kubernetes cluster up after shutdown","text":"<p>Kubernetes is good about recovering from a cluster shutdown and requires little intervention, though there is a specific order in which things should be powered back on to minimize errors.</p> <p>Power on any storage devices if applicable.</p> <p>Check with your storage vendor on how to properly power on you storage devices and verify that they are ready.</p> <p>For each etcd node: - Power on the system/start the instance. - Log into the system via ssh. - Ensure docker has started sudo service docker status or sudo systemctl status docker - Ensure etcd and kubelet\u2019s status shows Up in Docker sudo docker ps</p> <p>For each control plane node: - Power on the system/start the instance. - Log into the system via ssh. - Ensure docker has started sudo service docker status or sudo systemctl status docker - Ensure kube-apiserver, kube-scheduler, kube-controller-manager, and kubelet\u2019s status shows Up in Docker sudo docker ps</p> <p>For each worker node: - Power on the system/start the instance. - Log into the system via ssh. - Ensure docker has started sudo service docker status or sudo systemctl status docker - Ensure kubelet\u2019s status shows Up in Docker sudo docker ps - Log into K9s (or use kubectl) and check your various projects to ensure workloads have started as expected. This may take a few minutes depending on the number of workloads and your server capacity.</p>"},{"location":"admin-guide/administration/tutorials/single-node-cluster-adjustments/","title":"Single node cluster adjustments","text":"<p>Update the following changes, then commit and push.</p>"},{"location":"admin-guide/administration/tutorials/single-node-cluster-adjustments/#reduce-longhorn-replica-count","title":"Reduce Longhorn replica count","text":"<p>Set the <code>defaultClassReplicaCount</code> to 1:</p> system/longhorn-system/values.yaml<pre><code>---\n# Copyright The Ubiquity Authors.\n#\n# Licensed under the Apache License, Version 2.0. Previously licensed under the Functional Source License (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://github.com/ubiquitycluster/ubiquity/blob/main/LICENSE\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# This software was previously licensed under the Functional Source License but has now transitioned to an Apache 2.0 License\n# as of June 2025.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nlonghorn:\n#  global:\n#    nodeSelector:\n#      node-role.kubernetes.io/master: \"true\"\n  annotations:\n    prometheus.io/scrape: 'true'\n  defaultSettings:\n    defaultReplicaCount: 3\n    disableSchedulingOnCordonedNode: true\n    nodeDownPodDeletionPolicy: delete-both-statefulset-and-deployment-pod\n    replicaAutoBalance: best-effort\n    replicaSoftAntiAffinity: false\n    storageMinimalAvailablePercentage: 10\n    taintToleration: StorageNode=true:PreferNoSchedule\n  ingress:\n    annotations:\n      cert-manager.io/cluster-issuer: pebble-issuer\n      hajimari.io/appName: Longhorn\n      hajimari.io/icon: harddisk\n    enabled: true\n    host: longhorn.ubiquitycluster.uk\n    ingressClassName: nginx\n    path: /\n    secureBackends: true\n    tls: true\n    tlsSecret: longhorn-local-tls\n#  csi:\n#    attacher:\n#      nodeSelector:\n#        node-role.kubernetes.io/master: \"true\"\n  longhornRecoveryBackend:\n    replicas: 1\n#    nodeSelector: \n#      node-role.kubernetes.io/master: \"true\"\n  longhornAdmissionWebhook:\n    replicas: 3\n#    nodeSelector:\n#      node-role.kubernetes.io/master: \"true\"\n  longhornConversionWebhook:\n    replicas: 3\n#    nodeSelector:\n#      node-role.kubernetes.io/master: \"true\"\n  longhornDriver:\n#    nodeSelector:\n#      node-role.kubernetes.io/master: \"true\"\n    tolerations:\n    - effect: PreferNoSchedule\n      key: StorageNode\n      operator: Equal\n      value: 'true'\n  longhornManager:\n#    nodeSelector:\n#      node-role.kubernetes.io/master: \"true\"\n    tolerations:\n    - effect: PreferNoSchedule\n      key: StorageNode\n      operator: Equal\n      value: 'true'\n  longhornUI:\n    nodeSelector:\n      node-role.kubernetes.io/master: \"true\"\n    tolerations:\n    - effect: PreferNoSchedule\n      key: StorageNode\n      operator: Equal\n      value: 'true'\n  persistence:\n    defaultClass: true\n    defaultClassReplicaCount: 3\n    defaultFsType: ext4\n    defaultMkfsParams: ''\n    defaultReplicaAutoBalance: best-effort\n    replicaSoftAntiAffinity: false\n</code></pre>"},{"location":"admin-guide/administration/tutorials/single-node-cluster-adjustments/#disable-automatic-upgrade-for-os-and-k3s","title":"Disable automatic upgrade for OS and k3s","text":"<p>Because they will try to drain the only node, the pods will have no place to go. Remove them entirely:</p> <pre><code>mv system/kured disabled/system/kured\nmv system/system-upgrade disabled/system/system-upgrade\n</code></pre> <p>Commit and push the change. You can revert it later when you add more nodes by just moving them back:</p> <pre><code>mv disabled/system/kured system/kured\nmv disabled/system/system-upgrade system/system-upgrade\n</code></pre> <p>This process of enabling/disabling is the same for all the other components and makes it easy to add/remove them.</p>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/","title":"How to Change an SSL cert for your upstream git repository","text":""},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#situation","title":"Situation","text":"<p>Your git repo that you host your cluster configuration on needs to have an SSL certificate change. As all of the GitOps nature of the project requires that ArgoCD can read a git repo, it needs to be aware that an SSL certificate is needed - And if it's a self-cert SSL key then it needs to be created in the right format and applied.</p>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#task","title":"Task","text":"<p>This article provides instructions for safely applying a key to the Ubiquity ArgoCD instance.</p>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#requirements","title":"Requirements","text":"<ul> <li>A Ubiquity cluster</li> <li>A (valid) SSL key for your git repo domain (see next for detail)</li> </ul>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#background","title":"Background","text":"<p>If you have a need to replace a key to your git infrastructure that is read by a Ubiquity cluster, this guide will provide steps in the proper order to ensure a painless process.</p> <p>Please ensure you complete a backup of your old SSL key before continuing this process.</p>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#solution","title":"Solution","text":"<p>The following steps should be undertaken:</p>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#create-a-valid-ssl-key","title":"Create a valid SSL key","text":"<p>An extension to the x509 standard, the <code>subjectAltName</code> is used by ArgoCD to validate that a server matches the host. Please ensure this is present. In this example, we are creating a self-signed certificate using an existing CSR. For more information on creating an SSL certificate, CSRs etc please see the official OpenSSL documentation.</p> <p>N.B. it is important to note that you need an up-to-date version of OpenSSL. Version 1.1.1 or greater is appropriate as of this guide.</p> <p>To generate the key in this example (an example of updating a gitlab cert), we specify the <code>-addext</code> functionality present inside OpenSSL 1.1.1:</p> <pre><code>openssl req -new -nodes -x509 -subj \"/C=UK/ST=Example/L=Example/O=ExampleOrg/CN=example.com\" -days 3650 -keyout /etc/gitlab/ssl/gitlab.key -out /etc/gitlab/ssl/gitlab.crt -extensions v3_ca -addext \"subjectAltName = DNS:example.com\"\n</code></pre> <p>This process will give you a certificate with a subjectAltName field defined. You can verify this using:</p> <pre><code>openssl x509 -noout -text -in /etc/gitlab/ssl/gitlab.crt | grep DNS:\n</code></pre> <p>And you should see your DNS for example.com in that DNS field.</p>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#applying-the-key","title":"Applying the key","text":"<p>Once it is applied to GitLab (other git repository hosting is available), then from there you only need to update the yaml in the git repository, and apply the new certificate live. To do this:</p> <p>Edit the <code>bootstrap/argocd/argocd-&lt;repo&gt;-tls-cert.yaml</code> file - This file has a copy of the pem present that is stored in a configmap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-tls-certs-cm\n  namespace: argocd\n  labels:\n    app.kubernetes.io/name: argocd-cm\n    app.kubernetes.io/part-of: argocd\ndata:\n  example.com: |\n    -----BEGIN CERTIFICATE-----\n    &lt;cert information&gt;\n    -----END CERTIFICATE-----\n</code></pre> <p>Edit this and put the appropriate key in-place for the correct site that you generated.</p> <p>Then:</p> <pre><code>git commit -am \"Updated TLS cert for git repository\"\ngit push &lt;remote that you use, if origin you can simply use git push&gt;\n</code></pre> <p>Once this is completed, to help it take effect faster you can then apply that configmap straight away. </p>"},{"location":"admin-guide/administration/tutorials/ssl-cert-change-git/#opus-actions","title":"OPUS actions","text":"<p>To apply straight away, start up an OPUS container, go into your <code>bootstrap/argocd/</code> directory and apply the manifest manually:</p> <pre><code>kubectl -n argocd apply -f argocd-&lt;repo&gt;-tls-cert.yaml\n</code></pre> <p>This will apply live on the system.</p> <p>Then the final stage is to restart the applicationset controller in order for it to re-read all of the git repositories (we made our change to the git repo before this to prevent argocd reverting the live change to a configmap we just did). Using K9s, restarting a pod is a simple process. We go into the pods view, filter for argocd-applicationset-controller, and then ctrl-d to delete the pod - Because a statefulset exists for it, it will simply restart the pod:</p> <pre><code>k9s\n\n:pods\n\n/argocd-applicationset-controller\n\nctrl-d\n</code></pre> <p>Once the pod restarts, you can then also go and tail the logs of that pod by pressing the <code>l</code> hotkey.</p>"},{"location":"admin-guide/administration/tutorials/updating-documentation/","title":"Updating documentation (this website)","text":"<p>This project uses the Di\u00e1taxis technical documentation framework. The website is generated using Material for MkDocs and can be viewed at docs.ubiquitycluster.uk.</p> <p>There are 4 main parts:</p> <ul> <li>Getting started (tutorials): learning-oriented</li> <li>Concepts (explanation): understanding-oriented</li> <li>How-to guides: goal-oriented</li> <li>Reference: information-oriented</li> </ul>"},{"location":"admin-guide/administration/tutorials/updating-documentation/#local-development","title":"Local development","text":"<p>To edit and view locally, run:</p> <pre><code>make docs\n</code></pre> <p>Then visit localhost:8000</p>"},{"location":"admin-guide/administration/tutorials/updating-documentation/#deployment","title":"Deployment","text":"<p>It's running on my other cluster in the cjcshadowsan/ubiquity project (so if Devin goes down everyone can still read the documentation).</p>"},{"location":"admin-guide/administration/tutorials/use-both-github-and-gitea/","title":"Use both GitHub and Gitea","text":"<p>Even though we self-host Gitea, you may still want to use GitHub as a backup and for discovery.</p> <p>Add both push URLs (replace my repositories with yours):</p> <pre><code>git remote set-url --add --push origin git@git.ubiquitycluster.uk:ops/ubiquity\ngit remote set-url --add --push origin git@github.com:cjcshadowsan/ubiquity\n</code></pre> <p>Now you can just run <code>git push</code> like usual and it will push to both GitHub and Gitea.</p>"},{"location":"admin-guide/administration/tutorials/user-accounts/","title":"Keycloak \u2013 Create Realm, Client, Roles, and User","text":"<p>Keycloak is Open Source Identity and Access Management (IAM) tool developed by Red Hat. By using this you can add authentication to applications and secure services with minimum effort. No need to deal with storing users or authenticating users. Keycloak provides user federation, strong authentication, user management, fine-grained authorization, and more.</p> <p>Note: Keycloak is already installed on your machine. It is present at <code>https://keycloak.&lt;cluster-domain&gt;/</code> URL.</p> <p>In this guide, we will see how to Create a Realm, Client, and User in Keycloak.</p>"},{"location":"admin-guide/administration/tutorials/user-accounts/#creating-realm-in-keycloak","title":"Creating Realm in Keycloak","text":"<p>Realm is a management entity that controls a set of users, their credentials, roles, and groups. A user belongs to and logs into a realm. In the context of Keycloak, a realm refers to a security and administrative domain where users, applications, and roles are managed. It is a fundamental concept in Keycloak\u2019s architecture that allows you to isolate and organize resources, permissions, and configurations. Now let\u2019s see how to create a Realm in Keycloak.</p> <p>Step 1: Go to the Master drop-down menu, and click on the Create Realm button.</p> <p></p> <p>Step 2: Then provide your realm name and clock on the Create button and you are done with creating Realm in Keycloak. Remember that avoid using the master realm and create your own realm.</p> <p></p>"},{"location":"admin-guide/administration/tutorials/user-accounts/#creating-client-in-keycloak","title":"Creating Client in Keycloak","text":"<p>In Keycloak, a client refers to an application or service that interacts with the Keycloak server for authentication and authorization purposes. It can be a web application, a mobile app, a server-side API, or any other type of application that needs to authenticate and authorize its users. Now let\u2019s see how to create a Client in Keycloak.</p> <p>Step 1: To create a new client, click on the Clients menu from the left pane and click the Create Client button. Make sure you have chosen your realm instead of the Master realm.</p> <p></p> <p>Step 2: Then you will be prompted for a Client type, a Client ID, and other basic info. Keep the client type as same as OpenID Connect. You can give a name and description to the client as per your choice.</p> <p></p> <p>After clicking Next in the Capability config keep the default config. Then click next. In the Login settings, you have to provide the Valid Redirect URIs field. Here you have to provide the port in which your application is running.</p> <p>Note: Please don\u2019t provide 8080 as your Keycloak is running on this port.</p> <p></p> <p>And you are done with creating a Client in Keycloak.</p>"},{"location":"admin-guide/administration/tutorials/user-accounts/#creating-roles-in-keycloak","title":"Creating Roles in Keycloak","text":"<p>In Keycloak, roles are used to define and manage permissions and access levels for users and clients within a realm. Roles provide a way to control and enforce authorization policies, allowing you to specify what users or clients are allowed to do within your system. There are two main types of roles in Keycloak:</p> <ul> <li>Realm Roles: These roles are defined at the realm level and are available across all clients within that realm. Realm roles are typically used for global permissions that apply to all applications within the realm. For example, you can create roles like \u201cadmin,\u201d \u201cuser,\u201d or \u201cmanager\u201d at the realm level.</li> <li>Client Roles: These roles are specific to individual client applications within the realm. Each client can have its own set of roles that define access permissions specific to that application. For example, for a social media application, you might have roles like \u201cpost,\u201d \u201ccomment,\u201d or \u201clike\u201d at the client level.</li> </ul>"},{"location":"admin-guide/administration/tutorials/user-accounts/#creating-realm-roles","title":"Creating Realm Roles","text":"<p>Step 1: To create a new Realm Roles, click on the Realm roles menu from the left pane and click the Create role button.</p> <p></p> <p>Step 2: In the next screen provide the role name as per your requirements.</p> <p></p>"},{"location":"admin-guide/administration/tutorials/user-accounts/#creating-client-roles","title":"Creating Client Roles","text":"<p>Step 1: To create a new Client Role, navigate to clients from the left panel. Then select your client created by you. Then click Roles Press the Create role button.</p> <p></p> <p>Step 2: In the next screen provide the role name as per your requirements.</p> <p></p>"},{"location":"admin-guide/administration/tutorials/user-accounts/#creating-a-user-in-keycloak","title":"Creating a User in Keycloak","text":"<p>In Keycloak, a user represents an individual entity that can authenticate and interact with the Keycloak server. Users can be individuals who need to access applications or services secured by Keycloak or administrators who manage the Keycloak realm and its configurations.</p> <p>Step 1: In the Users page, click on Add user button.</p> <p></p> <p>Step 2: On the next page provide the required details of a User and click on Create button.</p> <p></p> <p>Step 3: Now we need to set a password for the user. Click the Credentials tab and press the Set password button.</p> <p></p> <p>Step 4: Now we need to set a role for the user. Click the Role mapping tab and press Assign role.</p> <p></p> <p>Now the Realm roles list will be available in the table. Check the required role and click Assign to map it to the respective user.</p> <p></p> <p>And you are done with creating a User.</p> <p>Whether you're preparing for your first job interview or aiming to upskill in this ever-evolving tech landscape, GeeksforGeeks Courses are your key to success. We provide top-quality content at affordable prices, all geared towards accelerating your growth in a time-bound manner. Join the millions we've already empowered, and we're here to do the same for you. Don't miss out - check it out now!</p>"},{"location":"admin-guide/concepts/certificate-management/","title":"Certificate management","text":"<p>Certificates are generated and managed by cert-manager with Let's Encrypt. By default certificates are valid for 90 days and will be renewed after 60 days.</p> <p>cert-manager watches <code>Ingress</code> resources across the cluster. When you create an <code>Ingress</code> with a supported annotation:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  name: foo\nspec:\n  rules:\n    - host: foo.example.com\n      # ...\n  tls:\n    - hosts:\n        - foo.example.com\n      secretName: foo-tls-certificate\n</code></pre> <pre><code>flowchart LR\n  User -- 6 --&gt; Ingress\n\n  subgraph cluster[Ubiquity cluster]\n    Ingress --- Secret\n    Ingress -. 1 .-&gt; Certificate\n    Certificate -. 5 .-&gt; Secret\n    Certificate -- 2 --&gt; CertificateRequest -- 3 --&gt; Order -- 4 --&gt; Challenge\n  end\n\n  Order -.- ACMEServer[ACME server]\n\n  subgraph dnsprovider[DNS provider]\n    TXT\n  end\n\n  Challenge -- 4.a --&gt; TXT\n  ACMEServer -.- Challenge\n  ACMEServer -. 4.b .-&gt; TXT</code></pre> <ol> <li>cert-manager creates a corresponding <code>Certificate</code> resources</li> <li>Based on the <code>Certificate</code> resource, cert-manager creates a <code>CertificateRequest</code> resource to request a signed certificate from the configured <code>ClusterIssuer</code></li> <li>The <code>CertificateRequest</code> will create an order with an ACME server (we use Let's Encrypt), which is represented by the <code>Order</code> resource</li> <li>Then cert-manager will perform a DNS-01 <code>Challenge</code>:<ol> <li>Create a DNS TXT record (contains a computed key)</li> <li>The ACME server retrieve this key via a DNS lookup and validate that we own the domain for the requested certificate</li> </ol> </li> <li>cert-manager stores the certificate (typically <code>tls.crt</code> and <code>tls.key</code>) in the <code>Secret</code> specified in the <code>Ingress</code> configuration</li> <li>Now you can access the HTTPS website with a valid certificate</li> </ol> <p>A much more detailed diagram can be found in the official documentation under certificate lifecycle.</p>"},{"location":"admin-guide/concepts/opus-container/","title":"OPUS container","text":"<p>The OPUS container makes it easy to get all of the dependencies needed to interact with a Ubiquity cluster.</p>"},{"location":"admin-guide/concepts/opus-container/#how-to-open-it","title":"How to open it","text":"<p>You can use the default Docker wrapper, or use Nix if you have Nix installed:</p> DockerNix <pre><code>make tools\n</code></pre> <pre><code>nix develop\n</code></pre> <p>Tip</p> <p>If you have <code>direnv</code> installed, you can run <code>direnv allow</code> once and it will automatically enter the Nix shell.</p> <p>It will open a shell like this:</p> <pre><code>[khuedoan@ryzentower:~/Documents/homelab]$ echo hello\nhello\n</code></pre>"},{"location":"admin-guide/concepts/opus-container/#how-it-works","title":"How it works","text":"<ul> <li>All dependencies are defined in <code>./flake.nix</code></li> <li>When you run <code>make tools</code>, it will run a thin Docker wrapper with the <code>nixos/nix</code> image (because not everyone has Nix installed) and mount some required volumes</li> <li><code>nix develop</code> will start an interactive shell based on the Nix expression in <code>./flake.nix</code> and install everything from there</li> </ul>"},{"location":"admin-guide/concepts/opus-container/#known-issues","title":"Known issues","text":"<ul> <li>If your Docker engine is not running in rootless mode, all files created by the tools container will be owned by <code>root</code></li> </ul>"},{"location":"admin-guide/concepts/pxe-boot/","title":"PXE boot","text":"<pre><code>flowchart TD\n  subgraph controller[Initial controller]\n    Ansible\n    dhcp[DHCP server]\n    tftp[TFTP server]\n    http[HTTP server]\n  end\n\n  machine[Bare metal machine]\n\n  Ansible -. 1 .-&gt; machine\n  machine &lt;-. 2, 3 .-&gt; dhcp\n  machine &lt;-. 4, 5 .-&gt; tftp\n  machine &lt;-. 6, 7 .-&gt; http</code></pre> <ol> <li>Ansible: Hey MAC address <code>xx:xx:xx:xx:xx:xx</code>, wake up!</li> <li>Machine: Hello everyone, I just woke up in network mode, could someone please show me how to boot?</li> <li>DHCP server: I hear you, here's your IP address, proceed to the next server to obtain your bootloader.</li> <li>Machine: Hello, could you please send me my bootloader?</li> <li>TFTP server: Here you go. Grab your boot configuration, kernel, and initial ramdisk as well.</li> <li>Machine: Hi, I just booted into my bootloader, and my boot parameters instructed me to get the installation instructions, packages, etc. from this site.</li> <li>HTTP server: It's all yours.</li> <li>Machine: Great, now I can install the OS and reboot!</li> </ol> <p>Here's how it looks like in action:</p> <p>TODO: add video</p>"},{"location":"admin-guide/deployment/configure/","title":"Configure","text":"<p>The configure script is located in <code>scripts/configure</code> and can be run directly.</p> <p>The script makes changes not only to the <code>.env</code> file, but also to multiple files within the git repository.</p> <p>If you are not happy with the changes, you can revert them with <code>git checkout .</code>.</p> <p>At any point you can type ? and be presented with help for any question. It also tells you what the default (or previously set) value is.</p> <p>The questions asked by the script are: - <code>Select text editor</code> - This is the text editor that will be used to inspect the configuration at the end of the configure process. It is recommended to use <code>nano</code> if you are not familiar with <code>vim</code>. This is used at the end of the configure process to inspect the configuration.</p> <ul> <li><code>What is the domain name of your Ubiquity deployment?</code> - This is the domain name that will be used to access Ubiquity. It is recommended to use a domain name that resolves to the machine where Ubiquity is deployed. If you are deploying Ubiquity on your laptop or on a cluster where you don't have DNS upstream access configured, you can use <code>nip.io</code>. If you are deploying Ubiquity on a server, you can use a domain name that resolves to the cluster. Wildcard DNS records are supported, just make sure that the * is removed from the domain name. For example, if you want to use <code>*.example.com</code>, you should enter <code>example.com</code> here. This domain name will be used to generate TLS certificates for Ubiquity. Note that if you are using <code>nip.io</code>, you will get a warning about the certificate being invalid. This is expected and you can safely ignore it. If you do use nip.io, you will need to use <code>https://&lt;your service&gt;.&lt;your-ip&gt;.nip.io</code> to access Ubiquity. For example, if you are using nip.io and your IP is <code>10.212.84.200</code>, you will need to use <code>https://&lt;your service&gt;.10-212-84-200.nip.io</code> to access Ubiquity. If you are using a domain name that resolves to the machine where Ubiquity is deployed, you can use <code>https://&lt;your service&gt;.&lt;your domain name&gt;</code> to access Ubiquity. For example, if you are using <code>example.com</code>, you will need to use <code>https://&lt;your service&gt;.example.com</code> to access Ubiquity.</li> </ul> <p>Note: If you use <code>nip.io</code> you are expected to be running non-production workloads. If that is the case, then your cert_provider should be set to <code>pebble-issuer</code> and your cert_provider_pebble_issuer_url should be set to <code>https://pebble-issuer:8080/cluster-ca</code>. If you are running production workloads, then you should use a real domain name and a real certificate provider which in our case should be <code>letsencrypt-prod</code>. You will need to set your cert_provider to <code>letsencrypt-prod</code> and your cert_provider_letsencrypt_prod_email to your email address.</p> <ul> <li> <p><code>Enter seed repo</code> - This is the original git repository that the cluster is defined at. This is used to pull the cluster definition from. It is also used to push the cluster definition to and so must have a service account with read access. This forms the basis of the cluster definition and is the source of truth for the cluster. It is also used as a core of the deployment, leveraging GitOps to deploy and maintain cluster services. Please make sure that if you choose to store sensitive information that this repository is private!</p> </li> <li> <p><code>Enter seed repo username</code> - This is the username that has permission to access the original git repository that the cluster is defined at in the question previously. This is the service account that has read access to the repository.</p> </li> <li> <p><code>Enter seed repo password</code> - This is the password that has permission to access the original git repository that the cluster is defined at in the question previously. This is the service account that has read access to the repository.</p> </li> <li> <p><code>Enter DNS server</code> - This is a DNS server for DNS records to be resolved. This is used to resolve upstream DNS for the cluster. This can be self-contained however it is recommended to reduce \"blast radius\" by using an upstream DNS server that you can cache records from locally on-cluster. Please make sure that this server is reachable!</p> </li> <li> <p><code>Enter NTP server</code> - This is a NTP server for network-time. This is used to ensure that all nodes are in sync with each other. This can be self-contained however it is recommended to reduce \"blast radius\" by using an upstream NTP server that you can serve the time locally on-cluster from. Please make sure that this server is reachable!</p> </li> <li> <p><code>Please define OS (Rocky or Ubuntu)</code> - This is an OS flavour. Currently Rocky or Ubuntu. Define version next!</p> </li> <li> <p><code>Please define OS version for Rocky or Ubuntu:</code> - This is an OS version. Supported versions are Rocky (8.7/8.8/8.9/9.1) or Ubuntu (22.04/23.04). Define flavour first!</p> </li> <li> <p><code>Enter time zone</code> - This is the timezone for your cluster specified in TZ format. For example <code>Europe/London</code> or <code>America/New_York</code>.</p> </li> <li> <p><code>Enter cluster name</code> - This is a name for your cluster. This is used to identify your cluster in Ubiquity. This is also used to identify your cluster in the cluster definition repository.</p> </li> <li> <p><code>Enter cluster domain</code> - this is a domain for your cluster. This domain is used to identify hostnames for your cluster. This is also used to identify your cluster in the cluster definition repository.</p> </li> <li> <p><code>Enter cluster network CIDR</code> - this is a network CIDR for your Kubernetes cluster. This is the address range that all of your pods will be provisioned on. This is normally a private address range.</p> </li> <li> <p><code>Enter cluster service CIDR</code> - this is a network CIDR for the services you want to run on your cluster. This is the address range that services provision themselves on. Services aren't often presented directly to the user and normally go through an Ingress Controller.</p> </li> <li> <p><code>Enable OFED</code> - this is if you want to enable OFED on your cluster or not. OFED is a high-performance network stack that is used for RDMA and other high-performance networking. This is used for high-performance workloads such as HPC and AI/ML. This is not required for all workloads.</p> </li> <li> <p><code>Enter OFED version</code> - this is the OFED version you want to install on your cluster. This is the version of the OFED stack that will be installed on your cluster. This is used for high-performance workloads such as HPC and AI/ML. This is not required for all workloads.</p> </li> <li> <p><code>Enter external interface</code> - this is the externally-facing interface for your HA cluster control plane. This can be an interface that is exposed to the internet or a private interface that is exposed to a load balancer. Used if you want to separate internal and external traffic.</p> </li> <li> <p><code>Enter internal interface</code> - this is the internally-facing interface for your HA cluster control plane. This can be an interface that is exposed to the internet or a private interface that is exposed to a load balancer. Used if you want to separate internal and external traffic.</p> </li> <li> <p><code>Enter keepalived interface</code> - this is the keepalived interface for your HA cluster control plane. This is the physical interface that will be used to provision a highly-available keepalived virtual IP on that floats between all 3 control plane masters. </p> </li> <li> <p><code>Enter keepalived CIDR</code> - this is the keepalived CIDR notation the keepalived address exists on. This is to define an address space and mask that the keepalived virtual IP will float on. This is normally in the format of <code>X.X.X.X/XX</code>.</p> </li> <li> <p><code>Enter keepalived VIP</code> - this is the keepalived virtual IP address for your HA cluster control plane. This address will be the address you will talk to your Kubernetes API on and will float between all 3 control plane masters. This is normally in the format of <code>X.X.X.X</code>.</p> </li> <li> <p><code>Enter MetalLB external IP range</code> - this is the MetalLB external IP range for your cluster that will be used for services. Future services will be able to assign from a pool of external addresses only.</p> </li> <li> <p><code>Enter MetalLB internal IP range</code> - this is the MetalLB internal IP range for your cluster. Future services will be able to assign from a pool of internal addresses only. Currently this feature is disabled.</p> </li> <li> <p><code>Enter k3s version</code> - this is the k3s version for your cluster. </p> </li> <li> <p><code>Enter k3s encryption secret</code> - this is the k3s encryption secret for your cluster. This is auto-generated but you can get it now here. This is used to encrypt all traffic between nodes in the cluster.</p> </li> <li> <p><code>Enter cert provider</code> - this is the certificate provider for the cluster. This can be pebble-issuer which is an internal-only provider, or letsencrypt-prod which is a production provider. This is used to generate TLS certificates for Ubiquity.</p> </li> <li> <p><code>Enter internal ipv4 address of bootstrapper</code> - this is the internal address of the bootstrap node. This is the address that sits on a bootstrap node and provisions on-premise environments. This is the address that is defined as the default gateway for the cluster during provisioning.</p> </li> <li> <p><code>Enter internal ipv4 network address space</code> - this is the internal address space used for the cluster. This is the address space that all of your pods will be provisioned on. This is normally a private address space.</p> </li> <li> <p><code>Enter internal ipv4 network gateway</code> - this is the internal network gateway address. This is the address that is defined as the default gateway for the cluster.</p> </li> <li> <p><code>Enter external ipv4 address of bootstrapper</code> - this is the external address of the bootstrap node. This is the address that sits on a bootstrap node and in some cases delivers NAT to the remaining nodes.</p> </li> <li> <p><code>Enter external ipv4 network address space</code> - this is the external address space used for the cluster.</p> </li> <li> <p><code>Enter external ipv4 network gateway</code> - this is the external network gateway address. This is the address that sits on a bootstrap node and provisions on-premise environments.</p> </li> <li> <p><code>Enter your username for the container registry:</code> - this is your username for accessing container images - Please refer to the documentation or GitHub repository for access credentials!</p> </li> <li> <p><code>Enter your registration key:</code> - this is the special token to allow you to log onto the container registry. Without this you will not be able to pull the images required to run Ubiquity.</p> </li> <li> <p><code>Enter your dockerhub username:</code> - this is your dockerhub username. This is used to prevent rate-limiting on dockerhub.</p> </li> <li> <p><code>Enter your password:</code> - this is your dockerhub password. This is used to prevent rate-limiting on dockerhub.</p> </li> </ul> <p>The most typical aspects for configuration are:</p> <ul> <li>Configuring identity providers. Ubiquity supports a range of OIDC and SAML based IdPs.</li> <li>Configuring storage providers. Ubiquity supports a range of storage providers.</li> <li>Configuring platform providers. Ubiquity supports a range of platform providers.</li> <li>Adding catalogue items for sharing amongst Ubiquity users as a self-serve function.</li> </ul>"},{"location":"admin-guide/deployment/external-resources/","title":"External resources","text":"<p>Info</p> <p>These resources are optional, the environment still works without them but will lack some features like trusted certificates and offsite backup</p> <p>Although I try to keep the amount of external resources to the minimum, there's still need for a few of them. Below is a list of external resources and why we need them (also see some alternatives below).</p> Provider Resource Purpose Terraform Cloud Workspace Terraform state backend Cloudflare DNS DNS and DNS-01 challenge for certificates Cloudflare Tunnel Public services to the internet without port-forwarding"},{"location":"admin-guide/deployment/external-resources/#create-credentials","title":"Create credentials","text":"<p>You'll be asked to provide these credentials on first build.</p>"},{"location":"admin-guide/deployment/external-resources/#create-terraform-workspace","title":"Create Terraform workspace","text":"<p>Terraform is stateful, which means it needs somewhere to store its state. Terraform Cloud is one option for a state backend with a generous free tier, perfect for a control plane.</p> <ol> <li>Sign up for a Terraform Cloud account</li> <li>Create a workspace named <code>ubiquity-external</code>, this is the workspace where your cluster state will be stored.</li> <li>Change the \"Execution Mode\" from \"Remote\" to \"Local\". This will ensure your local machine, which can access your cluster, is the one executing the terraform plan rather than the cloud runners.</li> </ol> <p>If you decide to use a different Terraform backend, you'll need to edit the <code>external/versions.tf</code> file as required.</p>"},{"location":"admin-guide/deployment/external-resources/#cloudflare","title":"Cloudflare","text":"<ul> <li>Buy a domain and transfer it to Cloudflare if you haven't already</li> <li>Get Cloudflare email and account ID</li> <li>Global API key: https://dash.cloudflare.com/profile/api-tokens</li> </ul>"},{"location":"admin-guide/deployment/external-resources/#alternatives","title":"Alternatives","text":"<ul> <li>Terraform Cloud: any other Terraform backends</li> <li>Cloudflare DNS: see manual DNS setup</li> <li>Cloudflare Tunnel: you can create a small VPS in the cloud and utilize Wireguard and HAProxy to route traffic via it, or just use simple port-forwarding if it's available (see also awesome tunneling)</li> </ul>"},{"location":"admin-guide/deployment/on-prem/","title":"On-Premises Production Deployment","text":"<p>This guide describes how to deploy Ubiquity on-premises in a production environment.</p>"},{"location":"admin-guide/deployment/on-prem/#prerequisites","title":"Prerequisites","text":"<ul> <li>at least 4 nodes:<ul> <li>3x Control-Plane nodes</li> <li>1x Compute Node</li> </ul> </li> <li>An appropriate network switch with VLAN support<ul> <li>1x VLAN for management</li> <li>1x VLAN for storage</li> <li>1x VLAN for HPC</li> <li>1x VLAN for Kubernetes</li> <li>1x VLAN for OOB</li> </ul> </li> <li>A DNS server (your laptop can be used for this and then pivoted to a dedicated DNS server provisioned by Ubiquity)</li> <li>A NTP server (your laptop can be used for this and then pivoted to a dedicated NTP server provisioned by Ubiquity)</li> <li>A bootstrap node (your laptop can be used for this)</li> </ul>"},{"location":"admin-guide/deployment/on-prem/#prepare-environment","title":"Prepare environment","text":"<pre><code># clone repo\ngit clone https://github.com/ubiquitycluster/ubiquity.git\ncd ubiquity\ngit submodule update --init --recursive\n</code></pre>"},{"location":"admin-guide/deployment/on-prem/#configuring-ubiquity","title":"Configuring Ubiquity","text":"<pre><code># jump into opus environment\nsudo make tools\n# configure\nmake configure\n</code></pre> <p><code>make configure</code> will ask you a few questions using a configure script and generate <code>.env</code> file with your answers.</p> <p>For more information, please see configuration guide.</p>"},{"location":"admin-guide/deployment/on-prem/#deploying-ubiquity","title":"Deploying Ubiquity","text":"<p>Prior to deploying Ubiquity, you should now push all of your changes to your chosen git repository that you configured during <code>make configure</code>.</p> <pre><code># deploy\ngit push origin\n</code></pre> <p>Then you can trigger a deployment of Ubiquity by running:</p> <pre><code># start Ubiquity environment\nmake\n\n# verify\nkubectl get pods -A\nk9s\n</code></pre> <p>Ubiquity will:  - Bootstrap a PXE environment - IPMI network boot the 3x control-plane nodes - Install Ubuntu 20.04 on the 3x control-plane nodes - Install k3s on the 3x control-plane nodes - Install MetalLB on the 3x control-plane nodes - Install Keepalived on the 3x control-plane nodes - Install Longhorn on the 3x control-plane nodes - Install ArgoCD on the 3x control-plane nodes - Get ArgoCD to provision the remaining environment components including DNS and NTP - Install the BareMetal Operator on the 3x control-plane nodes</p> <p>ArgoCD reads your configuration from the git repository you configured during <code>make configure-sandbox</code> and pushed.</p> <p>Once deployed, you can pivot to the provisioned DNS and NTP servers on the 3x control plane nodes by running:</p> <pre><code>make pivot\n</code></pre> <p>Which will take your NTP and DNS server settings from the .env file and configure your control plane nodes to use them.</p>"},{"location":"admin-guide/deployment/on-prem/#administrating-ubiquity","title":"Administrating Ubiquity","text":"<p>To administrate Ubiquity, go look at the admin-guide.</p> <p>To add users, go look at the user accounts section.</p>"},{"location":"admin-guide/deployment/on-prem/#accessing-ubiquity","title":"Accessing Ubiquity","text":"<p>To access Ubiquity, go look at the user-guide.</p>"},{"location":"admin-guide/deployment/on-prem/#logsmonitoring","title":"Logs/Monitoring","text":"<p>Ubiquity is monitored by using Prometheus and Grafana. You can access Grafana at <code>https://grafana.&lt;your domain&gt;.nip.io</code> (default credentials you can get by running <code>scripts/grafana-admin-password</code>). Logs emitted by the containers are collected and saved inside Loki. You can access them via Grafana located at <code>https://grafana.&lt;your domain&gt;.io</code> (default credentials you can get by running <code>scripts/grafana-admin-password</code>).</p>"},{"location":"admin-guide/deployment/on-prem/#maintenance","title":"Maintenance","text":"<p>Please see the admin-guide for more information.</p>"},{"location":"admin-guide/deployment/on-prem/#known-issues","title":"Known issues","text":"<p>When ubiquity is launched for the first time, it takes a little while to apply all configs. It means that you may need to wait few minutes until these applications are setup. This is normally a 10-15 minute process for sandbox mode.</p>"},{"location":"admin-guide/deployment/on-prem/#keycloak","title":"Keycloak","text":"<p>Keycloak is an Identity and Access Management software bundled with Ubiquity. it is used to authenticate users and manage their permissions.</p> <p>To find the keycloak admin account run:</p> <pre><code>./scripts/keycloak-admin-password\n</code></pre> <p>Login to the admin interface at <code>keycloak.&lt;your domain&gt;/auth/admin</code> and create ubiquity users. See the user accounts section.</p>"},{"location":"admin-guide/deployment/on-prem/#integration-with-slurm","title":"Integration with SLURM","text":"<p>SLURM integration already exists - In production mode, the concept of node as pod exists but can be pivoted to bare-metal if required. In the <code>hpc-ubiq</code> space a slurm cluster should already exist with 1x compute replica. You can check this by using K9s and attaching a shell to the <code>hpc-ubiq/slurmctld</code> instance. You can then run <code>sinfo</code> to see the cluster status.</p> <p>You can get this instance to be accessible by setting up a port-forward rule accordingly. For example:</p> <pre><code>kubectl port-forward -n hpc-ubiq svc/slurmctld 2222:22\n</code></pre> <p>If you wish to pivot to bare-metal, see the bare-metal provider documentation.</p>"},{"location":"admin-guide/deployment/on-prem/#updating-ubiquity","title":"Updating Ubiquity","text":"<p>Ubiquity can be updated by simply backing up your .env file, git pulling the latest changes, and then committing this back to your upstream git repository. ArgoCD will then automatically update your environment.</p>"},{"location":"admin-guide/deployment/post-installation/","title":"Post-installation","text":""},{"location":"admin-guide/deployment/post-installation/#backup-secrets","title":"Backup secrets","text":"<p>Save the following files to a safe location (like a password manager):</p> <ul> <li><code>~/.ssh/id_ed25519</code></li> <li><code>~/.ssh/id_ed25519.pub</code></li> <li><code>./metal/kubeconfig.yaml</code></li> <li><code>~/.terraform.d/credentials.tfrc.json</code></li> <li><code>./external/terraform.tfvars</code></li> </ul>"},{"location":"admin-guide/deployment/post-installation/#admin-credentials","title":"Admin credentials","text":"<ul> <li>ArgoCD:<ul> <li>Username: <code>admin</code></li> <li>Password: run <code>./scripts/argocd-admin-password</code></li> </ul> </li> <li>Vault:<ul> <li>Root token: run <code>./scripts/vault-root-token</code></li> </ul> </li> <li>Grafana:<ul> <li>Username: <code>admin</code></li> <li>Password: <code>prom-operator</code> (TODO: use random password)</li> </ul> </li> <li>Gitea:<ul> <li>Username: <code>gitea_admin</code></li> <li>Password: get from Vault</li> </ul> </li> </ul>"},{"location":"admin-guide/deployment/post-installation/#next-steps","title":"Next steps","text":"<ul> <li>User onboarding</li> </ul>"},{"location":"admin-guide/deployment/prerequisites/","title":"Prerequisites","text":""},{"location":"admin-guide/deployment/prerequisites/#fork-this-repository","title":"Fork this repository","text":"<p>Because this project applies GitOps practices, it's the source of truth for our development environment, so you'll need to fork it to make it yours:</p> <p> Fork cjcshadowsan/ubiquity</p> <p>By using this project you agree to the license.</p> <p>License TL;DR</p> <ul> <li>This project is free to use for any purpose, but it comes with no warranty</li> <li>If used for commercial purposes, you must gain an agreement from Q Associates</li> <li>You must use the same Apache v2.0 license  in <code>LICENSE</code></li> <li>You must keep the copy right notice and/or include an acknowledgement</li> <li>Your project must remain open-source and be published publicly</li> </ul>"},{"location":"admin-guide/deployment/prerequisites/#hardware-requirements","title":"Hardware requirements","text":""},{"location":"admin-guide/deployment/prerequisites/#initial-controller","title":"Initial controller","text":"<p>Info</p> <p>The initial controller is the machine used to bootstrap the cluster, we only need it once, you can use your laptop or desktop</p> <ul> <li>A Linux machine that can run Docker (because the <code>host</code> networking driver used for PXE boot only supports Linux, you can use a Linux virtual machine with bridged networking if you're on macOS or Windows).</li> </ul>"},{"location":"admin-guide/deployment/prerequisites/#servers","title":"Servers","text":"<p>Any modern <code>x86_64</code> computer(s) should work, you can use old PCs, laptops or servers.</p> <p>Info</p> <p>This is the requirements for each node</p> Component Minimum Recommended CPU 2 cores 4 cores RAM 8 GB 16 GB Hard drive 128 GB 512 GB (depending on your storage usage, the base installation will not use more than 128GB) Node count 1 (checkout the single node cluster adjustments tutorial) 3 or more for high availability <p>Additional capabilities:</p> <ul> <li>Ability to boot from the network (PXE boot)</li> <li>Wake-on-LAN capability, used to wake the machines up automatically without physically touching the power button</li> </ul>"},{"location":"admin-guide/deployment/prerequisites/#network-setup","title":"Network setup","text":"<ul> <li>All servers must be connected to the same wired network with the initial controller</li> <li>You have the access to change DNS config (on your router or at your domain registrar)</li> </ul>"},{"location":"admin-guide/deployment/prerequisites/#domain","title":"Domain","text":"<p>Buying a domain is highly recommended, but if you don't have one, see manual DNS setup.</p>"},{"location":"admin-guide/deployment/prerequisites/#bios-setup","title":"BIOS setup","text":"<p>Info</p> <p>You need to do it once per machine if the default config is not sufficent, usually for consumer hardware this can not be automated (it requires something like IPMI to automate).</p> <p>Common settings:</p> <ul> <li>Enable Wake-on-LAN (WoL) and network boot</li> <li>Use UEFI mode and disable CSM (legacy) mode</li> <li>Disable secure boot</li> </ul> <p>Boot order options (select one, each has their pros and cons):</p> <ol> <li>Only boot from the network if no operating system found: works on most hardware but you need to manually wipe your hard drive or delete the existing boot record for the current OS</li> <li>Prefer booting from the network if turned on via WoL: more convenience but your BIOS must support it, and you must test it throughly to ensure you don't accidentally wipe your servers</li> </ol> <p>Example</p> <p>Below is my BIOS setup for reference. Your motherboard may have a different name for the options, so you'll need to adapt it to your hardware.</p> <pre><code>Devices:\n  NetworkSetup:\n    PXEIPv4: true\n    PXEIPv6: false\nAdvanced:\n  CPUSetup:\n    VT-d: true\nPower:\n  AutomaticPowerOn:\n    WoL: Automatic  # Use network boot if Wake-on-LAN\nSecurity:\n  SecureBoot: false\nStartup:\n  CSM: false\n</code></pre>"},{"location":"admin-guide/deployment/prerequisites/#gather-information","title":"Gather information","text":"<ul> <li> MAC address for each machine</li> <li> OS disk name (for example <code>/dev/sda</code>)</li> <li> Network interface name (for example <code>eth0</code>)</li> <li> Choose a static IP address for each machine (just the desired address, we don't set anything up yet)</li> </ul>"},{"location":"admin-guide/deployment/sandbox/","title":"Ubiquity Sandbox Deployment","text":"<p>This guide describes how to deploy Ubiquity in a sandbox mode on a single machine. This is the fastest way to get Ubiquity up and running, but it is not suitable for production. It is recommended to use cloud or on-premises deployment for production.</p>"},{"location":"admin-guide/deployment/sandbox/#prerequisites","title":"Prerequisites","text":"<ul> <li>at least 16GB RAM to run all containers in k3d</li> <li>Docker v1.13+</li> </ul>"},{"location":"admin-guide/deployment/sandbox/#prepare-environment","title":"Prepare environment","text":"<pre><code># clone repo\ngit clone https://github.com/ubiquitycluster/ubiquity.git\ncd ubiquity\ngit submodule update --init --recursive\n</code></pre>"},{"location":"admin-guide/deployment/sandbox/#configuring-ubiquity","title":"Configuring Ubiquity","text":"<pre><code># jump into opus environment\nsudo make tools\n# configure\nmake configure-sandbox\n</code></pre> <p><code>make configure-sandbox</code> will ask you a few questions using a configure script with specialised arguments and generate <code>.env</code> file with your answers.</p> <p>This in sandbox mode, you can leave all the default values as it configures pretty much everything for you so please press enter to all questions.</p> <p>For more information, please see configuration guide.</p>"},{"location":"admin-guide/deployment/sandbox/#deploying-ubiquity","title":"Deploying Ubiquity","text":"<p>Prior to deploying Ubiquity, you should now push all of your changes to your chosen git repository that you configured during <code>make configure-sandbox</code>.</p> <pre><code># deploy\ngit push origin\n</code></pre> <p>Then you can trigger a deployment of Ubiquity by running:</p> <pre><code># start Ubiquity environment\nmake sandbox\n\n# verify\nkubectl get pods -A\nk9s\n</code></pre> <p>Ubiquity will start a K3d cluster with all the required containers and services, and get ArgoCD to deploy the Ubiquity environment - reading your configuration from the git repository you configured during <code>make configure-sandbox</code> and pushed.</p>"},{"location":"admin-guide/deployment/sandbox/#administrating-ubiquity","title":"Administrating Ubiquity","text":"<p>To administrate Ubiquity, go look at the admin-guide.</p> <p>To add users, go look at the user accounts section.</p>"},{"location":"admin-guide/deployment/sandbox/#accessing-ubiquity","title":"Accessing Ubiquity","text":"<p>To access Ubiquity, go look at the user-guide.</p>"},{"location":"admin-guide/deployment/sandbox/#logsmonitoring","title":"Logs/Monitoring","text":"<p>Ubiquity is monitored by using Prometheus and Grafana. You can access Grafana at <code>https://grafana.127-0.0.1.nip.io</code> (default credentials you can get by running <code>scripts/grafana-admin-password</code>). Logs emitted by the containers are collected and saved inside Loki. You can access them via Grafana located at <code>https://grafana.127-0-0-1.nip.io</code> (default credentials you can get by running <code>scripts/grafana-admin-password</code>).</p>"},{"location":"admin-guide/deployment/sandbox/#maintenance","title":"Maintenance","text":"<p>Please see the admin-guide for more information.</p>"},{"location":"admin-guide/deployment/sandbox/#known-issues","title":"Known issues","text":"<p>When ubiquity is launched for the first time, it takes a little while to apply all configs. It means that you may need to wait few minutes until these applications are setup. This is normally a 10-15 minute process for sandbox mode.</p>"},{"location":"admin-guide/deployment/sandbox/#keycloak","title":"Keycloak","text":"<p>Keycloak is an Identity and Access Management software bundled with Ubiquity. it is used to authenticate users and manage their permissions.</p> <p>To find the keycloak admin account run:</p> <pre><code>./scripts/keycloak-admin-password\n</code></pre> <p>Login to the admin interface at https://keycloak.127-0-0-1.nip.io/auth/admin and create ubiquity users</p>"},{"location":"admin-guide/deployment/sandbox/#integration-with-slurm","title":"Integration with SLURM","text":"<p>SLURM integration already exists - In sandbox mode, the concept of node as pod exists. In the <code>hpc-ubiq</code> space a slurm cluster should already exist with 1x compute replica. You can check this by using K9s and attaching a shell to the <code>hpc-ubiq/slurmctld</code> instance. You can then run <code>sinfo</code> to see the cluster status.</p> <p>You can get this instance to be accessible by setting up a port-forward rule accordingly. For example:</p> <pre><code>kubectl port-forward -n hpc-ubiq svc/slurmctld 2222:22\n</code></pre>"},{"location":"admin-guide/deployment/sandbox/#updating-ubiquity","title":"Updating Ubiquity","text":"<p>Ubiquity can be updated by simply backing up your .env file, git pulling the latest changes, and then running <code>make sandbox</code> again.</p>"},{"location":"admin-guide/deployment/cloud/","title":"Overview","text":""},{"location":"admin-guide/deployment/cloud/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Ubiquity Architecture Overview</li> <li>Installation</li> <li>User Guide</li> <li>Admin Guide</li> <li>Developer Guide</li> <li>Support</li> <li>Contributing</li> <li> <p>License</p> </li> <li> <p>Configuration</p> </li> <li>Cloud Specific Configuration</li> <li>DNS Configuration and SSL Certificates</li> <li>Planning</li> <li>Deployment</li> <li>Destruction</li> <li>customise Cluster Software Configuration</li> <li>customise Ubiquity Terraform Files</li> <li>customise Ubiquity Ansible Configuration</li> </ol>"},{"location":"admin-guide/deployment/cloud/#41-main-file","title":"4.1 Main File","text":"<ol> <li>Go to https://github.com/ubiquitycluster/ubiquity/releases.</li> <li>Download the latest release of Ubiquity.</li> <li>Open a Terminal.</li> <li>Uncompress the release: <code>tar xvf ubiquity*.tar.gz</code></li> <li>Rename the release folder after your favourite superhero: <code>mv ubiquity* hulk</code></li> <li>Move inside the folder: <code>cd hulk</code></li> </ol> <p>The file <code>main.tf</code> contains Terraform modules and outputs. Modules are files that define a set of resources that will be configured based on the inputs provided in the module block. Outputs are used to tell Terraform which variables of our module we would like to be shown on the screen once the resources have been instantiated.</p> <p>This file will be our main canvas to design our new clusters. As long as the module block parameters suffice to our need, we will be able to limit our configuration to this sole file. Further customization will be addressed during the second part of the workshop.</p>"},{"location":"admin-guide/deployment/cloud/#42-makefile","title":"4.2 Makefile","text":"<p>By default, the Makefile for Ubiquity allows you to both make the tools for the cluster, and to make the cluster itself. The Makefile is located in the root of the Ubiquity folder.</p> <p>The Makefile generally allows you to run the following commands:</p> <ul> <li><code>make tools</code>: builds the tools for the cluster.</li> <li><code>make cluster</code>: builds the cluster.</li> <li><code>make cloud</code>: builds the cluster on a cloud provider.</li> <li><code>make clean</code>: cleans the tools and the cluster.</li> <li><code>make help</code>: displays the help.</li> <li><code>make</code>: builds the tools and the cluster.</li> </ul>"},{"location":"admin-guide/deployment/cloud/#32-terraform","title":"3.2 Terraform","text":"<p>Again, if you don't want to run the Makefile yourself and wish to deploy manually via terraform you can do that. Terraform fetches the plugins required to interact with the cloud provider defined by our <code>main.tf</code> once when we initialise. To initialise, enter the following command: <pre><code>terraform init\n</code></pre></p> <p>The initialisation is specific to the folder where you are currently located. The initialisation process looks at all <code>.tf</code> files and fetches the plugins required to build the resources defined in these files. If you replace some or all <code>.tf</code> files inside a folder that has already been initialised, just call the command again to make sure you have all plugins.</p> <p>The initialisation process creates a <code>.terraform</code> folder at the root of your current folder. You do not need to look at its content for now.</p>"},{"location":"admin-guide/deployment/cloud/#321-terraform-modules-upgrade","title":"3.2.1 Terraform Modules Upgrade","text":"<p>Once Terraform folder has been initialised, it is possible to fetch the newest version of the modules used by calling: <pre><code>terraform init -upgrade\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#4-configuration","title":"4. Configuration","text":"<p>In the <code>main.tf</code> file, there is a module named after your cloud provider, i.e.: <code>module \"openstack\"</code>. This module corresponds to the high-level infrastructure of your cluster.</p> <p>The following sections describes each variable that can be used to customise the deployed infrastructure and its configuration. Optional variables can be absent from the example module. The order of the variables does not matter, but the following sections are ordered as the variables appear in the examples.</p>"},{"location":"admin-guide/deployment/cloud/#41-source","title":"4.1 source","text":"<p>The first line of the module block indicates to Terraform where it can find the files that define the resources that will compose your cluster. In the releases, this variable is a relative path to the cloud provider folder (i.e.: <code>./aws</code>).</p> <p>Requirement: Must be a path to a local folder containing the Ubiquity Terraform files for the cloud provider of your choice. It can also be a git repository. Refer to Terraform documentation on module source for more information.</p> <p>Post build modification effect: <code>terraform init</code> will have to be called again and the next <code>terraform apply</code> might propose changes if the infrastructure describe by the new module is different.</p>"},{"location":"admin-guide/deployment/cloud/#42-config_git_url","title":"4.2 config_git_url","text":"<p>Ubiquity configuration management is handled by Ansible. The Ansible configuration files are stored in a git repository. This is typically ubiquitycluster/ubiq-playbooks repository on GitHub. It is included in the project as a git submodule.</p> <p>Leave these variables to their current values to deploy a vanilla Ubiquity cluster.</p> <p>If you wish to customise the instances' role assignment, add services, or develop new features for Ubiquity, fork the ubiquitycluster/ubiq-playbooks and point this variable to your fork's URL. For more information on Ubiquity Ansible configuration customisation, refer to developer documentation.</p> <p>Requirement: Must be valid HTTPS URL to a git repository describing a Ansible environment compatible with Ubiquity.</p> <p>Post build modification effect: no effect. To change the Ansible configuration source, destroy the cluster or change it manually on the Ansible server.</p>"},{"location":"admin-guide/deployment/cloud/#43-config_version","title":"4.3 config_version","text":"<p>Since Ubiquity Cluster configuration is managed with git, it is possible to specify which version of the configuration you wish to use. Typically, it will match the version number of the release you have downloaded (i.e: <code>9.3</code>).</p> <p>Requirement: Must refer to a git commit, tag or branch existing in the git repository pointed by <code>config_git_url</code>.</p> <p>Post build modification effect: none. To change the Ansible configuration version, destroy the cluster or change it manually on the Ansible server.</p>"},{"location":"admin-guide/deployment/cloud/#44-cluster_name","title":"4.4 cluster_name","text":"<p>Defines the <code>ClusterName</code> variable in <code>slurm.conf</code> and the name of the cluster in the Slurm accounting database (see <code>slurm.conf</code> documentation).</p> <p>Requirement: Must be lowercase alphanumeric characters and start with a letter and can include dashes. cluster_name must be 63 characters or less.</p> <p>Post build modification effect: destroy and re-create all instances at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#45-domain","title":"4.5 domain","text":"<p>Defines * the Kerberos realm name when initializing FreeIPA. * the internal domain name and the <code>resolv.conf</code> search domain as <code>int.{cluster_name}.{domain}</code></p> <p>Optional modules following the current module in the example <code>main.tf</code> can be used to register DNS records in relation to your cluster if the DNS zone of this domain is administered by one of the supported providers. Refer to section 6. DNS Configuration and SSL Certificates for more details.</p> <p>Requirements:</p> <ul> <li>Must be a fully qualified DNS name and RFC-1035-valid. Valid format is a series of labels 1-63 characters long matching the regular expression <code>[a-z]([-a-z0-9]*[a-z0-9])</code>, concatenated with periods.</li> <li>No wildcard record A of the form <code>*.domain. IN A x.x.x.x</code> exists for that domain. You can verify no such record exist with <code>dig</code>:     <pre><code>dig +short '*.${domain}'\n</code></pre></li> </ul> <p>Post build modification effect: destroy and re-create all instances at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#46-image","title":"4.6 image","text":"<p>Defines the name of the image that will be used as the base image for the cluster nodes.</p> <p>You can use a custom image if you wish, but configuration management should be mainly done through Ansible. Image customization is mostly envisioned as a way to accelerate the configuration process by applying the security patches and OS updates in advance.</p> <p>To specify a different image for an instance type, use the <code>image</code> instance attribute</p> <p>Requirements: the operating system on the image must be from the RedHat family, This includes CentOS (7, 8), Rocky Linux (8), and AlmaLinux (8) - Or from the  Debian family, this includes Debian (10, 11, 12), Ubuntu (20.04, 22.04) and DGX OS.</p> <p>Post build modification effect: none. If this variable is modified, existing instances will ignore the change and future instances will use the new value.</p>"},{"location":"admin-guide/deployment/cloud/#461-aws","title":"4.6.1 AWS","text":"<p>The image field needs to correspond to the Amazon Machine Image (AMI) ID. AMI IDs are specific to regions and architectures. Make sure to use the right ID for the region and CPU architecture you are using (i.e: x86_64).</p> <p>To find out which AMI ID you need to use, refer to - AlmaLinux OS Amazon Web Services AMIs - CentOS list of official images available on the AWS Marketplace - Rocky Linux Amazon Web Services AMIs - Debian Cloud Images - Ubuntu Cloud Images</p> <p>Note: Before you can use the AMI, you will need to accept the usage terms and subscribe to the image on AWS Marketplace. On your first deployment, you will be presented an error similar to this one: <pre><code>\u2502 Error: Error launching source instance: OptInRequired: In order to use this AWS Marketplace product you need to accept terms and subscribe. To do so please visit https://aws.amazon.com/marketplace/pp?sku=cvugziknvmxgqna9noibqnnsy\n\u2502   status code: 401, request id: 2e05a85a-f37a-41b5-42c5-465eb3da6c4f\n\u2502\n\u2502   on aws/infrastructure.tf line 67, in resource \"aws_instance\" \"instances\":\n\u2502   67: resource \"aws_instance\" \"instances\" {\n</code></pre> To accept the terms and fix the error, visit the link provided in the error output, then click on the <code>Click to Subscribe</code> yellow button.</p>"},{"location":"admin-guide/deployment/cloud/#462-microsoft-azure","title":"4.6.2 Microsoft Azure","text":"<p>The image field for Azure can either be a string or a map.</p> <p>A string image specification will correspond to the image id. Image ids can be retrieved using the following command-line: <pre><code>az image builder list\n</code></pre></p> <p>A map image specification needs to contain the following fields <code>publisher</code>, <code>offer</code> <code>sku</code>, and optionally <code>version</code>. The map is used to specify images found in Azure Marketplace. Here is an example: <pre><code>{\n    publisher = \"OpenLogic\",\n    offer     = \"CentOS-CI\",\n    sku       = \"7-CI\"\n}\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#463-openstack","title":"4.6.3 OpenStack","text":"<p>The image name can be a regular expression. If more than one image is returned by the query to OpenStack, the most recent is selected.</p>"},{"location":"admin-guide/deployment/cloud/#47-instances","title":"4.7 instances","text":"<p>The <code>instances</code> variable is a map that defines the virtual machines that will form the cluster. The map' keys define the hostnames and the values are the attributes of the virtual machines.</p> <p>Each instance is identified by a unique hostname. An instance's hostname is written as the key followed by its index (1-based). The following map: <pre><code>instances = {\n  mgmt     = { type = \"p2-4gb\", tags = [...] },\n  login    = { type = \"p2-4gb\",     count = 1, tags = [...] },\n  node     = { type = \"c2-15gb-31\", count = 2, tags = [...] },\n  gpu-node = { type = \"gpu2.large\", count = 3, tags = [...] },\n}\n</code></pre> will spawn instances with the following hostnames: <pre><code>mgmt1\nlogin1\nnode1\nnode2\ngpu-node1\ngpu-node2\ngpu-node3\n</code></pre></p> <p>Hostnames must follow a set of rules, from <code>hostname</code> man page:</p> <p>Valid characters for hostnames are ASCII letters from a to z, the digits from 0 to 9, and the hyphen (-). A hostname may not start with a hyphen.</p> <p>Two attributes are expected to be defined for each instance: 1. <code>type</code>: name for varying combinations of CPU, memory, GPU, etc. (i.e: <code>t2.medium</code>); 2. <code>tags</code>: list of labels that defines the role of the instance.</p>"},{"location":"admin-guide/deployment/cloud/#471-tags","title":"4.7.1 tags","text":"<p>Tags are used in the Terraform code to identify if devices (volume, network) need to be attached to an instance, while in Ansible code tags are used to identify roles of the instances.</p> <p>Terraform tags: - <code>login</code>: identify instances that will be pointed by the domain name A record - <code>pool</code>: identify instances that will be created only if their hostname appears in the <code>var.pool</code> list. - <code>proxy</code>: identify instances that will be pointed by the vhost A records - <code>public</code>: identify instances that need to have a public ip address and be accessible from the Internet - <code>Ansible</code>: identify the instance that will be configured as the main Ansible server - <code>spot</code>: identify instances that are to be spawned as spot/preemptible instances. This tag is supported in AWS, Azure and GCP and ignored by OpenStack and OVH. - <code>efa</code>: attach an Elastic Fabric Adapter network interface to the instance. This tag is supported in AWS. - <code>ssl</code>: identify instances that will receive a copy of the SSL wildcard certificate for the domain</p> <p>Ansible tags expected by the ubiq-playbooks environment. - <code>login</code>: identify a login instance (minimum: 1 CPUs, 2GB RAM) - <code>mgmt</code>: identify a management instance i.e: FreeIPA server, Slurm controller, Slurm DB (minimum: 2 CPUs, 6GB RAM) - <code>nfs</code>: identify the instance that will act as an NFS server. - <code>node</code>: identify a compute node instance (minimum: 1 CPUs, 2GB RAM) - <code>pool</code>: when combined with <code>node</code>, it identifies compute nodes that Slurm can resume/suspend to meet workload demand. - <code>proxy</code>: identify the instance that will run the Caddy reverse proxy and JupyterHub.</p> <p>In the Ubiquity Ansible environment, an instance cannot be tagged as <code>mgmt</code> and <code>proxy</code>.</p> <p>You are free to define your own additional tags.</p>"},{"location":"admin-guide/deployment/cloud/#472-optional-attributes","title":"4.7.2 Optional attributes","text":"<p>Optional attributes can be defined: 1. <code>count</code>: number of virtual machines with this combination of hostname prefix, type and tags to create (default: 1). 2. <code>image</code>: specification of the image to use for this instance type. (default: global <code>image</code> value). Refer to section 10.12 - Create a compute node image to learn how this attribute can be leveraged to accelerate compute node configuration. 3. <code>disk_size</code>: size in gibibytes (GiB) of the instance's root disk containing the operating system and service software (default: see the next table). 4. <code>disk_type</code>: type of the instance's root disk (default: see the next table).</p> <p>Default root disk's attribute value per provider: | Provider | <code>disk_type</code> | <code>disk_size</code> (GiB) | | -------- | :---------- | ----------------: | | Azure    |<code>Premium_LRS</code>| 30                | | AWS      | <code>gp2</code>       | 10                | | GCP      | <code>pd-ssd</code>    | 20                | | OpenStack| <code>null</code>      | 10                | | OVH      | <code>null</code>      | 10                |</p> <p>For some cloud providers, it possible to define additional attributes. The following sections present the available attributes per provider.</p>"},{"location":"admin-guide/deployment/cloud/#aws","title":"AWS","text":"<p>For instances with the <code>spot</code> tags, these attributes can also be set: - <code>wait_for_fulfillment</code> (default: true) - <code>spot_type</code> (default: permanent) - <code>instance_interruption_behavior</code> (default: stop) - <code>spot_price</code> (default: not set) - <code>block_duration_minutes</code> (default: not set) [note 1] For more information on these attributes, refer to <code>aws_spot_instance_request</code> argument reference</p> <p>Note 1: <code>block_duration_minutes</code> is not available to new AWS accounts or accounts without billing history - AWS EC2 Spot Instance requests. When not available, its usage can trigger quota errors like this: <pre><code>Error requesting spot instances: MaxSpotInstanceCountExceeded: Max spot instance count exceeded\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#azure","title":"Azure","text":"<p>For instances with the <code>spot</code> tags, these attributes can also be set: - <code>max_bid_price</code> (default: not set) - <code>eviction_policy</code> (default: <code>Deallocate</code>) For more information on these attributes, refer to <code>azurerm_linux_virtual_machine</code> argument reference</p>"},{"location":"admin-guide/deployment/cloud/#gcp","title":"GCP","text":"<ul> <li><code>gpu_type</code>: name of the GPU model to attach to the instance. Refer to Google Cloud documentation for the list of available models per region</li> <li><code>gpu_count</code>: number of GPUs of the <code>gpu_type</code> model to attach to the instance</li> </ul>"},{"location":"admin-guide/deployment/cloud/#473-post-build-modification-effect","title":"4.7.3 Post build modification effect","text":"<p>Modifying any part of the map after the cluster is built will only affect the type of instances associated with what was modified at the next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#48-volumes","title":"4.8 volumes","text":"<p>The <code>volumes</code> variable is a map that defines the block devices that should be attached to instances that have the corresponding key in their list of tags. To each instance with the tag, unique block devices are attached, no multi-instance attachment is supported.</p> <p>Each volume in map is defined a key corresponding to its and a map of attributes: - <code>size</code>: size of the block device in GB. - <code>type</code> (optional): type of volume to use. Default value per provider:   - Azure: <code>Premium_LRS</code>   - AWS: <code>gp2</code>   - GCP: <code>pd-ssd</code>   - OpenStack: <code>null</code>   - OVH: <code>null</code></p> <p>Volumes with a tag that have no corresponding instance will not be created.</p> <p>In the following example: <pre><code>instances = { \n  server = { type = \"p4-6gb\", tags = [\"nfs\"] }\n}\nvolumes = {\n  nfs = {\n    home = { size = 100 }\n    project = { size = 100 }\n    scratch = { size = 100 }\n  }\n  mds = {\n    oss1 = { size = 500 }\n    oss2 = { size = 500 }\n  }\n}\n</code></pre></p> <p>The instance <code>server1</code> will have three volumes attached to it. The volumes tagged <code>mds</code> are not created since no instances have the corresponding tag.</p> <p>To define an infrastructure with no volumes, set the <code>volumes</code> variable to an empty map: <pre><code>volumes = {}\n</code></pre></p> <p>Post build modification effect: destruction of the corresponding volumes and attachments, and creation of new empty volumes and attachments. If an no instance with a corresponding tag exist following modifications, the volumes will be deleted.</p>"},{"location":"admin-guide/deployment/cloud/#49-public_keys","title":"4.9 public_keys","text":"<p>List of SSH public keys that will have access to your cluster sudoer account.</p> <p>Note 1: You will need to add the private key associated with one of the public keys to your local authentication agent (i.e: <code>ssh-add</code>) because Terraform will use this key to copy some configuration files with scp on the cluster. Otherwise, Ubiquity can create a key pair for unique to this cluster, see section 4.15 - generate_ssh_key (optional).</p> <p>Post build modification effect: trigger scp of hieradata files at next <code>terraform apply</code>. The sudoer account <code>authorized_keys</code> file will be updated by each instance's Ansible agent following the copy of the hieradata files.</p>"},{"location":"admin-guide/deployment/cloud/#410-nb_users-optional","title":"4.10 nb_users (optional)","text":"<p>default value: 0</p> <p>Defines how many guest user accounts will be created in FreeIPA. Each user account shares the same randomly generated password. The usernames are defined as <code>userX</code> where <code>X</code> is a number between 1 and the value of <code>nb_users</code> (zero-padded, i.e.: <code>user01 if X &lt; 100</code>, <code>user1 if X &lt; 10</code>).</p> <p>If an NFS NFS <code>home</code> volume is defined, each user will have a home folder on a shared NFS storage hosted on the NFS server node.</p> <p>User accounts do not have sudoer privileges. If you wish to use <code>sudo</code>, you will have to login using the sudoer account and the SSH keys listed in <code>public_keys</code>.</p> <p>If you would like to add a user account after the cluster is built, refer to section 10.3 and 10.4.</p> <p>Requirement: Must be an integer, minimum value is 0.</p> <p>Post build modification effect: trigger scp of vars config files at next <code>terraform apply</code>. If <code>nb_users</code> is increased, new guest accounts will be created during the following Ansible run on <code>mgmt1</code>. If <code>nb_users</code> is decreased, it will have no effect: the guest accounts already created will be left intact.</p>"},{"location":"admin-guide/deployment/cloud/#411-guest_passwd-optional","title":"4.11 guest_passwd (optional)","text":"<p>default value: 4 random words separated by dots</p> <p>Defines the password for the guest user accounts instead of using a randomly generated one.</p> <p>Requirement: Minimum length 8 characters.</p> <p>The password can be provided in a PKCS7 encrypted form. Refer to sub-section 4.13.1 Encrypting hieradata secrets for instructions on how to encrypt the password.</p> <p>Post build modification effect: trigger scp of hieradata files at next <code>terraform apply</code>. Password of all guest accounts will be changed to match the new password value.</p>"},{"location":"admin-guide/deployment/cloud/#412-sudoer_username-optional","title":"4.12 sudoer_username (optional)","text":"<p>default value: <code>centos</code></p> <p>Defines the username of the account with sudo privileges. The account ssh authorized keys are configured with the SSH public keys with <code>public_keys</code>.</p> <p>Post build modification effect: none. To change sudoer username, destroy the cluster or redefine the value of [<code>sudoer_username</code>](https://github.com/ubiquitycluster/ubiq-playbooks#profilelogin <code>vars/main.yml</code>.</p>"},{"location":"admin-guide/deployment/cloud/#413-vars-optional","title":"4.13 vars (optional)","text":"<p>default value: empty string</p> <p>Defines custom variable values that are injected in the Ansible vars file. Useful to override common configuration of Ansible configuration for the playbooks.</p> <p>List of useful examples: - Receive logs of Ansible runs with changes to your email, add the following line to the string:     <pre><code>profile::base::admin_email: \"me@example.org\"\n</code></pre> - Define ip addresses that can never be banned by fail2ban:     <pre><code>profile::fail2ban::ignore_ip: ['132.203.0.0/16', '8.8.8.8']\n</code></pre> - Remove one-time password field from JupyterHub login page:     <pre><code>jupyterhub::enable_otp_auth: false\n</code></pre></p> <p>Refer to the following Ansible modules' documentation to know more about the key-values that can be defined: - [ubiq-playbooks](https://github.com/ubiquitycluster/ubiq-playbooks/blob/main/README.md#AnsibleAnsible-jupyterhub](https://github.com/ubiquitycluster/Ansible-jupyterhub/blob/main/README.md#hieradata-configuration)</p> <p>The file created from this string can be found on <code>Ansible</code> as <pre><code>/etc/Ansiblelabs/data/user_data.yaml\n</code></pre></p> <p>Requirement: The string needs to respect the YAML syntax.</p> <p>Post build modification effect: trigger scp of hieradata files at next <code>terraform apply</code>. Each instance's Ansible agent will be reloaded following the copy of the hieradata files.</p>"},{"location":"admin-guide/deployment/cloud/#4131-encrypting-hieradata-secrets","title":"4.13.1. Encrypting hieradata secrets","text":"<p>If you plan to track the cluster configuration files in git (i.e:<code>main.tf</code>, <code>user_data.yaml</code>), it would be a good idea to encrypt the sensitive property values.</p> <p>Ubiquity uses Ansible hiera-eyaml to provide a per-value encryption of sensitive properties to be used by Ansible.</p> <p>To encrypt the data, you need to access the eyaml public certificate file of your cluster. This file is located on the Ansible server at <code>/opt/Ansiblelabs/Ansible/eyaml/public_key.pkcs7.pem</code>. With the public certificate file, you can encrypt the values with eyaml: <pre><code>eyaml encrypt -l profile::myclass::password -s 'your-secret' --pkcs7-public-key public_key.pkcs7.pem -o string\n</code></pre></p> <p>You can encrypt the value remotely using SSH jump host: <pre><code>ssh -J centos@your-cluster.yourdomain.cloud centos@Ansible /opt/Ansiblelabs/Ansible/bin/eyaml encrypt  -l profile::myclass::password -s 'your-secret' --pkcs7-public-key=/etc/Ansiblelabs/Ansible/eyaml/public_key.pkcs7.pem -o string\n</code></pre></p> <p>The openssl command-line can also be used to encrypt a value with the certificate file: <pre><code>echo 'your-secret' |  openssl smime -encrypt -aes-256-cbc -outform der public_key.pkcs7.pem | base64 | xargs printf \"ENC['PKCS7,%s']\\n\"\n</code></pre></p> <p>To learn more about <code>public_key.pkcs7.pem</code> and how it can be generated before the cluster creation, refer to section 10.13 Generate and replace Ansible hieradata encryption keys.</p>"},{"location":"admin-guide/deployment/cloud/#414-firewall_rules-optional","title":"4.14 firewall_rules (optional)","text":"<p>default value: <pre><code>[\n  { \"name\" = \"SSH\",     \"from_port\" = 22,    \"to_port\" = 22,    \"ip_protocol\" = \"tcp\", \"cidr\" = \"0.0.0.0/0\" },\n  { \"name\" = \"HTTP\",    \"from_port\" = 80,    \"to_port\" = 80,    \"ip_protocol\" = \"tcp\", \"cidr\" = \"0.0.0.0/0\" },\n  { \"name\" = \"HTTPS\",   \"from_port\" = 443,   \"to_port\" = 443,   \"ip_protocol\" = \"tcp\", \"cidr\" = \"0.0.0.0/0\" },\n  { \"name\" = \"Globus\",  \"from_port\" = 2811,  \"to_port\" = 2811,  \"ip_protocol\" = \"tcp\", \"cidr\" = \"54.237.254.192/29\" },\n  { \"name\" = \"MyProxy\", \"from_port\" = 7512,  \"to_port\" = 7512,  \"ip_protocol\" = \"tcp\", \"cidr\" = \"0.0.0.0/0\" },\n  { \"name\" = \"GridFTP\", \"from_port\" = 50000, \"to_port\" = 51000, \"ip_protocol\" = \"tcp\", \"cidr\" = \"0.0.0.0/0\" }\n]\n</code></pre></p> <p>Defines a list of firewall rules that control external traffic to the public nodes. Each rule is defined as a map of fives key-value pairs : <code>name</code>, <code>from_port</code>, <code>to_port</code>, <code>ip_protocol</code> and <code>cidr</code>. To add new rules, you will have to recopy the preceding list and add rules to it.</p> <p>Post build modification effect: modify the cloud provider firewall rules at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#415-generate_ssh_key-optional","title":"4.15 generate_ssh_key (optional)","text":"<p>default_value: <code>false</code></p> <p>If true, Terraform will generate an ssh key pair that would then be used when copying file with Terraform file-provisioner. The public key will be added to the sudoer account authorized keys.</p> <p>This parameter is useful when Terraform does not have access to one of the private key associated with the public keys provided in <code>public_keys</code>.</p> <p>Post build modification effect: - <code>false</code> -&gt; <code>true</code>: will cause Terraform failure. Terraform will try to use the newly created private SSH key to connect to the cluster, while the corresponding public SSH key is yet registered with the sudoer account. - <code>true</code> -&gt; <code>false</code>: will trigger a scp of terraform_data.yaml at next terraform apply. The Terraform public SSH key will be removed from the sudoer account <code>authorized_keys</code> file at next Ansible agent run.</p>"},{"location":"admin-guide/deployment/cloud/#416-software_stack-optional","title":"4.16 software_stack (optional)","text":"<p>default_value: <code>ubiquity</code></p> <p>Defines the research computing software stack to be provided. The default value <code>ubiquity</code> provides the Compute Canada software stack, but Ubiquity also supports the EESSI software stack (as an alternative) by setting this value to <code>eessi</code>.</p> <p>Post build modification effect: trigger scp of hieradata files at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#417-pool-optional","title":"4.17 pool (optional)","text":"<p>default_value: <code>[]</code></p> <p>Defines a list of hostnames with the tag <code>\"pool\"</code> that have to be online. This variable is typically managed by the workload scheduler through Terraform API. For more information, refer to Enable Ubiquity Autoscaling</p> <p>Post build modification effect: <code>pool</code> tagged hosts with name present in the list will be instantiated, others will stay uninstantiated or will be destroyed if previously instantiated.</p>"},{"location":"admin-guide/deployment/cloud/#5-cloud-specific-configuration","title":"5. Cloud Specific Configuration","text":""},{"location":"admin-guide/deployment/cloud/#51-amazon-web-services","title":"5.1 Amazon Web Services","text":""},{"location":"admin-guide/deployment/cloud/#511-region","title":"5.1.1 region","text":"<p>Defines the label of the AWS EC2 region where the cluster will be created (i.e.: <code>us-east-2</code>).</p> <p>Requirement: Must be in the list of available EC2 regions.</p> <p>Post build modification effect: rebuild of all resources at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#512-availability_zone-optional","title":"5.1.2 availability_zone (optional)","text":"<p>default value: None</p> <p>Defines the label of the data center inside the AWS region where the cluster will be created (i.e.: <code>us-east-2a</code>). If left blank, it chosen at random amongst the availability zones of the selected region.</p> <p>Requirement: Must be in a valid availability zone for the selected region. Refer to AWS documentation to find out how list the availability zones.</p>"},{"location":"admin-guide/deployment/cloud/#52-microsoft-azure","title":"5.2 Microsoft Azure","text":""},{"location":"admin-guide/deployment/cloud/#521-location","title":"5.2.1 location","text":"<p>Defines the label of the Azure location where the cluster will be created (i.e.: <code>eastus</code>).</p> <p>Requirement: Must be a valid Azure location. To get the list of available location, you can use Azure CLI : <code>az account list-locations -o table</code>.</p> <p>Post build modification effect: rebuild of all resources at next <code>terraform apply</code>.</p> <p>Post build modification effect: rebuild of all instances and disks at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#522-azure_resource_group-optional","title":"5.2.2 azure_resource_group (optional)","text":"<p>default value: None</p> <p>Defines the name of an already created resource group to use. Terraform will no longer attempt to manage a resource group for Ubiquity if this variable is defined and will instead create all resources within the provided resource group. Define this if you wish to use an already created resource group or you do not have a subscription-level access to create and destroy resource groups.</p> <p>Post build modification effect: rebuild of all instances at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#523-plan-optional","title":"5.2.3 plan (optional)","text":"<p>default value: <pre><code>{\n  name      = null\n  product   = null\n  publisher = null\n}\n</code></pre></p> <p>Purchase plan information for Azure Marketplace image. Certain images from Azure Marketplace requires a terms acceptance or a fee to be used. When using this kind of image, you must supply the plan details.</p> <p>For example, to use the official AlmaLinux image, you have to first add it to your account. Then to use it with Ubiquity, you must supply the following plan information: <pre><code>plan = {\n  name      = \"8_5\"\n  product   = \"almalinux\"\n  publisher = \"almalinux\"\n}\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#53-google-cloud","title":"5.3 Google Cloud","text":""},{"location":"admin-guide/deployment/cloud/#531-project","title":"5.3.1 project","text":"<p>Defines the label of the unique identifier associated with the Google Cloud project in which the resources will be created. It needs to corresponds to GCP project ID, which is composed of the project name and a randomly assigned number.</p> <p>Requirement: Must be a valid Google Cloud project ID.</p> <p>Post build modification effect: rebuild of all resources at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#532-region","title":"5.3.2 region","text":"<p>Defines the name of the specific geographical location where the cluster resources will be hosted.</p> <p>Requirement: Must be a valid Google Cloud region. Refer to Google Cloud documentation for the list of available regions and their characteristics.</p>"},{"location":"admin-guide/deployment/cloud/#533-zone-optional","title":"5.3.3 zone (optional)","text":"<p>default value: None</p> <p>Defines the name of the zone within the region where the cluster resources will be hosted.</p> <p>Requirement: Must be a valid Google Cloud zone. Refer to Google Cloud documentation for the list of available zones and their characteristics.</p>"},{"location":"admin-guide/deployment/cloud/#54-openstack-and-ovh","title":"5.4 OpenStack and OVH","text":""},{"location":"admin-guide/deployment/cloud/#541-os_floating_ips-optional","title":"5.4.1 os_floating_ips (optional)","text":"<p>default value: <code>{}</code></p> <p>Defines a map as an association of instance names (key) to pre-allocated floating ip addresses (value). Example: <pre><code>  os_floating_ips = {\n    login1 = 132.213.13.59\n    login2 = 132.213.13.25\n  }\n</code></pre> - instances tagged as public that have an entry in this map will be assigned the corresponding ip address; - instances tagged as public that do not have an entry in this map will be assigned a floating ip managed by Terraform. - instances not tagged as public that have an entry in this map will not be assigned a floating ip.</p> <p>This variable can be useful if you manage your DNS manually and you would like the keep the same domain name for your cluster at each build.</p> <p>Post build modification effect: change the floating ips assigned to the public instances.</p>"},{"location":"admin-guide/deployment/cloud/#542-os_ext_network-optional","title":"5.4.2 os_ext_network (optional)","text":"<p>default value: None</p> <p>Defines the name of the external network that provides the floating ips. Define this only if your OpenStack cloud provides multiple external networks, otherwise, Terraform can find it automatically.</p> <p>Post build modification effect: change the floating ips assigned to the public nodes.</p>"},{"location":"admin-guide/deployment/cloud/#544-subnet_id-optional","title":"5.4.4 subnet_id (optional)","text":"<p>default value: None</p> <p>Defines the ID of the internal IPV4 subnet to which the instances are connected. Define this if you have or intend to have more than one subnets defined in your OpenStack project. Otherwise, Terraform can find it automatically. Can be used to force a v4 subnet when both v4 and v6 exist.</p> <p>Post build modification effect: rebuild of all instances at next <code>terraform apply</code>.</p>"},{"location":"admin-guide/deployment/cloud/#6-dns-configuration-and-ssl-certificates","title":"6. DNS Configuration and SSL Certificates","text":"<p>Some functionalities in Ubiquity require the registration of DNS records under the cluster name in the selected domain. This includes web services like JupyterHub, Globus and FreeIPA web portal.</p> <p>If your domain DNS records are managed by one of the supported providers, follow the instructions in the corresponding sections to have the DNS records and SSL certificates managed by Ubiquity.</p> <p>If your DNS provider is not supported, you can manually create the DNS records and generate the SSL certificates. Refer to the last subsection for more details.</p> <p>Requirement: A private key associated with one of the public keys needs to be tracked (i.e: <code>ssh-add</code>) by the local authentication agent (i.e: <code>ssh-agent</code>). This module uses the ssh-agent tracked SSH keys to authenticate and to copy SSL certificate files to the proxy nodes after their creation.</p>"},{"location":"admin-guide/deployment/cloud/#61-cloudflare","title":"6.1 Cloudflare","text":"<ol> <li>Uncomment the <code>dns</code> module for Cloudflare in your <code>main.tf</code>.</li> <li>Uncomment the <code>output \"hostnames\"</code> block.</li> <li>In the <code>dns</code> module, configure the variable <code>email</code> with your email address. This will be used to generate the Let's Encrypt certificate.</li> <li>Download and install the Cloudflare Terraform module: <code>terraform init</code>.</li> <li>Export the environment variables <code>CLOUDFLARE_EMAIL</code> and <code>CLOUDFLARE_API_KEY</code>, where <code>CLOUDFLARE_EMAIL</code> is your Cloudflare account email address and <code>CLOUDFLARE_API_KEY</code> is your account Global API Key available in your Cloudflare profile.</li> </ol>"},{"location":"admin-guide/deployment/cloud/#612-cloudflare-api-token","title":"6.1.2 Cloudflare API Token","text":"<p>If you prefer using an API token instead of the global API key, you will need to configure a token with the following four permissions with the Cloudflare API Token interface.</p> Section Subsection Permission Account Account Settings Read Zone Zone Settings Read Zone Zone Read Zone DNS Edit <p>Instead of step 5, export only <code>CLOUDFLARE_API_TOKEN</code>, <code>CLOUDFLARE_ZONE_API_TOKEN</code>, and <code>CLOUDFLARE_DNS_API_TOKEN</code> equal to the API token generated previously.</p>"},{"location":"admin-guide/deployment/cloud/#62-google-cloud","title":"6.2 Google Cloud","text":"<p>requirement: Install the Google Cloud SDK</p> <ol> <li>Login to your Google account with gcloud CLI : <code>gcloud auth application-default login</code></li> <li>Uncomment the <code>dns</code> module for Google Cloud in your <code>main.tf</code>.</li> <li>Uncomment the <code>output \"hostnames\"</code> block.</li> <li>In <code>main.tf</code>'s <code>dns</code> module, configure the variable <code>email</code> with your email address. This will be used to generate the Let's Encrypt certificate.</li> <li>In <code>main.tf</code>'s <code>dns</code> module, configure the variables <code>project</code> and <code>zone_name</code> with their respective values as defined by your Google Cloud project.</li> <li>Download and install the Google Cloud Terraform module: <code>terraform init</code>.</li> </ol>"},{"location":"admin-guide/deployment/cloud/#63-unsupported-providers","title":"6.3 Unsupported providers","text":"<p>If your DNS provider is not currently supported by Ubiquity, you can create the DNS records and the SSL certificates manually.</p>"},{"location":"admin-guide/deployment/cloud/#631-dns-records","title":"6.3.1 DNS Records","text":"<p>Ubiquity provides a module that creates a text file with the DNS records that can then be imported manually in your DNS zone. To use this module, add the following snippet to your <code>main.tf</code>:</p> <pre><code>module \"dns\" {\n    source           = \"./dns/txt\"\n    name             = module.openstack.cluster_name\n    domain           = module.openstack.domain\n    public_ip        = module.openstack.ip\n}\n</code></pre> <p>Find and replace <code>openstack</code> in the previous snippet by your cloud provider of choice if not OpenStack (i.e: <code>aws</code>, <code>gcp</code>, etc.).</p> <p>The file will be created after the <code>terraform apply</code> in the same folder as your <code>main.tf</code> and will be named as <code>${name}.${domain}.txt</code>.</p>"},{"location":"admin-guide/deployment/cloud/#632-ssl-certificates","title":"6.3.2 SSL Certificates","text":"<p>Ubiquity generates with Let's Encrypt a wildcard certificate for <code>*.cluster_name.domain</code>. You can use certbot DNS challenge plugin to generate the wildcard certificate.</p> <p>You will then need to copy the certificate files in the proper location on each login node. The reverse proxy configuration expects the following files to exist: - <code>/etc/letsencrypt/live/${domain_name}/fullchain.pem</code> - <code>/etc/letsencrypt/live/${domain_name}/privkey.pem</code> - <code>/etc/letsencrypt/live/${domain_name}/chain.pem</code></p> <p>Refer to the [reverse proxy configuration](https://github.com/ubiquitycluster/ubiq-playbooks/blob/main/site/profile/manifests/reverse_proxy.ppAnsiblemore details.</p>"},{"location":"admin-guide/deployment/cloud/#64-acme-account-private-key","title":"6.4 ACME Account Private Key","text":"<p>To create the wildcard SSL certificate associated with the domain name, Ubiquity creates a private key and register a new ACME account with this key. This account registration process is done for each new cluster. However, ACME limits the number of new accounts that can be created to a maximum of 10 per IP Address per 3 hours.</p> <p>If you plan to create more than 10 clusters per 3 hours, we recommend registering an ACME account first and then provide its private key in PEM format to Ubiquity DNS module, using the <code>acme_key_pem</code> variable.</p>"},{"location":"admin-guide/deployment/cloud/#641-how-to-generate-an-acme-account-private-key","title":"6.4.1 How to Generate an ACME Account Private Key","text":"<p>In a separate folder, create a file with the following content <pre><code>terraform {\n  required_version = \"&gt;= 1.2.1\"\n  required_providers {\n    acme = {\n      source = \"vancluever/acme\"\n    }\n    tls = {\n      source = \"hashicorp/tls\"\n    }\n  }\n}\n\nvariable \"email\" {}\n\nprovider \"acme\" {\n  server_url = \"https://acme-v02.api.letsencrypt.org/directory\"\n}\nresource \"tls_private_key\" \"private_key\" {\n  algorithm = \"RSA\"\n}\nresource \"acme_registration\" \"reg\" {\n  account_key_pem = tls_private_key.private_key.private_key_pem\n  email_address   = var.email\n}\nresource \"local_file\" \"acme_key_pem\" {\n    content     = tls_private_key.private_key.private_key_pem\n    filename = \"acme_key.pem\"\n}\n</code></pre></p> <p>In the same folder, enter the following commands and follow the instructions: <pre><code>terraform init\nterraform apply\n</code></pre></p> <p>Once done, copy the file named <code>acme_key.pem</code> somewhere safe, and where you will be able to refer to later on. Then, when the time comes to create a new cluster, add the following variable to the DNS module in your <code>main.tf</code>: <pre><code>acme_key_pem = file(\"path/to/your/acme_key.pem\")\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#7-planning","title":"7. Planning","text":"<p>Once your initial cluster configuration is done, you can initiate a planning phase where you will ask Terraform to communicate with your cloud provider and verify that your cluster can be built as it is described by the <code>main.tf</code> configuration file.</p> <p>Terraform should now be able to communicate with your cloud provider. To test your configuration file, enter the following command <pre><code>terraform plan\n</code></pre></p> <p>This command will validate the syntax of your configuration file and communicate with the provider, but it will not create new resources. It is only a dry-run. If Terraform does not report any error, you can move to the next step. Otherwise, read the errors and fix your configuration file accordingly.</p>"},{"location":"admin-guide/deployment/cloud/#8-deployment","title":"8. Deployment","text":"<p>To create the resources defined by your main, enter the following command <pre><code>terraform apply\n</code></pre></p> <p>The command will produce the same output as the <code>plan</code> command, but after the output it will ask for a confirmation to perform the proposed actions. Enter <code>yes</code>.</p> <p>Terraform will then proceed to create the resources defined by the configuration file. It should take a few minutes. Once the creation process is completed, Terraform will output the guest account usernames and password, the sudoer username and the floating ip of the login node.</p> <p>Warning: although the instance creation process is finished once Terraform outputs the connection information, you will not be able to connect and use the cluster immediately. The instance creation is only the first phase of the cluster-building process. The configuration: the creation of the user accounts, installation of FreeIPA, Slurm, configuration of JupyterHub, etc.; takes around 15 minutes after the instances are created.</p> <p>Once it is booted, you can follow an instance configuration process by looking at:</p> <ul> <li><code>/var/log/cloud-init-output.log</code></li> <li><code>journalctl -u Ansible</code></li> </ul> <p>If unexpected problems occur during configuration, you can provide these logs to the authors of Ubiquity to help you debug.</p>"},{"location":"admin-guide/deployment/cloud/#81-deployment-customization","title":"8.1 Deployment Customization","text":"<p>You can modify the <code>main.tf</code> at any point of your cluster's life and apply the modifications while it is running.</p> <p>Warning: Depending on the variables you modify, Terraform might destroy some or all resources, and create new ones. The effects of modifying each variable are detailed in the subsections of Configuration.</p> <p>For example, to increase the number of computes nodes by one. Open <code>main.tf</code>, add 1 to <code>node</code>'s <code>count</code> , save the document and call <pre><code>terraform apply\n</code></pre></p> <p>Terraform will analyze the difference between the current state and the future state, and plan the creation of a single new instance. If you accept the action plan, the instance will be created, provisioned and eventually automatically add to the Slurm cluster configuration.</p> <p>You could do the opposite and reduce the number of compute nodes to 0.</p>"},{"location":"admin-guide/deployment/cloud/#9-destruction","title":"9. Destruction","text":"<p>Once you're done working with your cluster and you would like to recover the resources, in the same folder as <code>main.tf</code>, enter: <pre><code>terraform destroy -refresh=false\n</code></pre></p> <p>The <code>-refresh=false</code> flag is to avoid an issue where one or many of the data sources return no results and stall the cluster destruction with a message like the following: <pre><code>Error: Your query returned no results. Please change your search criteria and try again.\n</code></pre> This type of error happens when for example the specified image no longer exists (see issue #40).</p> <p>As for <code>apply</code>, Terraform will output a plan that you will have to confirm by entering <code>yes</code>.</p> <p>Warning: once the cluster is destroyed, nothing will be left, even the shared storage will be erased.</p>"},{"location":"admin-guide/deployment/cloud/#91-instance-destruction","title":"9.1 Instance Destruction","text":"<p>It is possible to destroy only the instances and keep the rest of the infrastructure like the floating ip, the volumes, the generated SSH host key, etc. To do so, set the count value of the instance type you wish to destroy to 0.</p>"},{"location":"admin-guide/deployment/cloud/#92-reset","title":"9.2 Reset","text":"<p>On some occasions, it is desirable to rebuild some of the instances from scratch. Using <code>terraform taint</code>, you can designate resources that will be rebuilt at next application of the plan.</p> <p>To rebuild the first login node : <pre><code>terraform taint 'module.openstack.openstack_compute_instance_v2.instances[\"login1\"]'\nterraform apply\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#10-customise-cluster-software-configuration","title":"10. customise Cluster Software Configuration","text":"<p>Once the cluster is online and configured, you can modify its configuration as you see fit. We list here how to do most commonly asked for customizations.</p> <p>Some customizations are done from the Ansible server instance (<code>Ansible</code>). To connect to the Ansible server, follow these steps:</p> <ol> <li>Make sure your SSH key is loaded in your ssh-agent.</li> <li>SSH in your cluster with forwarding of the authentication agent connection enabled: <code>ssh -A centos@cluster_ip</code>. Replace <code>centos</code> by the value of <code>sudoer_username</code> if it is different.</li> <li>SSH in the Ansible server instance: <code>ssh Ansible</code></li> </ol> <p>Note on Google Cloud: In GCP, OS Login lets you use Compute Engine IAM roles to manage SSH access to Linux instances. This feature is incompatible with Ubiquity. Therefore, it is turned off in the instances metadata (<code>enable-oslogin=\"FALSE\"</code>). The only account with sudoer rights that can log in the cluster is configured by the variable <code>sudoer_username</code> (default: <code>centos</code>).</p>"},{"location":"admin-guide/deployment/cloud/#101-disable-ansible","title":"10.1 Disable Ansible","text":"<p>If you plan to modify configuration files manually, you will need to disable Ansible. Otherwise, you might find out that your modifications have disappeared in a 30-minute window.</p> <p>Ansible executes a run every 30 minutes and at reboot. To disable Ansible: <pre><code>sudo Ansible agent --disable \"&lt;MESSAGE&gt;\"\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#102-replace-the-guest-account-password","title":"10.2 Replace the Guest Account Password","text":"<p>Refer to section 4.11.</p>"},{"location":"admin-guide/deployment/cloud/#103-add-ldap-users","title":"10.3 Add LDAP Users","text":"<p>Users can be added to Ubiquity LDAP database (FreeIPA) with either one of the following methods: hieradata, command-line, and Mokey web-portal. Each method is presented in the following subsections.</p> <p>New LDAP users are automatically assigned a home folder on NFS.</p> <p>Ubiquity determines if an LDAP user should be member of a Slurm account based on its POSIX groups. When a user is added to a POSIX group, a daemon try to match the group name to the following regular expression: <pre><code>(ctb|def|rpp|rrg)-[a-z0-9_-]*\n</code></pre></p> <p>If there is a match, the user will be added to a Slurm account with the same name, and will gain access to the corresponding project folder under <code>/project</code>.</p> <p>Note: The regular expression represents how Compute Canada names its resources allocation. The regular expression can be redefined, see <code>profile::accounts:::project_regex</code></p>"},{"location":"admin-guide/deployment/cloud/#1031-hieradata","title":"10.3.1 hieradata","text":"<p>Using the hieradata variable in the <code>main.tf</code>, it is possible to define LDAP users.</p> <p>Examples of LDAP user definition with hieradata are provided in ubiq-playbooks documentation.</p>"},{"location":"admin-guide/deployment/cloud/#1032-command-line","title":"10.3.2 Command-Line","text":"<p>To add a user account after the cluster is built, log in <code>mgmt1</code> and call: <pre><code>kinit admin\nIPA_GUEST_PASSWD=&lt;new_user_passwd&gt; /sbin/ipa_create_user.py &lt;username&gt; [--group &lt;group_name&gt;]\nkdestroy\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#1033-mokey","title":"10.3.3 Mokey","text":"<p>If user sign-up with Mokey is enabled, users can create their own account at <pre><code>https://mokey.yourcluster.domain.tld/auth/signup\n</code></pre></p> <p>It is possible that an administrator is required to enable the account with Mokey. You can access the administrative panel of FreeIPA at : <pre><code>https://ipa.yourcluster.domain.tld/\n</code></pre></p> <p>The FreeIPA administrator credentials can be retrieved from an encrypted file on the Ansible server. Refer to section 10.14 to know how.</p>"},{"location":"admin-guide/deployment/cloud/#104-increase-the-number-of-guest-accounts","title":"10.4 Increase the Number of Guest Accounts","text":"<p>To increase the number of guest accounts after creating the cluster with Terraform, simply increase the value of <code>nb_users</code>, then call : <pre><code>terraform apply\n</code></pre></p> <p>Each instance's Ansible agent will be reloaded following the copy of the hieradata files, and the new accounts will be created.</p>"},{"location":"admin-guide/deployment/cloud/#105-restrict-ssh-access","title":"10.5 Restrict SSH Access","text":"<p>By default, port 22 of the instances tagged <code>public</code> is reachable by the world. If you know the range of ip addresses that will connect to your cluster, we strongly recommend that you limit the access to port 22 to this range.</p> <p>To limit the access to port 22, refer to section 4.14 firewall_rules, and replace the <code>cidr</code> of the <code>SSH</code> rule to match the range of ip addresses that have be the allowed to connect to the cluster.</p>"},{"location":"admin-guide/deployment/cloud/#106-add-packages-to-jupyter-default-python-kernel","title":"10.6 Add Packages to Jupyter Default Python Kernel","text":"<p>The default Python kernel corresponds to the Python installed in <code>/opt/ipython-kernel</code>. Each compute node has its own copy of the environment. To add packages to this environment, add the following lines to <code>hieradata</code> in <code>main.tf</code>: <pre><code>jupyterhub::kernel::venv::packages:\n  - package_A\n  - package_B\n  - package_C\n</code></pre></p> <p>and replace <code>package_*</code> by the packages you need to install. Then call: <pre><code>terraform apply\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#107-activate-globus-endpoint","title":"10.7 Activate Globus Endpoint","text":"<p>Refer to Ubiquity Globus Endpoint documentation.</p>"},{"location":"admin-guide/deployment/cloud/#108-recovering-from-ansible-rebuild","title":"10.8 Recovering from Ansible rebuild","text":"<p>The modifications of some of the parameters in the <code>main.tf</code> file can trigger the rebuild of the <code>Ansible</code> instance. This instance hosts the Ansible Server on which depends the Ansible agent of the other instances. When <code>Ansible</code> is rebuilt, the other Ansible agents cease to recognize Ansible Server identity since the Ansible Server identity and certificates have been regenerated.</p> <p>To fix the Ansible agents, you will need to apply the following commands on each instance other than <code>Ansible</code> once <code>Ansible</code> is rebuilt: <pre><code>sudo systemctl stop Ansible\nsudo rm -rf /etc/Ansiblelabs/Ansible/ssl/\nsudo systemctl start Ansible\n</code></pre></p> <p>Then, on <code>Ansible</code>, you will need to sign the new certificate requests made by the instances. First, you can list the requests: <pre><code>sudo /opt/Ansiblelabs/bin/Ansibleserver ca list\n</code></pre></p> <p>Then, if every instance is listed, you can sign all requests: <pre><code>sudo /opt/Ansiblelabs/bin/Ansibleserver ca sign --all\n</code></pre></p> <p>If you prefer, you can sign individual request by specifying their name: <pre><code>sudo /opt/Ansiblelabs/bin/Ansibleserver ca sign --certname NAME[,NAME]\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#109-dealing-with-banned-ip-addresses-fail2ban","title":"10.9 Dealing with banned ip addresses (fail2ban)","text":"<p>Login nodes run fail2ban, an intrusion prevention software that protects login nodes from brute-force attacks. fail2ban is configured to ban ip addresses that attempted to login 20 times and failed in a window of 60 minutes. The ban time is 24 hours.</p> <p>In the context of a workshop with SSH novices, the 20-attempt rule might be triggered, resulting in participants banned and puzzled, which is a bad start for a workshop. There are solutions to mitigate this problem.</p>"},{"location":"admin-guide/deployment/cloud/#1091-define-a-list-of-ip-addresses-that-can-never-be-banned","title":"10.9.1 Define a list of ip addresses that can never be banned","text":"<p>fail2ban keeps a list of ip addresses that are allowed to fail to login without risking jail time. To add an ip address to that list,  add the following lines to the variable <code>hieradata</code> in <code>main.tf</code>: <pre><code>fail2ban::ignoreip:\n  - x.x.x.x\n  - y.y.y.y\n</code></pre> where <code>x.x.x.x</code> and <code>y.y.y.y</code> are ip addresses you want to add to the ignore list. The ip addresses can be written using CIDR notations. The ignore ip list on Ubiquity already includes <code>127.0.0.1/8</code> and the cluster subnet CIDR.</p> <p>Once the line is added, call: <pre><code>terraform apply\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#1092-remove-fail2ban-ssh-route-jail","title":"10.9.2 Remove fail2ban ssh-route jail","text":"<p>fail2ban rule that banned ip addresses that failed to connect with SSH can be disabled. To do so, add the following line to the variable <code>hieradata</code> in <code>main.tf</code>: <pre><code>fail2ban::jails: ['ssh-ban-root']\n</code></pre> This will keep the jail that automatically ban any ip that tries to login as root, and remove the ssh failed password jail.</p> <p>Once the line is added, call: <pre><code>terraform apply\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#1093-unban-ip-addresses","title":"10.9.3 Unban ip addresses","text":"<p>fail2ban ban ip addresses by adding rules to iptables. To remove these rules, you need to tell fail2ban to unban the ips.</p> <p>To list the ip addresses that are banned, execute the following command: <pre><code>sudo fail2ban-client status ssh-route\n</code></pre></p> <p>To unban ip addresses, enter the following command followed by the ip addresses you want to unban: <pre><code>sudo fail2ban-client set ssh-route unbanip\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#1094-disable-fail2ban","title":"10.9.4 Disable fail2ban","text":"<p>While this is not recommended, fail2ban can be completely disabled. To do so, add the following line to the variable <code>hieradata</code> in <code>main.tf</code>: <pre><code>fail2ban::service_ensure: 'stopped'\n</code></pre></p> <p>then call : <pre><code>terraform apply\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#1010-generate-a-new-ssl-certificate","title":"10.10 Generate a new SSL certificate","text":"<p>The SSL certificate configured by the dns module is valid for 90 days. If you plan to use your cluster for more than 90 days, you will need to generate a new SSL certificate before the one installed on the cluster expires.</p> <p>To generate a new certificate, use the following command on your computer: <pre><code>terraform taint 'module.dns.module.acme.acme_certificate.certificate'\n</code></pre></p> <p>Then apply the modification: <pre><code>terraform apply\n</code></pre></p> <p>The apply generates a new certificate, uploads it on the nodes that need it and reloads the reverse proxy if it is configured.</p>"},{"location":"admin-guide/deployment/cloud/#1011-set-selinux-in-permissive-mode","title":"10.11 Set SELinux in permissive mode","text":"<p>SELinux can be set in permissive mode to debug new workflows that would be prevented by SELinux from working properly. To do so, add the following line to the variable <code>hieradata</code> in <code>main.tf</code>: <pre><code>selinux::mode: 'permissive'\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#1012-create-a-compute-node-image","title":"10.12 Create a compute node image","text":"<p>When scaling the compute node pool, either manually by changing the count or automatically with Slurm autoscale, it can become beneficial to reduce the time spent configuring the machine when it boots for the first time, hence reducing the time requires before it becomes available in Slurm. One way to achieve this is to clone the root disk of a fully configured compute node and use it as the base image of future compute nodes.</p> <p>This process has three steps:</p> <ol> <li>Prepare the volume for image cloning</li> <li>Create the image</li> <li>Configure Ubiquity Terraform code to use the new image</li> </ol> <p>The following subsection explains how to accomplish each step.</p> <p>Warning: While it will work in most cases, avoid re-using the compute node image of a previous deployment. The preparation steps cleans most of the deployment specific configuration and secrets, but there is no guarantee that the configuration will be entirely compatible with a different deployment.</p>"},{"location":"admin-guide/deployment/cloud/#10121-prepare-the-volume-for-cloning","title":"10.12.1 Prepare the volume for cloning","text":"<p>The environment ubiq-playbooks installs a Ansible that prepares the voluAnsible cloning named <code>prepare4image.sh</code>.</p> <p>To make sure a node is ready for cloning, open its Ansible agent log and validate the catalog was successfully applied at least once: <pre><code>journalctl -u Ansible | grep \"Applied catalog\"\n</code></pre></p> <p>To prepare the volume for cloning, execute the following line while connected to the compute node: <pre><code>sudo /usr/sbin/prepare4image.sh\n</code></pre></p> <p>Be aware that, since it is preferable for the instance to be powered off when cloning its volume, the script halts the machine once it is completed. Therefore, after executing <code>prepare4image.sh</code>, you will be disconnected from the instance.</p> <p>The script <code>prepare4image.sh</code> executes the following steps in order:</p> <ol> <li>Stop and disable Ansible agent</li> <li>Stop and disable slurm compute node daemon (<code>slurmd</code>)</li> <li>Stop and disable consul agent daemon</li> <li>Stop and disable consul-template daemon</li> <li>Unenroll the host from the IPA server</li> <li>Remove Ansible agent configuration files in <code>/etc</code></li> <li>Remove consul agent identification files</li> <li>Unmount NFS directories</li> <li>Remove NFS directories <code>/etc/fstab</code></li> <li>Stop syslog</li> <li>Clear <code>/var/log/message</code> content</li> <li>Remove cloud-init's logs and artifacts so it can re-run</li> <li>Power off the machine</li> </ol>"},{"location":"admin-guide/deployment/cloud/#10122-create-the-image","title":"10.12.2 Create the image","text":"<p>Once the instance is powered off, access your cloud provider dashboard, find the instance and follow the provider's instructions to create the image.</p> <ul> <li>AWS</li> <li>Azure</li> <li>GCP</li> <li>OpenStack</li> <li>OVH</li> </ul> <p>Note down the name/id of the image you created, it will be needed during the next step.</p>"},{"location":"admin-guide/deployment/cloud/#10123-configure-ubiquity-terraform-code-to-use-the-new-image","title":"10.12.3 Configure Ubiquity Terraform code to use the new image","text":"<p>Edit your <code>main.tf</code> and add <code>image = \"name-or-id-of-your-image\"</code> to the dictionary defining the instance. The instance previously powered off will be powered on and future non-instantiated machines will use the image at the next execution of <code>terraform apply</code>.</p> <p>If the cluster is composed of heterogeneous compute nodes, it is possible to create an image for each type of compute nodes. Here is an example with Google Cloud <pre><code>instances = {\n  mgmt   = { type = \"n2-standard-2\", tags = [\"Ansible\", \"mgmt\", \"nfs\"], count = 1 }\n  login  = { type = \"n2-standard-2\", tags = [\"login\", \"public\", \"proxy\"], count = 1 }\n  node   = {\n    type = \"n2-standard-2\"\n    tags = [\"node\", \"pool\"]\n    count = 10\n    image = \"rocky-mc-cpu-node\"\n  }\n  gpu    = {\n    type = \"n1-standard-2\"\n    tags = [\"node\", \"pool\"]\n    count = 10\n    gpu_type = \"nvidia-tesla-t4\"\n    gpu_count = 1\n    image = \"rocky-mc-gpu-node\"\n  }\n}\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/#1013-generate-and-replace-ansible-hieradata-encryption-keys","title":"10.13 Generate and replace Ansible hieradata encryption keys","text":"<p>During the Ansible server initial boot, a pair of hiera-eyaml encryptions keys are generated in <code>/opt/Ansiblelabs/Ansible/eyaml</code>: - <code>private_key.pkcs7.pem</code> - <code>public_key.pkcs7.pem</code></p> <p>To encrypt the values before creating the cluster, the encryptions keys can be generated beforehand and then transferred on the Ansible server.</p> <p>The keys can be generated with <code>eyaml</code>: <pre><code>eyaml createkeys\n</code></pre></p> <p>or <code>openssl</code>: <pre><code>openssl req -x509 -nodes -days 100000 -newkey rsa:2048 -keyout private_key.pkcs7.pem -out public_key.pkcs7.pem -subj '/'\n</code></pre></p> <p>The resulting public key can then be used to encrypt secrets, while the private and the public keys have to be transferred on the Ansible server to allow it to decrypt the values.</p> <ol> <li>Transfer the keys on the Ansible server using SCP with SSH jumphost     <pre><code>scp -J centos@cluster.yourdomain.cloud {public,private}_key.pkcs7.pem centos@Ansible:~/\n</code></pre></li> <li>Replace the existing keys by the one transferred:     <pre><code>ssh -J centos@cluster.yourdomain.cloud centos@Ansible sudo cp {public,private}_key.pkcs7.pem /opt/Ansiblelabs/Ansible/eyaml\n</code></pre></li> <li>Remove the keys from the admin account home folder:     <pre><code>ssh -J centos@cluster.yourdomain.cloud centos@Ansible rm {public,private}_key.pkcs7.pem\n</code></pre></li> </ol> <p>To backup the encryption keys from an existing Ansible server:</p> <ol> <li>Create a readable copy of the encryption keys in the sudoer home account     <pre><code>ssh -J centos@cluster.yourdomain.cloud centos@Ansible 'sudo rsync --owner --group --chown=centos:centos /etc/Ansiblelabs/Ansible/eyaml/{public,private}_key.pkcs7.pem ~/'\n</code></pre></li> <li>Transfer the files locally:     <pre><code>scp -J centos@cluster.yourdomain.cloud centos@Ansible:~/{public,private}_key.pkcs7.pem .\n</code></pre></li> <li>Remove the keys from the sudoer account home folder:     <pre><code>ssh -J centos@cluster.yourdomain.cloud centos@Ansible rm {public,private}_key.pkcs7.pem\n</code></pre></li> </ol>"},{"location":"admin-guide/deployment/cloud/#1014-read-and-edit-secret-values-generated-at-boot","title":"10.14 Read and edit secret values generated at boot","text":"<p>During the cloud-init initialisation phase, <code>bootstrap.sh</code> script is executed. This script generates a Ansible encrypted secret values that are required by the Ubiquity Ansible environment: - <code>profile::consul::acl_api_token</code> - <code>profile::freeipa::mokey::password</code> - <code>profile::freeipa::server::admin_password</code> - <code>profile::freeipa::server::ds_password</code> - <code>profile::slurm::accounting::password</code> - <code>profile::slurm::base::munge_key</code></p> <p>To read or change the value of one of these keys, use <code>eyaml edit</code> command on the <code>Ansible</code> host, like this: <pre><code>sudo /opt/Ansiblelabs/Ansible/bin/eyaml edit \\\n  --pkcs7-private-key /etc/Ansiblelabs/Ansible/eyaml/boot_private_key.pkcs7.pem \\\n  --pkcs7-public-key /etc/Ansiblelabs/Ansible/eyaml/boot_public_key.pkcs7.pem \\\n  /etc/Ansiblelabs/code/environments/production/data/bootstrap.yaml\n</code></pre></p> <p>It also possible to redefine the values of these keys by adding the key-value pair to the hieradata configuration file. Refer to section 4.13 hieradata. User defined values take precedence over boot generated values in the Ubiquity Ansible data hierarchy.</p>"},{"location":"admin-guide/deployment/cloud/#11-customise-ubiquity-terraform-files","title":"11. customise Ubiquity Terraform Files","text":"<p>You can modify the Terraform module files in the folder named after your cloud provider (e.g: <code>gcp</code>, <code>openstack</code>, <code>aws</code>, etc.)</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/","title":"Terraform Cloud","text":"<p>This document explains how to use Ubiquity with Terraform Cloud.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#what-is-terraform-cloud","title":"What is Terraform Cloud?","text":"<p>Terraform Cloud is HashiCorp\u2019s managed service that allows to provision infrastructure using a web browser or a REST API instead of the command-line. This also means that the provisioned infrastructure parameters can be modified by a team and the state is stored in the cloud instead of a local machine.</p> <p>When provisioning in commercial cloud, Terraform Cloud can also provide a cost estimate of the resources.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#getting-started-with-terraform-cloud","title":"Getting started with Terraform Cloud","text":"<ol> <li>Create a Terraform Cloud account</li> <li>Create an organization, join one or choose one available to you</li> </ol>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#managing-a-ubiquity-cluster-with-terraform-cloud","title":"Managing a Ubiquity cluster with Terraform Cloud","text":""},{"location":"admin-guide/deployment/cloud/terraform_cloud/#creating-the-workspace","title":"Creating the workspace","text":"<ol> <li>Create a git repository in GitHub, GitLab, or any of the version control system provider supported by Terraform Cloud</li> <li>In this git repository, add a copy of the Ubiquity example <code>main.tf</code> available for the cloud of your choice</li> <li>Log in Terraform Cloud account</li> <li>Create a new workspace<ol> <li>Choose Type: \"Version control workflow\"</li> <li>Connect to VCS: choose the version control provider that hosts your repository</li> <li>Choose the repository that contains your <code>main.tf</code></li> <li>Configure settings: tweak the name and description to your liking</li> <li>Click on \"Create workspace\"</li> </ol> </li> </ol> <p>You will be redirected automatically to your new workspace.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#providing-cloud-provider-credentials-to-terraform-cloud","title":"Providing cloud provider credentials to Terraform Cloud","text":"<p>Terraform Cloud will invoke Terraform command-line in a remote virtual environment. For the CLI to be able to communicate with your cloud provider API, we need to define environment variables that Terraform will use to authenticate. The next sections explain which environment variables to define for each cloud provider and how to retrieve the values of the variable from the provider.</p> <p>If you plan on using these environment variables with multiple workspaces, it is recommended to create a credential variable set in Terraform Cloud.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#aws","title":"AWS","text":"<p>You need to define these environment variables: - <code>AWS_ACCESS_KEY_ID</code> - <code>AWS_SECRET_ACCESS_KEY</code> (sensitive)</p> <p>The value of these variables can either correspond to the value of access key created on the AWS Security Credentials - Access keys page, or you can add user dedicated to Terraform Cloud in AWS IAM Users, and use its access key.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#azure","title":"Azure","text":"<p>You need to define these environment variables: - <code>ARM_CLIENT_ID</code> - <code>ARM_CLIENT_SECRET</code> (sensitive) - <code>ARM_SUBSCRIPTION_ID</code> - <code>ARM_TENANT_ID</code></p> <p>Refer to Terraform Azure Provider - Creating a Service Principal to know how to create a Service Principal and retrieve the values for these environment variables.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#google-cloud","title":"Google Cloud","text":"<p>You need to define this environment variable: - <code>GOOGLE_CLOUD_KEYFILE_JSON</code> (sensitive)</p> <p>The value of the variable will be the content of a Google Cloud service account JSON key file expressed a single line string. Example: <pre><code>{\"type\": \"service_account\",\"project_id\": \"project-id-1234\",\"private_key_id\": \"abcd1234\",...}\n</code></pre></p> <p>You can use <code>jq</code> to format the string from the JSON file provided by Google: <pre><code>jq . -c project-name-123456-abcdefjg.json\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#openstack-ovh","title":"OpenStack / OVH","text":"<p>You need to define these environment variables: - <code>OS_AUTH_URL</code> - <code>OS_PROJECT_ID</code> - <code>OS_REGION_NAME</code> - <code>OS_INTERFACE</code> - <code>OS_IDENTITY_API_VERSION</code> - <code>OS_USER_DOMAIN_NAME</code> - <code>OS_USERNAME</code> - <code>OS_PASSWORD</code> (sensitive)</p> <p>Apart from <code>OS_PASSWORD</code>, the values for these variables are available in OpenStack RC file provided for your project.</p> <p>If you prefer to use OpenStack application credentials, you need to define at least these variables: - <code>OS_AUTH_TYPE</code>  - <code>OS_AUTH_URL</code> - <code>OS_APPLICATION_CREDENTIAL_ID</code> - <code>OS_APPLICATION_CREDENTIAL_SECRET</code></p> <p>and potentially these too: - <code>OS_IDENTITY_API_VERSION</code>  - <code>OS_REGION_NAME</code> - <code>OS_INTERFACE</code></p> <p>The values for these variables are available in OpenStack RC file provided when creating the application credentials.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#providing-dns-provider-credentials-to-terraform-cloud","title":"Providing DNS provider credentials to Terraform Cloud","text":"<p>Terraform Cloud will invoke Terraform command-line in a remote virtual environment. For the CLI to be able to communicate with your DNS provider API, we need to define environment variables that Terraform will use to authenticate. The next sections explain which environment variables to define for each DNS provider and how to retrieve the values of the variable from the provider.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#cloudflare","title":"CloudFlare","text":"<p>You need to define these environment variables: - <code>CLOUDFLARE_EMAIL</code> - <code>CLOUDFLARE_API_KEY</code> (sensitive)</p> <p>If you prefer using an API token instead of the global API key, you need to define these environment variables instead: - <code>CLOUDFLARE_API_TOKEN</code> (sensitive) - <code>CLOUDFLARE_ZONE_API_TOKEN</code> (sensitive) - <code>CLOUDFLARE_ZONE_DNS_TOKEN</code> (sensitive)</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#google-cloud-dns","title":"Google Cloud DNS","text":"<p>Refer to Google Cloud section under Providing cloud provider credentials to Terraform Cloud. Make sure the Google Cloud service account can modify your DNS zone.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#managing-ubiquity-variables-with-terraform-cloud-ui","title":"Managing Ubiquity variables with Terraform Cloud UI","text":"<p>It is possible to use Terraform Cloud web interface to define variable values in your <code>main.tf</code>. For example, you could want to define a guest password without writing it directly in <code>main.tf</code> to avoid displaying publicly.</p> <p>To manage a variable with Terraform Cloud: 1. edit your <code>main.tf</code> to define the variables you want to manage. In the following example, we want to manage the number of nodes and the guest password.</p> <pre><code>Add the variables at the beginning of the `main.tf`:\n  ```hcl\n  variable \"nb_nodes\" {}\n  variable \"password\" {}\n  ```\n\nThen replace the static value by the variable in our `main.tf`,\n\ncompute node count\n  ```hcl\n  node = { type = \"p2-3gb\", tags = [\"node\"], count = var.nb_nodes }\n  ```\nguest password\n  ```hcl\n  guest_passwd = var.password\n  ```\n</code></pre> <ol> <li>Commit and push this changes to your git repository.</li> <li>In Terraform Cloud workspace associated with that repository, go in \"Variables.</li> <li>Under \"Terraform Variables\", click the \"Add variable\" button and create a variable for each one defined previously in the <code>main.tf</code>. Check \"Sensitive\" if the variable content should not never be shown in the UI or the API.</li> </ol> <p>You may edit the variables at any point of your cluster lifetime.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#applying-changes","title":"Applying changes","text":"<p>To create your cluster, apply changes made to your <code>main.tf</code> or the variables, you will need to queue a plan. When you push to the default branch of the linked git repository, a plan will be automatically created. You can also create a plan manually. To do so, click on the \"Queue plan manually\" button inside your workspace, then \"Queue plan\".</p> <p>Once the plan has been successfully created, you can apply it using the \"Runs\" section. Click on the latest queued plan, then on the \"Apply plan\" button at the bottom of the plan page.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#auto-apply","title":"Auto apply","text":"<p>It is possible to apply automatically a successful plan. Go in the \"Settings\" section, and under \"Apply method\" select \"Auto apply\". Any following successful plan will then be automatically applied.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#ubiquity-terraform-cloud-and-the-cli","title":"Ubiquity, Terraform Cloud and the CLI","text":"<p>Terraform cloud only allows to apply or destroy the plan as stated in the main.tf, but sometimes it can be useful to run some other terraform commands that are only available through the command-line interface, for example <code>terraform taint</code>.</p> <p>It is possible to import the terraform state of a cluster on your local computer and then use the CLI on it.</p> <ol> <li> <p>Log in Terraform cloud: <pre><code>terraform login\n</code></pre></p> </li> <li> <p>Create a folder where the terraform state will be stored: <pre><code>mkdir my-cluster-1\n</code></pre></p> </li> <li> <p>Create a file named <code>cloud.tf</code> with the following content in your cluster folder: <pre><code>terraform {\n  cloud {\n    organization = \"REPLACE-BY-YOUR-TF-CLOUD-ORG\"\n    workspaces {\n      name = \"REPLACE-BY-THE-NAME-OF-YOUR-WORKSPACE\"\n    }\n  }\n}\n</code></pre> replace the values of <code>organization</code> and <code>name</code> with the appropriate value for your cluster.</p> </li> <li> <p>Initialize the folder and retrieve the state: <pre><code>terraform init\n</code></pre></p> </li> </ol> <p>To confirm the workspace has been properly imported locally, you can list the resources using: <pre><code>terraform state list\n</code></pre></p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#enable-ubiquity-autoscaling","title":"Enable Ubiquity Autoscaling","text":"<p>Ubiquity in combination with Terraform Cloud (TFE) can be configured to give Slurm the ability to create and destroy instances based on the job queue content.</p> <p>To enable this feature: 1. Create a TFE API Token and save it somewhere safe.</p> <pre><code>1.1. If you subscribe to Terraform Cloud Team &amp; Governance plan, you can generate\na [Team API Token](https://www.terraform.io/cloud-docs/users-teams-organizations/api-tokens#team-api-tokens).\nThe team associated with this token requires no access to organization and can be secret.\nIt does not have to include any member. Team API token is preferable as its permissions can be\nrestricted to the minimum required for autoscale purpose.\n</code></pre> <ol> <li> <p>Create a workspace in TFE</p> <p>2.1. Make sure the repo is private as it will contain the API token.</p> <p>2.2. If you generated a Team API Token in 1, provide access to the workspace to the team:</p> <ol> <li>Workspace Settings -&gt; Team Access -&gt; Add team and permissions</li> <li>Select the team</li> <li>Click on \"Customize permissions for this team\"</li> <li>Under \"Runs\" select \"Apply\"</li> <li>Under \"Variables\" select \"Read and write\"</li> <li>Leave the rest as is and click on \"Assign custom permissions\"</li> </ol> <p>2.3 In Configure settings, under Advanced options, for Apply method, select Auto apply.</p> </li> <li> <p>Create the environment variables of the cloud provider credentials in TFE</p> </li> <li>Create a variable named <code>pool</code> in TFE. Set value to <code>[]</code> and check HCL.</li> <li>Add a file named <code>data.yaml</code> in your git repo with the following content:     <pre><code>---\nprofile::slurm::controller::tfe_token: &lt;TFE API token&gt;\nprofile::slurm::controller::tfe_workspace: &lt;TFE workspace id&gt;\n</code></pre>     Complete the file by replacing <code>&lt;TFE API TOKEN&gt;</code> with the token generated at step 1     and <code>&lt;TFE workspace id&gt;</code> (i.e.: <code>ws-...</code>) by the id of the workspace created at step 2.     It is recommended to encrypt the TFE API token before committing <code>data.yaml</code> in git. Refer     to section 4.13.1 of README.md to     know how to encrypt the token.</li> <li>Add <code>data.yaml</code> in git and push.</li> <li> <p>Modify <code>main.tf</code>:</p> <ol> <li>If not already present, add the following definition of the pool variable at the beginning of your <code>main.tf</code>.</li> </ol> <pre><code>variable \"pool\" { description = \"Slurm pool of compute nodes\" }\n</code></pre> <ol> <li>Add instances to <code>instances</code> with the tags <code>pool</code> and <code>node</code>. These are   the nodes that Slurm will able to create and destroy.</li> <li>If not already present, add the following line after the instances definition to pass the list of compute nodes from Terraform cloud workspace variable to the provider module:</li> </ol> <pre><code>pool = var.pool\n</code></pre> <ol> <li>On the right-hand-side of <code>public_keys =</code>, replace <code>[file(\"~/.ssh/id_rsa.pub\")]</code>   by a list of SSH public keys that will have admin access to the cluster.</li> <li>After the line <code>public_keys = ...</code>, add <code>hieradata = file(\"data.yaml\")</code>.</li> <li>After the line <code>hieradata = ...</code>, add <code>generate_ssh_key = true</code>. This will provide   Terraform Cloud SSH admin access to the cluster and it will be used to upload configuration   files.</li> <li>Stage changes, commit and push to git repo.</li> </ol> </li> <li> <p>Go to your workspace in TFE, click on Actions -&gt; Start a new run -&gt; Plan and apply -&gt; Start run. Then, click on \"Confirm &amp; Apply\" and \"Confirm Plan\".</p> </li> <li>Compute nodes defined in step 8 can be modified at any point in the cluster lifetime and more pool compute nodes can be added or removed if needed.</li> </ol> <p>To reduce the time required for compute nodes to become available in Slurm, consider creating a compute node image.</p>"},{"location":"admin-guide/deployment/cloud/terraform_cloud/#troubleshoot-autoscaling-with-terraform-cloud","title":"Troubleshoot autoscaling with Terraform Cloud","text":"<p>If after enabling autoscaling with Terraform Cloud for your Ubiquity cluster, the number of nodes does not increase when submitting jobs, verify the following points:</p> <ol> <li>Go to the Terraform Cloud workspace webpage, and look for errors in the runs. If the runs were only triggered by changes to the git repo, it means scaling signals from the cluster do not reach the Terraform cloud workspace or no signals were sent at all.</li> <li>Make sure the Terraform Cloud workspace id matches with the value of <code>profile::slurm::controller::tfe_workspace</code> in <code>data.yaml</code>.</li> <li>Execute <code>squeue</code> on the cluster, and verify the reasons why jobs are still in the queue. If under the column <code>(Reason)</code>, there is the keyword <code>ReqNodeNotAvail</code>, it implies Slurm tried to boot the listed nodes, but they would not show up before the timeout, therefore Slurm marked them as down. It can happen if your cloud provider is slow to build the instances, or following a configuration problem like in 2. When Slurm marks a node as down, a trace is left in slurmctld's log - using zgrep on the slurm controller node (typically <code>mgmt1</code>):     <pre><code>sudo zgrep \"marking down\" /var/log/slurm/slurmctld.log*\n</code></pre>     To tell Slurm these nodes are available again, enter the following command:     <pre><code>sudo /opt/software/slurm/bin/scontrol update nodename=node[Y-Z] state=IDLE\n</code></pre>     Replace <code>node[Y-Z]</code> by the hostname range listed next to <code>ReqNodeNotAvail</code> in <code>squeue</code>.</li> <li>Under <code>mgmt1:/var/log/slurm</code>, look for errors in the file <code>slurm_resume.log</code>.</li> </ol>"},{"location":"admin-guide/deployment/production/airgapping/","title":"Air gapped Network Deployment","text":"<p>Ubiquity can be deployed in an \"air gapped\" network environment, provided the follow requirements are met: 1. A local container registry is deployed, or a registry service is available from the infrastructure provider with access from the secure network. 2. At least one \"bastion\" node (or instance) is available to transfer Ubiquity software from the Internet over to the secure network; the best practice is for this system to be able to access the same container registry that is available to the secure network, and ideally, also the Kubernetes API itself.</p>"},{"location":"admin-guide/deployment/production/airgapping/#supported-air-gapped-configurations","title":"Supported Air Gapped Configurations","text":"<p>There is 1 supported configuration, although Ubiquity can be made to work with many more restrictions beyond this.  In both of the supported cases, the following is true: 1. \"Node\" can be either a physical compute node, a virtual machine, or a cloud provider instance 2. \"Bastion Node\" refers to a Linux node that has Docker installed as well as a Git client 3. \"Install Node\" refers to a Linux node that has a Kubernetes and Helm client installed, with admin-level client certificates for the Kubernetes API; note that for the preferred configuration, this is combined with the \"Bastion Node\" 4. \"Registry\" is any OCI-compliant registry - note that if using a cloud infrastructure provider, it is best to use that platform's secure registry service, if any, in order to avoid having to configure \"insecure registries\" in each node on the secure network side. 5. NGINX Ingress (or LoadBalancer) into Ubiquity and Ubiquity jobs must still be configured and accessible by whatever clients expect to access it, whether in the secure network or not.  This configuration is not covered in this document and is implementation specific.</p>"},{"location":"admin-guide/deployment/production/airgapping/#default-configuration-preferred","title":"Default Configuration (Preferred)","text":"<p>This topology is most resource efficient and simplifies the deployment as much as possible.</p> <p></p> <p>Note that in this topology only the \"Bastion Node\" + \"Install Node\" environment must be able to access the container registry as well as the Kubernetes API; it does not need to be able to access the Kubernetes workers directly.  This node will be used to pull, tag, and push containers from upstream (Internet) to the secure container registry, and deploy+update Ubiquity using the Kubernetes and Helm clients.</p> <p>In addition to mirroring containers to the secure registry, the Bastion node in this configuration must be able to initially mirror the Ubiquity repository in GitHub over to itself and then commit back to the local repository. This local repository could clearly be the bastion node in this diagram.</p> <p>Note the bastion node could quite easily separate its functions into two separate nodes, one for mirroring containers and one for deploying/updating Ubiquity, but this is not recommended as it will require additional configuration to transfer the container images from the mirroring node to the deployment node.</p>"},{"location":"admin-guide/deployment/production/airgapping/#mirroring-containers","title":"Mirroring containers","text":""},{"location":"admin-guide/deployment/production/airgapping/#system-containers","title":"System Containers","text":"<p>The script scripts/Ubiquity-pull-system-images can be used to pull, tag, and optionally push Ubiquity system containers from the upstream registry to the secure registry, and should be run on the Bastion Node for both initial installs and updates.  Run without arguments for usage.  If specifying a push registry and push repository, make sure to populate these values into <code>Ubiquity.Ubiquity_SYSTEM_REGISTRY</code> and <code>Ubiquity.Ubiquity_SYSTEM_REPO_BASE</code> respectively as helm chart or overrides options for deployment.  The value of <code>--Ubiquity-version</code>, to the left of the branch (e.g. starting with the first numeric digit), should be specified as <code>Ubiquity.IMAGES_VERSION</code>; the value to the left (e.g. <code>Ubiquity-master</code>) should be specified as <code>Ubiquity.Ubiquity_IMAGES_TAG</code>.</p> <p>If not using this script to push (e.g. if using an alternate method to transfer container images from one registry to another), be sure to match the repository base and the version tag with what is configured in the Helm chart or overrides options, as explained above.</p>"},{"location":"admin-guide/deployment/production/airgapping/#additional-containers","title":"Additional Containers","text":"<p>To determine additional containers to mirror, run the following command in the <code>Ubiquity-helm</code> repository:</p> <p><code>grep \"image:\" values.yaml|awk {'print $2'} &amp;&amp; grep  '_IMAGE:' values.yaml|awk '{print $3}'</code></p> <p>The example output will show what containers need to be mirrored, tagged, and pushed to the secure registry, one on each line:</p> <pre><code>$ grep \"image:\" values.yaml|awk {'print $2'} &amp;&amp; grep  '_IMAGE:' values.yaml|awk '{print $3}'\nubiquitycluster/ubiquity-cache-pull:latest\nubiquitycluster/lxcfs:3.0.3-3\nnvidia/k8s-device-plugin:1.11\nxilinxatg/xilinx_k8s_fpga_plugin:latest\nUbiquity/k8s-rdma-device:1.0.1\nmysql:5.6.41\nubiquitycluster/postfix:3.11_3.4.9-r0\nubiquitycluster/idmapper\nmemcached:1.5\nregistry:2\nubiquitycluster/unfs3\nalpine:3.8\n</code></pre> <p>Once mirrored, edit the entries in the <code>override.yaml</code> file to the new tags, by searching for the matching tags from the list above.</p>"},{"location":"admin-guide/deployment/production/airgapping/#application-containers","title":"Application Containers","text":"<p>Appsync cannot be used in an air gapped configuration, so application containers must be mirrored individually.  A matching application target must be created manually in the Ubiquity system (can be performed as the <code>root</code> user), and then those applications can be marked public in the Administration-&gt;Apps view of the portal.  Note that the desired list of application containers must be obtained from the Ubiquity community.  Each application target should also be explicitly pulled after being created to download the in-container metadata (e.g. AppDef).</p>"},{"location":"admin-guide/deployment/production/airgapping/#additional-configuration","title":"Additional Configuration","text":"<ol> <li>The Bastion node user must log in with the upstream service account for the Ubiquity system and application containers.</li> <li>The <code>Ubiquity.imagePullSecret</code> value must be set to the secure registry's service account (or username/password) since the containers will be pulled from there to be executed.  Note that this is different than the upstream service account, which must be used on the Bastion node.</li> </ol>"},{"location":"admin-guide/deployment/production/airgapping/#updates","title":"Updates","text":"<p>Container mirroring should be repeated when the system is updated, including the Helm chart or overrides configuration.  The updated version of Ubiquity will be deployed automatically by running a Helm upgrade after updating the configuration parameters.  Note that as a best practice, all containers should be mirrored, or at least checked, in case they changed between versions following the Git update of the <code>Ubiquity</code> repository.</p>"},{"location":"admin-guide/deployment/production/airgapping/#limitations","title":"Limitations","text":"<p>The following limitations in air gapped environments currently exist: 1. Ubiquity is a multi-architecture platform but the default mirroring scheme will only pull containers for the current architecture.  In most cases, the infrastructure architecture (e.g. <code>amd64</code>) will match between the Bastion node and the nodes on the secure network side; if they don't, additional work will be required to create manifests so that multiple architecture containers are available (and pulled correctly) - this is currently not explicitly supported, but can be on request. 2. Appsync cannot be used and should be disabled in the <code>override.yaml</code> file before deploying; see above for information on mirroring application containers and creating the appropriate application targets in the system. 3. If using a self-hosted container registry on the secure network, serving over HTTP rather than HTTPS, the Docker daemons on all nodes (including Bastion node) must be configured to support this as an \"insecure registry\"; it is highly recommended that either you use an infrastructure-provider secure registry, or apply CA-signed certificates (and associated DNS configurations) to the self-hosted registry to avoid this problem.</p>"},{"location":"admin-guide/deployment/production/awx-postinstall/","title":"Post-installation: AWX","text":"<p>AWX provides a great way to integrate with nodes, and orchestrate the configuration management of those nodes. However, to do this needs a few steps, listed below.</p>"},{"location":"admin-guide/deployment/production/awx-postinstall/#get-awx-password","title":"Get AWX Password","text":"<p>You can retrieve this using the opus container, using the ./scripts/awx-admin-password script:</p> <pre><code>[root@bootstrap ubiquity]# ./scripts/awx-admin-password\nWARNING: AWX admin can do most baremetal node config in the cluster, only use it for just enough initial setup or in emergencies.\n5Q95BpvbMwCVRegH7FUGr7iCSy4sdes0gl[\n</code></pre>"},{"location":"admin-guide/deployment/production/awx-postinstall/#logon-to-awx-interface","title":"Logon to AWX interface","text":"<p>This is provisioned on your cluster using the <code>https://awx.&lt;my-cluster-domain&gt;</code> address. If you use nip.io, it will be something like <code>https://awx.10-0-0-200.nip.io</code> for example.</p> <p> <p>Once you log on to this page, you will be presented with the dashboard of the current cluster:</p> <p> <p>This looks a little sparse, with only 1 node, and a generic test project. We should probably fix that...</p>"},{"location":"admin-guide/deployment/production/awx-postinstall/#setup-oauth-integration-optional","title":"Setup OAuth integration - Optional","text":"<p>If you wish to allow a group of users to logon and you have this attached to KeyCloak, then you can attach it going through the Generic OIDC settings.</p>"},{"location":"admin-guide/deployment/production/awx-postinstall/#setup-awx-project","title":"Setup AWX project","text":""},{"location":"admin-guide/deployment/production/awx-postinstall/#configure-awx-token","title":"Configure AWX Token","text":"<p>AWX has the concept of tokens for authentication, so hosts can check-in, execute tasks, etc.</p> <ul> <li>Go to Resources &gt; Credentials on the left-hand side of the navigation headings. In there you will see a Demo credential and an ansible galaxy credential.</li> </ul> <p> <ul> <li>Click add</li> </ul> <p> <ul> <li>Once there you are going to give it a name of <code>Ubiquity</code> - A description of <code>Private key - ed25519</code>, An organization of <code>Default</code>, and a Credential Type of <code>Machine</code>.</li> <li>Then, a section will appear below:</li> </ul> <p> <ul> <li>Simply take your key that opus created (inside your .ssh/ed25519 folder - You did back them up, right?) and paste it into the textbox below.</li> </ul> <p>&lt; show pasted key in a window &gt;</p> <ul> <li> <p>You can define if you requested a passphrase to specify that on the system.</p> </li> <li> <p>Then save - You've now got a machine credential to log onto all of your compute nodes. Don't worry, this key is encrypted:</p> </li> </ul> <p>&lt; show encrypted credential view &gt;</p>"},{"location":"admin-guide/deployment/production/awx-postinstall/#setup-an-inventory","title":"Setup an Inventory","text":"<p>Next up is to create our default Ubiquity inventory. This is where all our hosts live.</p> <ul> <li>Click the Resources &gt; Inventories tab</li> </ul> <p>&lt; screenshot of resources-inventories &gt;</p> <ul> <li>Then we'll click add, and create our Ubiquity inventory. The name will be Ubiquity, Organization is Default, and the only other thing as a placeholder is we'll define the ansible_python_interpreter in the variables section. Hit save.</li> </ul> <p>&lt; screenshot of adding new inventory &gt;</p> <ul> <li>Note now when you hit save, You'll be presented with more options. We'll need to go into a few of these.</li> </ul> <p>&lt; screenshot of initial newly created inventory &gt;</p> <ul> <li>Let's create some groups. Click the Groups subheading. Note that there aren't any groups yet... We need to fix that.</li> </ul> <p>&lt; show inventories no groups yet pic &gt;</p> <ul> <li>Add a new group. Call it compute. At the bottom of this, just like the overall inventory - There is now a section for you to put variables. These are now specific variables for this group. Place any specific variables for your playbooks (an example is inside the ubiq-playbooks repo for defaults) here.</li> </ul> <p>&lt; screenshot of variables &gt;</p> <ul> <li> <p>Save.</p> </li> <li> <p>Repeat this process for: cesgpfs, We don't explicitly right now get nodes to auto-execute AWX playbooks, but we do enable the automatic check-in of hosts to an inventory.</p> </li> </ul>"},{"location":"admin-guide/deployment/production/awx-postinstall/#next-steps","title":"Next-Steps","text":""},{"location":"admin-guide/deployment/production/configuration/","title":"Configuration","text":"<p>Open the tools container, which includes all the tools needed:</p> DockerNix <pre><code>make tools\n</code></pre> <pre><code>nix-shell\n</code></pre> <p>Note</p> <p>It will take a while to build the tools container on the first time</p> <p>Run the following script to configure the environment:</p> <pre><code>make configure\n</code></pre> <p>Example</p> <p> <pre><code>Text editor (nvim):\nEnter seed repo (github.com/cjcshadowsan/ubiquity): github.com/my-cluster/ubiquity\nEnter your domain (ubiquitycluster.uk): example.com\n</code></pre></p> <p>It will prompt you to edit the inventory:</p> <ul> <li>IP address: the desired one, not the current one, since your servers have no operating system installed yet</li> <li>Disk: based on <code>/dev/$DISK</code>, in my case it's <code>sda</code>, but yours can be <code>sdb</code>, <code>nvme0n1</code>...</li> <li>Network interface: usually it's <code>eth0</code>, mine is <code>eno1</code>, could be <code>en&lt;s for slot, number&gt;&lt;f for function number starting from 0&gt; - so ens4f0</code></li> <li>External address: an address. Can be the same as the internal IP address</li> <li>External interface (optional): usually it's <code>eth0</code>, mine is <code>eno1</code>, could be <code>en&lt;s for slot, number&gt;&lt;f for function number starting from 0&gt; - so ens4f0</code></li> <li>Wake on Lan: true or false, or otherwise whether to use IPMI or not</li> <li>MAC address: the lowercase, colon separated MAC address of the above network interface</li> </ul> <p>Example</p> metal/inventories/prod.yml<pre><code>---\n# Copyright The Ubiquity Authors.\n#\n# Licensed under the Apache License, Version 2.0. Previously licensed under the Functional Source License (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://github.com/ubiquitycluster/ubiquity/blob/main/LICENSE\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# This software was previously licensed under the Functional Source License but has now transitioned to an Apache 2.0 License\n# as of June 2025.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# The metal group contains the physical nodes in the cluster\nmetal:\n  children:\n    masters:\n      hosts:\n        cp1: {ansible_host: 10.1.0.1, mac: 'de:ad:be:ef:be:ef', disk: sda, ipmi_addr: 10.0.0.1, ipmi_user: USERID, ipmi_pass: 'PASSW0RD', wol: false }\n        cp2: {ansible_host: 10.1.0.2, mac: 'de:ad:be:ef:be:ef', disk: sda, ipmi_addr: 10.0.0.2, ipmi_user: USERID, ipmi_pass: 'PASSW0RD', wol: false }\n        cp3: {ansible_host: 10.1.0.3, mac: 'de:ad:be:ef:be:ef', disk: sda, ipmi_addr: 10.0.0.3, ipmi_user: USERID, ipmi_pass: 'PASSW0RD', wol: false }\n    workers:\n      hosts:\n#        worker1: {ansible_host: 192.168.1.121, mac: 'de:ad:be:ef:83:b9', disk: sda, network_interface: eno1, ipmi_addr: 192.168.0.121, ipmi_user: root, ipmi_pass: calvin2, wol: false }\n#        worker2: {ansible_host: 192.168.1.122, mac: '90:b1:1c:58:83:b9', disk: sda, network_interface: eno1, ipmi_addr: 192.168.0.122, ipmi_user: root, ipmi_pass: calvin2, wol: false }\n#        worker3: {ansible_host: 192.168.1.123, mac: '90:b1:1c:58:83:b9', disk: sda, network_interface: eno1, ipmi_addr: 192.168.0.123, ipmi_user: root, ipmi_pass: calvin2, wol: false }\n</code></pre> <p>At the end it will show what has changed. After examining the diff, commit and push the changes.</p>"},{"location":"admin-guide/deployment/production/deployment/","title":"Deployment","text":"<p>Open the tools container if you haven't already:</p> DockerNix <pre><code>make tools\n</code></pre> <pre><code>nix-shell\n</code></pre> <p>Build the cluster:</p> <pre><code>make\n</code></pre> <p>Yes it's that simple!</p> <p>Example</p> <p></p> <p>It will take a while to download everything, you can read the architecture document while waiting for the deployment to complete.</p>"},{"location":"admin-guide/deployment/production/external-resources/","title":"External resources","text":"<p>Info</p> <p>These resources are optional, the environment still works without them but will lack some features like trusted certificates and offsite backup</p> <p>Although I try to keep the amount of external resources to the minimum, there's still need for a few of them. Below is a list of external resources and why we need them (also see some alternatives below).</p> Provider Resource Purpose Terraform Cloud Workspace Terraform state backend Cloudflare DNS DNS and DNS-01 challenge for certificates Cloudflare Tunnel Public services to the internet without port-forwarding"},{"location":"admin-guide/deployment/production/external-resources/#create-credentials","title":"Create credentials","text":"<p>You'll be asked to provide these credentials on first build.</p>"},{"location":"admin-guide/deployment/production/external-resources/#create-terraform-workspace","title":"Create Terraform workspace","text":"<p>Terraform is stateful, which means it needs somewhere to store its state. Terraform Cloud is one option for a state backend with a generous free tier, perfect for a control plane.</p> <ol> <li>Sign up for a Terraform Cloud account</li> <li>Create a workspace named <code>ubiquity-external</code>, this is the workspace where your cluster state will be stored.</li> <li>Change the \"Execution Mode\" from \"Remote\" to \"Local\". This will ensure your local machine, which can access your cluster, is the one executing the terraform plan rather than the cloud runners.</li> </ol> <p>If you decide to use a different Terraform backend, you'll need to edit the <code>external/versions.tf</code> file as required.</p>"},{"location":"admin-guide/deployment/production/external-resources/#cloudflare","title":"Cloudflare","text":"<ul> <li>Buy a domain and transfer it to Cloudflare if you haven't already</li> <li>Get Cloudflare email and account ID</li> <li>Global API key: https://dash.cloudflare.com/profile/api-tokens</li> </ul>"},{"location":"admin-guide/deployment/production/external-resources/#alternatives","title":"Alternatives","text":"<ul> <li>Terraform Cloud: any other Terraform backends</li> <li>Cloudflare DNS: see manual DNS setup</li> <li>Cloudflare Tunnel: you can create a small VPS in the cloud and utilize Wireguard and HAProxy to route traffic via it, or just use simple port-forwarding if it's available (see also awesome tunneling)</li> </ul>"},{"location":"admin-guide/deployment/production/hfi-infiniband/","title":"CNI Support for MPI over InfiniBand","text":"<p>The following document is a HOWTO for how to enable Ubiquity support for using MPI over InfiniBand. Note that the referenced components are managed by their respective third party maintainers.</p>"},{"location":"admin-guide/deployment/production/hfi-infiniband/#howto-enable-ipoib-in-ubiquity","title":"Howto enable IPoIB in Ubiquity","text":"<p>Assumptions: - working Ubiquity K3s cluster with a functional primary CNI - required drivers already installed for InfiniBand host adapter - Installed via Ubiquity, using NVIDIA Netowork Operator - an existing IPoIB interface is up and active on the hosts - Installed via Ubiquity. using Ansible - this has only been tested with NVIDIA InfiniBand cards and Mellanox OFED - Other network card providers instructions may (will) differ.</p>"},{"location":"admin-guide/deployment/production/hfi-infiniband/#required-3rd-party-tools","title":"Required 3rd party tools","text":"<ul> <li>Multus project will be utilized to add the secondary IpoIB interface</li> <li>ipoib-cni from NVIDIA/Mellanox</li> <li> <p>Whereabouts for ipam</p> </li> <li> <p>Install Multus by applying multus-daemonset.yaml.  <pre><code>kubectl apply -f multus-daemonset.yml\n</code></pre></p> </li> <li>Deploy the ipoib-cni. <pre><code>kubectl apply -f ipoib-cni-daemonset.yaml\n</code></pre></li> <li>Install the whereabouts ipam. <pre><code>git clone https://github.com/k8snetworkplumbingwg/whereabouts &amp;&amp; cd whereabouts\nkubectl apply \\\n    -f doc/crds/daemonset-install.yaml \\\n    -f doc/crds/whereabouts.cni.cncf.io_ippools.yaml \\\n    -f doc/crds/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml\n</code></pre></li> <li>Define the network for IPoIB with a NetworkAttachmentDefinition. Please modify ip range and master interface as required.</li> </ul> <pre><code>apiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: ubiquity-ipoib\nspec:\n  config: '{\n  \"cniVersion\": \"0.3.1\",\n  \"type\": \"ipoib\",\n  \"name\": \"ubiquity-ipoib\",\n  \"master\": \"ib0\",\n  \"ipam\": {\n    \"type\": \"whereabouts\",\n    \"range\": \"192.168.0.0/24\"\n  }\n}'\n</code></pre> <ol> <li>Validate that containers can launch and attach this secondary network by running the following pod:</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ipoib-test\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ubiquity-ipoib\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - image: \n    name: ipoib-test\n    securityContext:\n      capabilities:\n        add: [ \"IPC_LOCK\" ]\n    resources:\n      limits:\n        .com/rdma: 1\n    command:\n    - sh\n    - -c\n    - |\n      ls -l /dev/infiniband /sys/class/infiniband /sys/class/net\n      sleep 1000000\n</code></pre>"},{"location":"admin-guide/deployment/production/hfi-infiniband/#rootless-applications-and-memlock-rlimit-when-infiniband","title":"Rootless applications and Memlock rlimit when Infiniband","text":"<p>When using rootless applications, which is default with appdef V2, init executions do not posses  sufficient privileges to set Memlock rlimit to unlimited, leading to failure of Infiniband usage.</p> <p>Two possibilities are available to bypass this issue:</p> <ol> <li>Cluster administrator need to force unlimited Memlock rlimites at docker / containerd level.</li> <li>For docker, append <code>--default-ulimit memlock=-1:-1</code> inside docker service file, at <code>ExecStart=/usr/bin/dockerd</code> line, then reload systemctl and restart docker service.</li> <li>For containerd, edit service file, and add under <code>[SERVICE]</code> line <code>LimitMEMLOCK=infinity</code>, then reload systemctl and restart containerd service.</li> <li>Or allow apps to run init as root, before dropping to unprivileged user. To do so, set <code>JARVICE_APP_ALLOW_ROOT_INIT</code> value to <code>true</code> in <code>override.yaml</code>. This will unlock root usage for appdef V2 apps, which will set Memlock rlimit to unlimited before MPI execution.</li> </ol>"},{"location":"admin-guide/deployment/production/post-installation/","title":"Post-installation","text":""},{"location":"admin-guide/deployment/production/post-installation/#backup-secrets","title":"Backup secrets","text":"<p>Save the following files to a safe location (like a password manager):</p> <ul> <li><code>~/.ssh/id_ed25519</code></li> <li><code>~/.ssh/id_ed25519.pub</code></li> <li><code>./metal/kubeconfig.yaml</code></li> <li><code>~/.terraform.d/credentials.tfrc.json</code></li> <li><code>./external/terraform.tfvars</code></li> </ul>"},{"location":"admin-guide/deployment/production/post-installation/#admin-credentials","title":"Admin credentials","text":"<ul> <li>ArgoCD:<ul> <li>Username: <code>admin</code></li> <li>Password: run <code>./scripts/argocd-admin-password</code></li> </ul> </li> <li>Vault:<ul> <li>Root token: run <code>./scripts/vault-root-token</code></li> </ul> </li> <li>Grafana:<ul> <li>Username: <code>admin</code></li> <li>Password: <code>prom-operator</code> (TODO: use random password)</li> </ul> </li> <li>Gitea:<ul> <li>Username: <code>gitea_admin</code></li> <li>Password: get from Vault</li> </ul> </li> </ul>"},{"location":"admin-guide/deployment/production/post-installation/#next-steps","title":"Next steps","text":"<ul> <li>Setup of AWX</li> <li>User onboarding</li> </ul>"},{"location":"admin-guide/deployment/production/prerequisites/","title":"Prerequisites","text":""},{"location":"admin-guide/deployment/production/prerequisites/#fork-this-repository","title":"Fork this repository","text":"<p>Because this project applies GitOps practices, it's the source of truth for our development environment, so you'll need to fork it to make it yours:</p> <p> Fork cjcshadowsan/ubiquity</p> <p>By using this project you agree to the license.</p> <p>License TL;DR</p> <ul> <li>This project is free to use for any purpose, but it comes with no warranty</li> <li>If used for commercial purposes, you must gain an agreement from Q Associates</li> <li>You must use the same Apache v2.0 license  in <code>LICENSE</code></li> <li>You must keep the copy right notice and/or include an acknowledgement</li> <li>Your project must remain open-source and be published publicly</li> </ul>"},{"location":"admin-guide/deployment/production/prerequisites/#hardware-requirements","title":"Hardware requirements","text":""},{"location":"admin-guide/deployment/production/prerequisites/#initial-controller","title":"Initial controller","text":"<p>Info</p> <p>The initial controller is the machine used to bootstrap the cluster, we only need it once, you can use your laptop or desktop</p> <ul> <li>A Linux machine that can run Docker (because the <code>host</code> networking driver used for PXE boot only supports Linux, you can use a Linux virtual machine with bridged networking if you're on macOS or Windows).</li> </ul>"},{"location":"admin-guide/deployment/production/prerequisites/#servers","title":"Servers","text":"<p>Any modern <code>x86_64</code> computer(s) should work, you can use old PCs, laptops or servers.</p> <p>Info</p> <p>This is the requirements for each node</p> Component Minimum Recommended CPU 2 cores 4 cores RAM 8 GB 16 GB Hard drive 128 GB 512 GB (depending on your storage usage, the base installation will not use more than 128GB) Node count 1 (checkout the single node cluster adjustments tutorial) 3 or more for high availability <p>Additional capabilities:</p> <ul> <li>Ability to boot from the network (PXE boot)</li> <li>Wake-on-LAN capability, used to wake the machines up automatically without physically touching the power button</li> <li>Failing WOL, IPMI works</li> </ul>"},{"location":"admin-guide/deployment/production/prerequisites/#network-setup","title":"Network setup","text":"<ul> <li>All servers must be connected to the same wired network with the initial controller</li> <li>You have the access to change DNS config (on your router or at your domain registrar)</li> </ul>"},{"location":"admin-guide/deployment/production/prerequisites/#domain","title":"Domain","text":"<p>Buying a domain is highly recommended, but if you don't have one, see manual DNS setup.</p>"},{"location":"admin-guide/deployment/production/prerequisites/#bios-setup","title":"BIOS setup","text":"<p>Info</p> <p>You need to do it once per machine if the default config is not sufficent, usually for consumer hardware this can not be automated (it requires something like IPMI to automate).</p> <p>Common settings:</p> <ul> <li>Enable Wake-on-LAN (WoL) and network boot OR:</li> <li>Enable IPMI and network boot</li> <li>Use UEFI mode and disable CSM (legacy) mode - This is UEFI-only. The standard has been around for 10 years - Ditch Legacy boot!</li> <li>Disable secure boot</li> </ul> <p>Boot order options (select one, each has their pros and cons):</p> <ol> <li>Only boot from the network if no operating system found: works on most hardware but you need to manually wipe your hard drive or delete the existing boot record for the current OS</li> <li>Prefer booting from the network if turned on via WoL: more convenience but your BIOS must support it, and you must test it throughly to ensure you don't accidentally wipe your servers</li> </ol> <p>Example</p> <p>Below is my BIOS setup for reference. Your motherboard may have a different name for the options, so you'll need to adapt it to your hardware.</p> <pre><code>Devices:\n  NetworkSetup:\n    PXEIPv4: true\n    PXEIPv6: false\nAdvanced:\n  CPUSetup:\n    VT-d: true\nPower:\n  AutomaticPowerOn:\n    WoL: Automatic  # Use network boot if Wake-on-LAN\nSecurity:\n  SecureBoot: false\nStartup:\n  CSM: false\n</code></pre>"},{"location":"admin-guide/deployment/production/prerequisites/#gather-information","title":"Gather information","text":"<ul> <li> MAC address for each machine</li> <li> OS disk name (for example <code>/dev/sda</code>)</li> <li> Network interface name (for example <code>eth0</code>)</li> <li> Choose a static IP address for each machine (just the desired address, we don't set anything up yet)</li> <li> Choose an external IP address for each machine</li> <li> Choose the authentication mechanism (LDAP, etc)</li> </ul>"},{"location":"admin-guide/runbooks/argocd/","title":"ArgoCD","text":""},{"location":"admin-guide/runbooks/argocd/#argocd_1","title":"ArgoCD","text":""},{"location":"admin-guide/runbooks/argocd/#scope","title":"Scope","text":"<p>The <code>argocd</code> role installs ArgoCD, a GitOps tool, in the cluster.</p>"},{"location":"admin-guide/runbooks/argocd/#accessing-argocd","title":"Accessing ArgoCD","text":"<p>The ArgoCD <code>\"admin\"</code> password is available from a secret:</p> <pre><code>$ ARGOCD_INITIAL_ADMIN_PASSWORD=\"$(kubectl get secrets -n argocd argocd-initial-admin-secret -ojson | jq .data.password -r | base64 -d)\"\n$ echo $ARGOCD_INITIAL_ADMIN_PASSWORD\nAbCdEf\n</code></pre> <p>This password can be used for full-access on the ArgoCD web interface, or for the CLI in opus:</p> <pre><code>$ argocd login\nUsername: admin\nPassword:\n'admin:login' logged in successfully\nContext 'port-forward' updated\n$ argocd app list\n[...]\n</code></pre>"},{"location":"admin-guide/runbooks/argocd/#argocd-application","title":"ArgoCD application","text":"<p>Do not call this directly, use GitOps application instead.</p>"},{"location":"admin-guide/runbooks/cert-manager/","title":"cert-manager","text":"<p>TODO</p>"},{"location":"admin-guide/runbooks/dnsmasq/","title":"Dnsmasq","text":""},{"location":"admin-guide/runbooks/dnsmasq/#overview","title":"Overview","text":"<p>Dnsmasq is used as either a DHCP server or DHCP proxy server for PXE metal provisioning.</p> <p>Proxy mode can be enabled, allowing the use of existing DHCP servers on the network. A good description on how DHCP Proxy works can be found on the related FOG project wiki page</p>"},{"location":"admin-guide/runbooks/dnsmasq/#disabling-proxy-mode","title":"Disabling Proxy Mode","text":"<p>Certain scenarios will require this project to use a DHCP server, such as an air-gap deployment or dedicated VLAN. To disable proxy mode thereby using dnsmasq as a DHCP server, modify <code>metal/roles/pxe_server/defaults/main.yml</code> and set <code>dhcp_proxy</code> to <code>false</code></p>"},{"location":"admin-guide/runbooks/documentation/","title":"Documentation (this website)","text":"<p>Documents can be viewed at https://ubiquitycluster.github.io/ubiquity/.</p>"},{"location":"admin-guide/runbooks/documentation/#github-pages-setup","title":"GitHub Pages Setup","text":"<p>The documentation is automatically built and deployed using GitHub Actions to GitHub Pages with a custom domain.</p>"},{"location":"admin-guide/runbooks/documentation/#configuration-files","title":"Configuration Files","text":"<ul> <li>MkDocs Configuration: <code>mkdocs.yml</code> - Site configuration and navigation</li> <li>GitHub Actions: <code>.github/workflows/docs.yml</code> - Automated build and deployment</li> <li>Dependencies: <code>requirements.txt</code> - Python package dependencies</li> <li>Custom Domain: <code>docs/CNAME</code> - Custom domain configuration</li> </ul>"},{"location":"admin-guide/runbooks/documentation/#local-development","title":"Local Development","text":"<p>To edit and view locally, run:</p> <pre><code># Install dependencies\npip install -r requirements.txt\n\n# Serve locally with hot reload\nmkdocs serve\n</code></pre> <p>Then visit localhost:8000</p> <p>Alternatively, if available: <pre><code>make docs\n</code></pre></p>"},{"location":"admin-guide/runbooks/documentation/#deployment","title":"Deployment","text":"<p>Documentation is automatically deployed when: - Changes are pushed to the <code>main</code> branch - Files in the <code>docs/</code> directory are modified - The <code>mkdocs.yml</code> configuration is updated</p> <p>The GitHub Action will: 1. Build the static site using MkDocs Material 2. Deploy to GitHub Pages 3. Serve at the standard GitHub Pages URL <code>ubiquitycluster.github.io/ubiquity/</code></p>"},{"location":"admin-guide/runbooks/documentation/#features","title":"Features","text":"<ul> <li>Material Design: Modern, responsive design</li> <li>Search: Full-text search functionality  </li> <li>Navigation: Expandable navigation with indexes</li> <li>Code Copying: Copy buttons for code blocks</li> <li>Mermaid Diagrams: Support for Mermaid diagrams</li> <li>Emoji Support: GitHub-style emoji rendering</li> </ul>"},{"location":"admin-guide/runbooks/etcd-alerts/","title":"Alerts","text":""},{"location":"admin-guide/runbooks/etcd-alerts/#etcddatabasehighfragmentationratio","title":"etcdDatabaseHighFragmentationRatio","text":"<p>Example:</p> <pre><code>name: etcdDatabaseHighFragmentationRatio\nexpr: (last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes[5m]) / last_over_time(etcd_mvcc_db_total_size_in_bytes[5m])) &lt; 0.5\nfor: 10m\nlabels:\n  severity: warning\nannotations:\n  description: etcd cluster \"{{ $labels.job }}\": database size in use on instance {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.\n  runbook_url: https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation\n  summary: etcd database size in use is less than 50% of the actual allocated storage.\n</code></pre> <p>To fix on a kubespray installation:</p> <ul> <li>On each affected etcd member, as <code>root</code>:</li> </ul> <pre><code>. /etc/etcd.env\nexport ETCDCTL_API\nexport ETCDCTL_CERT\nexport ETCDCTL_KEY\nexport ETCDCTL_CACERT\nexport ETCDCTL_ENDPOINTS\netcdctl defrag\n</code></pre> <p>To fix on a cluster-api installation:</p> <pre><code>first_etcd_pod=\"$(kubectl get pods  -n kube-system --selector=component=etcd -A -o name | head -n 1)\"\nkubectl exec -n kube-system \"$first_etcd_pod\" -- \\\n  etcdctl defrag --cluster \\\n  --cacert /etc/kubernetes/pki/etcd/ca.crt \\\n  --key /etc/kubernetes/pki/etcd/server.key \\\n  --cert /etc/kubernetes/pki/etcd/server.crt\n</code></pre>"},{"location":"admin-guide/runbooks/etcd-alerts/#other-alerts","title":"Other alerts","text":"<p>See kube-prometheus runbooks.</p>"},{"location":"admin-guide/runbooks/gitea/","title":"Gitea","text":"<p>Gitea is a self-hosted Git service that provides a lightweight, fast, and secure Git hosting solution for the Ubiquity platform. It serves as the central repository for all code, configurations, and GitOps workflows within the cluster.</p>"},{"location":"admin-guide/runbooks/gitea/#overview","title":"Overview","text":"<p>Gitea in Ubiquity serves multiple critical functions:</p> <ul> <li>Central Git Repository: Hosts all cluster configuration and application code</li> <li>GitOps Source: Primary source for ArgoCD deployments and configuration management</li> <li>CI/CD Integration: Integrates with Argo Workflows for automated builds and deployments</li> <li>Repository Migration: Automatically migrates repositories from GitHub during cluster initialization</li> <li>OAuth Provider: Provides authentication for other services via OAuth2</li> </ul>"},{"location":"admin-guide/runbooks/gitea/#architecture","title":"Architecture","text":""},{"location":"admin-guide/runbooks/gitea/#core-components","title":"Core Components","text":"<ul> <li>Gitea Application: Main Git service with web interface</li> <li>PostgreSQL Database: Persistent storage for repositories and metadata</li> <li>Memcached: Caching layer for improved performance</li> <li>Persistent Storage: Longhorn-backed storage for repository data</li> </ul>"},{"location":"admin-guide/runbooks/gitea/#high-availability-configuration","title":"High Availability Configuration","text":"<p>Gitea is configured for high availability with: - 3 Replicas: Distributed across master nodes - Session Affinity: Cookie-based routing for consistent user experience - Node Selector: Runs exclusively on control plane nodes for stability</p>"},{"location":"admin-guide/runbooks/gitea/#configuration","title":"Configuration","text":""},{"location":"admin-guide/runbooks/gitea/#default-settings","title":"Default Settings","text":"<pre><code>gitea:\n  replicacount: 3\n  nodeSelector:\n    node-role.kubernetes.io/master: \"true\"\n  gitea:\n    config:\n      server:\n        LANDING_PAGE: explore\n        ROOT_URL: https://git.ubiquitycluster.uk\n</code></pre>"},{"location":"admin-guide/runbooks/gitea/#storage-configuration","title":"Storage Configuration","text":"<ul> <li>Persistence: 10Gi Longhorn storage for repository data</li> <li>Database: PostgreSQL with persistent storage</li> <li>Caching: Memcached for session and object caching</li> </ul>"},{"location":"admin-guide/runbooks/gitea/#ingress-configuration","title":"Ingress Configuration","text":"<p>Gitea is accessible via HTTPS with: - Domain: <code>git.ubiquitycluster.uk</code> - TLS: Automatic certificate management via cert-manager - Session Affinity: Cookie-based load balancing for consistent sessions</p>"},{"location":"admin-guide/runbooks/gitea/#accessing-gitea","title":"Accessing Gitea","text":""},{"location":"admin-guide/runbooks/gitea/#web-interface","title":"Web Interface","text":"<p>Access the Gitea web interface at: <pre><code>https://git.ubiquitycluster.uk\n</code></pre></p>"},{"location":"admin-guide/runbooks/gitea/#admin-credentials","title":"Admin Credentials","text":"<ul> <li>Username: <code>gitea_admin</code></li> <li>Password: Retrieved from Vault using External Secrets</li> </ul> <p>To get the admin password: <pre><code>kubectl get secret gitea-admin-secret -n gitea -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre></p>"},{"location":"admin-guide/runbooks/gitea/#git-access","title":"Git Access","text":"<p>Clone repositories using HTTPS: <pre><code>git clone https://git.ubiquitycluster.uk/ops/ubiquity.git\n</code></pre></p> <p>Or using SSH (requires SSH key setup): <pre><code>git clone git@git.ubiquitycluster.uk:ops/ubiquity.git\n</code></pre></p>"},{"location":"admin-guide/runbooks/gitea/#automated-configuration","title":"Automated Configuration","text":""},{"location":"admin-guide/runbooks/gitea/#repository-migration","title":"Repository Migration","text":"<p>Gitea automatically migrates repositories during cluster initialization:</p> <p>From GitHub to Gitea: - <code>ubiquitycluster/ubiquity</code> \u2192 <code>ops/ubiquity</code> - <code>ubiquitycluster/blog</code> \u2192 <code>ubiquity/blog</code> (mirror) - <code>ubiquitycluster/backstage</code> \u2192 <code>ubiquity/backstage</code> (mirror)</p>"},{"location":"admin-guide/runbooks/gitea/#organization-setup","title":"Organization Setup","text":"<p>Automatic organization creation: - ops: Operations team organization - ubiquity: General project organization</p>"},{"location":"admin-guide/runbooks/gitea/#oauth-applications","title":"OAuth Applications","text":"<p>Gitea automatically creates OAuth applications for: - Dex: SSO integration - Keycloak: Identity provider integration - ArgoCD: GitOps authentication (future)</p>"},{"location":"admin-guide/runbooks/gitea/#access-tokens","title":"Access Tokens","text":"<p>Automated access token generation for: - Renovate: Dependency update automation - CI/CD Systems: Automated builds and deployments</p>"},{"location":"admin-guide/runbooks/gitea/#integration-with-ubiquity-components","title":"Integration with Ubiquity Components","text":""},{"location":"admin-guide/runbooks/gitea/#argocd-gitops","title":"ArgoCD GitOps","text":"<p>Gitea serves as the primary source for ArgoCD applications:</p> <ol> <li>Initial Bootstrap: ArgoCD initially sources from GitHub</li> <li>Migration: Repository is migrated to Gitea during platform deployment</li> <li>Switch Over: ArgoCD switches to use Gitea as the source</li> <li>Self-Hosting: Complete GitOps workflow becomes self-contained</li> </ol>"},{"location":"admin-guide/runbooks/gitea/#cicd-integration","title":"CI/CD Integration","text":"<p>Integration with Argo Workflows: - Webhook Triggers: Git push events trigger workflow execution - Build Pipelines: Automated container builds and deployments - Security Scanning: Automated vulnerability scanning of code</p>"},{"location":"admin-guide/runbooks/gitea/#secret-management","title":"Secret Management","text":"<p>Integration with Vault for: - Admin Credentials: Secure storage of admin passwords - OAuth Secrets: Client IDs and secrets for SSO - Access Tokens: Service account tokens for automation</p>"},{"location":"admin-guide/runbooks/gitea/#repository-management","title":"Repository Management","text":""},{"location":"admin-guide/runbooks/gitea/#creating-repositories","title":"Creating Repositories","text":"<p>Via Web Interface: 1. Navigate to <code>https://git.ubiquitycluster.uk</code> 2. Click \"+\" \u2192 \"New Repository\" 3. Configure repository settings 4. Initialize with README if needed</p> <p>Via Configuration: Add to <code>platform/gitea/files/config/config.yaml</code>: <pre><code>repositories:\n  - name: my-new-repo\n    owner: ops\n    private: false\n</code></pre></p>"},{"location":"admin-guide/runbooks/gitea/#repository-permissions","title":"Repository Permissions","text":"<p>Organization-based access: - ops: Administrative repositories - ubiquity: General project repositories - Team membership: Controls access levels</p>"},{"location":"admin-guide/runbooks/gitea/#backup-and-recovery","title":"Backup and Recovery","text":"<p>Automatic Backups: - Repository data stored on Longhorn (replicated storage) - Database backups via PostgreSQL backup procedures - Configuration stored in Git (self-healing)</p>"},{"location":"admin-guide/runbooks/gitea/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":""},{"location":"admin-guide/runbooks/gitea/#health-monitoring","title":"Health Monitoring","text":"<p>Gitea health is monitored through: - Kubernetes Probes: Liveness and readiness checks - Prometheus Metrics: Application performance metrics - Grafana Dashboards: Visual monitoring of service health</p>"},{"location":"admin-guide/runbooks/gitea/#log-analysis","title":"Log Analysis","text":"<p>Access Gitea logs: <pre><code># View application logs\nkubectl logs -n gitea deployment/gitea -f\n\n# View all pod logs\nkubectl logs -n gitea -l app.kubernetes.io/name=gitea\n</code></pre></p>"},{"location":"admin-guide/runbooks/gitea/#database-maintenance","title":"Database Maintenance","text":"<p>PostgreSQL maintenance: <pre><code># Access database\nkubectl exec -it -n gitea statefulset/gitea-postgresql -- psql -U gitea\n\n# Check database size\nkubectl exec -it -n gitea statefulset/gitea-postgresql -- psql -U gitea -c \"\\l+\"\n</code></pre></p>"},{"location":"admin-guide/runbooks/gitea/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/runbooks/gitea/#common-issues","title":"Common Issues","text":""},{"location":"admin-guide/runbooks/gitea/#repository-clone-failures","title":"Repository Clone Failures","text":"<p>Symptoms: Unable to clone repositories via HTTPS or SSH</p> <p>Solutions: 1. Check ingress controller status: <code>kubectl get ingress -n gitea</code> 2. Verify certificate validity: <code>kubectl describe certificate gitea-tls-certificate -n gitea</code> 3. Test connectivity: <code>curl -I https://git.ubiquitycluster.uk</code></p>"},{"location":"admin-guide/runbooks/gitea/#authentication-problems","title":"Authentication Problems","text":"<p>Symptoms: Unable to login or access repositories</p> <p>Solutions: 1. Verify admin secret: <code>kubectl get secret gitea-admin-secret -n gitea</code> 2. Check External Secrets: <code>kubectl get externalsecret -n gitea</code> 3. Validate Vault connectivity: <code>kubectl logs -n gitea deployment/gitea | grep vault</code></p>"},{"location":"admin-guide/runbooks/gitea/#performance-issues","title":"Performance Issues","text":"<p>Symptoms: Slow Git operations or web interface</p> <p>Solutions: 1. Check Memcached status: <code>kubectl get pods -n gitea -l app.kubernetes.io/name=memcached</code> 2. Monitor resource usage: <code>kubectl top pods -n gitea</code> 3. Review storage performance: <code>kubectl get pvc -n gitea</code></p>"},{"location":"admin-guide/runbooks/gitea/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check Gitea service status\nkubectl get all -n gitea\n\n# View configuration\nkubectl get configmap gitea-config -n gitea -o yaml\n\n# Check storage usage\nkubectl exec -n gitea deployment/gitea -- df -h /data\n\n# Test database connectivity\nkubectl exec -n gitea deployment/gitea -- gitea doctor check\n\n# View recent events\nkubectl get events -n gitea --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"admin-guide/runbooks/gitea/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"admin-guide/runbooks/gitea/#pod-recovery","title":"Pod Recovery","text":"<pre><code># Restart Gitea pods\nkubectl rollout restart deployment/gitea -n gitea\n\n# Force pod recreation\nkubectl delete pods -n gitea -l app.kubernetes.io/name=gitea\n</code></pre>"},{"location":"admin-guide/runbooks/gitea/#database-recovery","title":"Database Recovery","text":"<pre><code># Access PostgreSQL\nkubectl exec -it -n gitea statefulset/gitea-postgresql -- bash\n\n# Check database integrity\ngitea doctor check --all\n</code></pre>"},{"location":"admin-guide/runbooks/gitea/#security-considerations","title":"Security Considerations","text":""},{"location":"admin-guide/runbooks/gitea/#access-control","title":"Access Control","text":"<ul> <li>Admin Access: Limited to necessary personnel only</li> <li>Organization Permissions: Role-based access control</li> <li>SSH Keys: Regular rotation and monitoring</li> <li>OAuth Scopes: Minimal required permissions</li> </ul>"},{"location":"admin-guide/runbooks/gitea/#network-security","title":"Network Security","text":"<ul> <li>TLS Encryption: All traffic encrypted in transit</li> <li>Internal Communication: Pod-to-pod encryption</li> <li>Network Policies: Restricted network access</li> <li>Ingress Protection: WAF and rate limiting</li> </ul>"},{"location":"admin-guide/runbooks/gitea/#data-protection","title":"Data Protection","text":"<ul> <li>Repository Encryption: Git objects encrypted at rest</li> <li>Database Encryption: PostgreSQL data encryption</li> <li>Backup Encryption: Encrypted backup storage</li> <li>Secret Management: Vault-based secret storage</li> </ul>"},{"location":"admin-guide/runbooks/gitea/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/runbooks/gitea/#repository-organization","title":"Repository Organization","text":"<ol> <li>Use Organizations: Group related repositories</li> <li>Naming Conventions: Consistent repository naming</li> <li>Branch Protection: Protect main/master branches</li> <li>Access Reviews: Regular permission audits</li> </ol>"},{"location":"admin-guide/runbooks/gitea/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Large Files: Use Git LFS for large assets</li> <li>Repository Size: Monitor and manage repository growth</li> <li>Caching: Leverage Memcached for performance</li> <li>Resource Limits: Appropriate CPU/memory allocation</li> </ol>"},{"location":"admin-guide/runbooks/gitea/#backup-strategy","title":"Backup Strategy","text":"<ol> <li>Multiple Replicas: Longhorn 3-way replication</li> <li>Database Backups: Regular PostgreSQL dumps</li> <li>Configuration Backup: Git-based configuration management</li> <li>Disaster Recovery: Documented recovery procedures</li> </ol>"},{"location":"admin-guide/runbooks/gitea/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"admin-guide/runbooks/gitea/#custom-themes","title":"Custom Themes","text":"<p>Customize Gitea appearance: 1. Create custom CSS/templates 2. Mount as ConfigMap or Secret 3. Update deployment to include custom assets</p>"},{"location":"admin-guide/runbooks/gitea/#hook-scripts","title":"Hook Scripts","text":"<p>Implement custom Git hooks: 1. Create hook scripts 2. Mount via ConfigMap 3. Configure repository-specific hooks</p>"},{"location":"admin-guide/runbooks/gitea/#federation","title":"Federation","text":"<p>For multi-cluster scenarios: 1. Configure repository mirroring 2. Set up cross-cluster authentication 3. Implement distributed backup strategies</p>"},{"location":"admin-guide/runbooks/gitea/#migration-procedures","title":"Migration Procedures","text":""},{"location":"admin-guide/runbooks/gitea/#importing-existing-repositories","title":"Importing Existing Repositories","text":"<p>From GitHub: <pre><code># Add to config.yaml\nrepositories:\n  - name: existing-repo\n    owner: ops\n    migrate:\n      source: https://github.com/org/existing-repo\n      mirror: false\n</code></pre></p> <p>From GitLab: 1. Export repository data 2. Create new repository in Gitea 3. Push existing data to new repository</p>"},{"location":"admin-guide/runbooks/gitea/#exporting-data","title":"Exporting Data","text":"<p>Repository Export: <pre><code># Clone with full history\ngit clone --mirror https://git.ubiquitycluster.uk/ops/repo.git\n\n# Export to external location\ngit push --mirror https://external-git.example.com/org/repo.git\n</code></pre></p>"},{"location":"admin-guide/runbooks/gitea/#integration-examples","title":"Integration Examples","text":""},{"location":"admin-guide/runbooks/gitea/#argocd-integration","title":"ArgoCD Integration","text":"<pre><code># Application source configuration\nsource:\n  repoURL: https://git.ubiquitycluster.uk/ops/ubiquity.git\n  targetRevision: main\n  path: system/monitoring-system\n</code></pre>"},{"location":"admin-guide/runbooks/gitea/#webhook-configuration","title":"Webhook Configuration","text":"<pre><code># Argo Events webhook\nwebhook:\n  endpoint: /gitea\n  method: POST\n  url: https://git.ubiquitycluster.uk/ops/ubiquity/settings/hooks\n</code></pre> <p>This comprehensive documentation provides administrators with everything needed to manage, troubleshoot, and optimize Gitea within the Ubiquity platform.</p>"},{"location":"admin-guide/runbooks/longhorn/","title":"Longhorn","text":"<p>Longhorn is a distributed block storage system designed for Kubernetes that provides persistent storage for containerized workloads in Ubiquity clusters. It creates replicated block storage across multiple nodes to ensure high availability and data persistence.</p>"},{"location":"admin-guide/runbooks/longhorn/#overview","title":"Overview","text":"<p>Longhorn provides distributed storage capabilities for Ubiquity by:</p> <ul> <li>Creating replicated block storage volumes across cluster nodes</li> <li>Enabling persistent storage for stateful applications</li> <li>Providing snapshot and backup capabilities</li> <li>Offering a web-based management interface</li> <li>Integrating with Kubernetes Storage Classes and Persistent Volume Claims</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#architecture","title":"Architecture","text":"<p>Longhorn consists of several key components:</p>"},{"location":"admin-guide/runbooks/longhorn/#manager-pods","title":"Manager Pods","text":"<ul> <li>longhorn-manager: Runs on each node, manages volumes and handles orchestration</li> <li>longhorn-driver: CSI driver components for Kubernetes integration</li> <li>longhorn-ui: Web interface for management and monitoring</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#storage-components","title":"Storage Components","text":"<ul> <li>Longhorn Engine: Handles volume operations and replication</li> <li>Replica Instances: Store actual data blocks across multiple nodes</li> <li>Recovery Backend: Handles backup and restore operations</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#configuration","title":"Configuration","text":""},{"location":"admin-guide/runbooks/longhorn/#default-settings","title":"Default Settings","text":"<p>The Longhorn system in Ubiquity is configured with these defaults:</p> <pre><code>defaultSettings:\n  defaultReplicaCount: 3\n  disableSchedulingOnCordonedNode: true\n  nodeDownPodDeletionPolicy: delete-both-statefulset-and-deployment-pod\n  replicaAutoBalance: best-effort\n  replicaSoftAntiAffinity: false\n  storageMinimalAvailablePercentage: 10\n  taintToleration: StorageNode=true:PreferNoSchedule\n</code></pre>"},{"location":"admin-guide/runbooks/longhorn/#storage-classes","title":"Storage Classes","text":"<p>Longhorn provides the default storage class with: - Default replica count: 3 replicas across different nodes - File system: ext4 - Replica auto-balance: best-effort for even distribution</p>"},{"location":"admin-guide/runbooks/longhorn/#node-configuration","title":"Node Configuration","text":"<p>During installation, nodes are prepared with: - NFS client tools for NFS support - Dedicated <code>/var/lib/longhorn</code> partition (40-60% of data volume) - Proper kernel modules loaded</p>"},{"location":"admin-guide/runbooks/longhorn/#accessing-longhorn-ui","title":"Accessing Longhorn UI","text":"<p>The Longhorn web interface is available at: <pre><code>https://longhorn.ubiquitycluster.uk\n</code></pre></p> <p>Features include: - Volume management and monitoring - Node and disk management - Backup and snapshot operations - Performance metrics and health status</p>"},{"location":"admin-guide/runbooks/longhorn/#common-operations","title":"Common Operations","text":""},{"location":"admin-guide/runbooks/longhorn/#creating-persistent-volumes","title":"Creating Persistent Volumes","text":"<ol> <li> <p>Using Storage Class (Recommended): <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\n  namespace: my-namespace\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre></p> </li> <li> <p>Direct Volume Creation:</p> </li> <li>Use Longhorn UI to create volumes manually</li> <li>Attach to nodes as needed</li> </ol>"},{"location":"admin-guide/runbooks/longhorn/#volume-operations","title":"Volume Operations","text":"<ul> <li>Snapshots: Create point-in-time snapshots via UI or CLI</li> <li>Backups: Configure S3-compatible backup targets</li> <li>Volume Expansion: Resize volumes online through PVC expansion</li> <li>Volume Migration: Move volumes between nodes for maintenance</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#monitoring-integration","title":"Monitoring Integration","text":"<p>Longhorn integrates with Prometheus monitoring: - ServiceMonitor automatically configured - Metrics endpoint: <code>:9500/metrics</code> - Grafana dashboards available for visualization</p>"},{"location":"admin-guide/runbooks/longhorn/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/runbooks/longhorn/#common-issues","title":"Common Issues","text":""},{"location":"admin-guide/runbooks/longhorn/#volume-mount-failures","title":"Volume Mount Failures","text":"<p>Symptoms: Pods stuck in ContainerCreating with volume mount errors</p> <p>Solutions: 1. Check node disk space: <code>df -h /var/lib/longhorn</code> 2. Verify longhorn-manager pods are running: <code>kubectl get pods -n longhorn-system</code> 3. Check volume status in Longhorn UI</p>"},{"location":"admin-guide/runbooks/longhorn/#replica-scheduling-problems","title":"Replica Scheduling Problems","text":"<p>Symptoms: Volumes showing degraded state or insufficient replicas</p> <p>Solutions: 1. Verify node labels and taints: <code>kubectl get nodes --show-labels</code> 2. Check disk space on storage nodes 3. Review replica anti-affinity settings in Longhorn UI</p>"},{"location":"admin-guide/runbooks/longhorn/#performance-issues","title":"Performance Issues","text":"<p>Symptoms: Slow I/O operations or high latency</p> <p>Solutions: 1. Monitor disk I/O on storage nodes 2. Check network connectivity between nodes 3. Review replica placement and consider rebalancing 4. Verify storage node resources (CPU/Memory)</p>"},{"location":"admin-guide/runbooks/longhorn/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check Longhorn system status\nkubectl get pods -n longhorn-system\n\n# View Longhorn manager logs\nkubectl logs -n longhorn-system -l app=longhorn-manager\n\n# Check volume status\nkubectl get pv,pvc -A\n\n# View storage class configuration\nkubectl get storageclass longhorn -o yaml\n\n# Check node storage capacity\nkubectl get nodes -o custom-columns=NAME:.metadata.name,CAPACITY:.status.capacity.storage\n</code></pre>"},{"location":"admin-guide/runbooks/longhorn/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"admin-guide/runbooks/longhorn/#node-failure-recovery","title":"Node Failure Recovery","text":"<ol> <li>Longhorn automatically handles single node failures</li> <li>Replicas on failed node will be rebuilt on healthy nodes</li> <li>Monitor rebuild progress in Longhorn UI</li> </ol>"},{"location":"admin-guide/runbooks/longhorn/#data-recovery-from-backup","title":"Data Recovery from Backup","text":"<ol> <li>Configure backup target in Longhorn settings</li> <li>Create regular volume backups</li> <li>Restore from backup when needed via UI</li> </ol>"},{"location":"admin-guide/runbooks/longhorn/#maintenance","title":"Maintenance","text":""},{"location":"admin-guide/runbooks/longhorn/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Monitor Storage Usage: Keep storage utilization below 85%</li> <li>Check Replica Health: Ensure all volumes have healthy replicas</li> <li>Backup Critical Volumes: Schedule regular backups to external storage</li> <li>Node Maintenance: Properly drain nodes before maintenance</li> </ol>"},{"location":"admin-guide/runbooks/longhorn/#storage-expansion","title":"Storage Expansion","text":"<p>To add storage capacity: 1. Add new nodes with storage to the cluster 2. Label nodes appropriately for Longhorn scheduling 3. Longhorn will automatically discover and use new storage</p>"},{"location":"admin-guide/runbooks/longhorn/#single-node-adjustments","title":"Single Node Adjustments","text":"<p>For single-node clusters, modify replica settings: <pre><code># In system/longhorn-system/values.yaml\npersistence:\n  defaultClassReplicaCount: 1\n</code></pre></p>"},{"location":"admin-guide/runbooks/longhorn/#integration-with-ubiquity-components","title":"Integration with Ubiquity Components","text":""},{"location":"admin-guide/runbooks/longhorn/#hpc-workloads","title":"HPC Workloads","text":"<ul> <li>Provides persistent storage for Slurm job data</li> <li>Supports shared storage scenarios through ReadWriteMany access modes</li> <li>Integrates with NFS for traditional HPC workflows</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#backup-integration","title":"Backup Integration","text":"<ul> <li>Works with Velero for cluster-wide backup/restore</li> <li>Supports volume snapshots for application-consistent backups</li> <li>Integrates with external backup targets (S3, NFS)</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#monitoring-integration_1","title":"Monitoring Integration","text":"<ul> <li>Metrics exposed to Prometheus</li> <li>Grafana dashboards for storage monitoring</li> <li>Alert rules for storage capacity and health</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#security-considerations","title":"Security Considerations","text":"<ul> <li>Volume encryption at rest (when supported by underlying storage)</li> <li>Network traffic encryption between replicas</li> <li>RBAC integration for access control</li> <li>Secure backup target configuration</li> </ul>"},{"location":"admin-guide/runbooks/longhorn/#performance-tuning","title":"Performance Tuning","text":""},{"location":"admin-guide/runbooks/longhorn/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use local SSDs for better performance</li> <li>Configure appropriate replica count based on availability requirements</li> <li>Monitor and tune network performance between storage nodes</li> <li>Use volume locality settings for performance-critical workloads</li> </ol>"},{"location":"admin-guide/runbooks/longhorn/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Minimum: 2 CPU cores, 4GB RAM per storage node</li> <li>Recommended: 4+ CPU cores, 8GB+ RAM for production workloads</li> <li>Storage: Dedicated storage devices or partitions preferred</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/","title":"Onyxia","text":"<p>Onyxia is a self-service data science platform that provides researchers and data scientists with on-demand access to containerized data science tools, HPC environments, and interactive development environments. In the Ubiquity platform, Onyxia serves as the primary interface for self-service HPC and data science workloads.</p>"},{"location":"admin-guide/runbooks/onyxia/#overview","title":"Overview","text":"<p>Onyxia in Ubiquity serves multiple critical functions:</p> <ul> <li>Self-Service Data Science Platform: Provides on-demand access to Jupyter notebooks, RStudio, VS Code, and other data science tools</li> <li>HPC Job Launcher: Integrates with SLURM and HTCondor for submitting and managing HPC workloads</li> <li>Interactive Computing Environment: Offers containerized environments with pre-configured software stacks</li> <li>Multi-User Platform: Supports user isolation and resource quotas through Kubernetes namespaces</li> <li>Catalog Management: Provides curated collections of data science and HPC applications</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/#architecture","title":"Architecture","text":""},{"location":"admin-guide/runbooks/onyxia/#core-components","title":"Core Components","text":"<ul> <li>Onyxia Web UI: React-based web interface for service management</li> <li>Onyxia API: Backend service for orchestrating Kubernetes deployments</li> <li>Helm Chart Catalogs: Collections of pre-configured applications and services</li> <li>Service Discovery: Integration with Kubernetes for service management</li> <li>Authentication Integration: OAuth2/OIDC integration with Keycloak</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/#high-availability-configuration","title":"High Availability Configuration","text":"<p>Onyxia is configured for high availability with: - Single Replica API: Lightweight API server on master nodes - Node Selector: Runs exclusively on control plane nodes for stability - Session Affinity: Cookie-based routing for consistent user experience - Ingress Load Balancing: NGINX-based load balancing with SSL termination</p>"},{"location":"admin-guide/runbooks/onyxia/#configuration","title":"Configuration","text":""},{"location":"admin-guide/runbooks/onyxia/#default-settings","title":"Default Settings","text":"<pre><code>onyxia:\n  serviceAccount:\n    clusterAdmin: true\n  ui:\n    image:\n      name: inseefrlab/onyxia-web\n      version: 2.13.53\n    nodeSelector:\n      node-role.kubernetes.io/master: \"true\"\n    env:\n      KEYCLOAK_REALM: ubiquity\n      KEYCLOAK_CLIENT_ID: ubiquity-client\n      THEME_ID: ultraviolet\n      HEADER_ORGANIZATION: Ubiquity\n      HEADER_USECASE_DESCRIPTION: HPCLab\n</code></pre>"},{"location":"admin-guide/runbooks/onyxia/#ingress-configuration","title":"Ingress Configuration","text":"<p>Onyxia is accessible via HTTPS with: - Domain: <code>datalab.ubiquitycluster.uk</code> - TLS: Automatic certificate management via cert-manager - Session Affinity: Cookie-based load balancing for consistent sessions - CORS: Enabled for cross-origin requests</p>"},{"location":"admin-guide/runbooks/onyxia/#authentication-configuration","title":"Authentication Configuration","text":"<ul> <li>Provider: Keycloak OAuth2/OIDC</li> <li>Realm: <code>ubiquity</code></li> <li>Client ID: <code>ubiquity-client</code></li> <li>JWT Token: Used for API authentication</li> <li>User Namespace Isolation: Automatic namespace creation per user</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/#accessing-onyxia","title":"Accessing Onyxia","text":""},{"location":"admin-guide/runbooks/onyxia/#web-interface","title":"Web Interface","text":"<p>Access the Onyxia web interface at: <pre><code>https://datalab.ubiquitycluster.uk\n</code></pre></p>"},{"location":"admin-guide/runbooks/onyxia/#authentication-flow","title":"Authentication Flow","text":"<ol> <li>Initial Access: Navigate to Onyxia URL</li> <li>Keycloak Redirect: Automatic redirect to Keycloak for authentication</li> <li>User Credentials: Login with Ubiquity cluster credentials</li> <li>Token Exchange: OAuth2 token exchange for API access</li> <li>Dashboard Access: Access to personalized Onyxia dashboard</li> </ol>"},{"location":"admin-guide/runbooks/onyxia/#user-dashboard-features","title":"User Dashboard Features","text":"<ul> <li>Service Catalog: Browse available data science tools and HPC applications</li> <li>Running Services: Monitor and manage active deployments</li> <li>File Browser: Access to persistent storage and shared filesystems</li> <li>Configuration Management: Personal settings and preferences</li> <li>Resource Monitoring: View resource usage and quotas</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/#service-catalogs","title":"Service Catalogs","text":""},{"location":"admin-guide/runbooks/onyxia/#available-catalogs","title":"Available Catalogs","text":"<p>Ubiquity Data Science Catalog: - Repository: <code>https://cjcshadowsan.github.io/helm-charts-datascience</code> - Maintainer: maintainers@ubiquitycluster.org - Status: Production - Content: Custom data science tools and HPC applications</p> <p>InseeFrLab Datascience Catalog: - Repository: <code>https://inseefrlab.github.io/helm-charts-datascience</code> - Maintainer: innovation@insee.fr - Status: Production - Content: Comprehensive data science and analytics tools</p> <p>InseeFrLab Interactive Services: - Repository: <code>https://inseefrlab.github.io/helm-charts-interactive-services</code> - Maintainer: innovation@insee.fr - Status: Production - Content: Interactive development environments and tools</p>"},{"location":"admin-guide/runbooks/onyxia/#common-applications","title":"Common Applications","text":"<p>Data Science Tools: - Jupyter Notebooks (Python, R, Scala) - RStudio Server - VS Code Server - Apache Zeppelin</p> <p>Analytics Platforms: - Apache Spark - Apache Flink - Dask - Ray</p> <p>Machine Learning: - MLflow - Kubeflow - TensorFlow Serving - PyTorch</p> <p>Databases: - PostgreSQL - MongoDB - Redis - InfluxDB</p> <p>HPC Tools: - SLURM Job Submission Interface - HTCondor Submission Portal - Parallel Computing Environments - GPU-accelerated Computing</p>"},{"location":"admin-guide/runbooks/onyxia/#user-namespace-management","title":"User Namespace Management","text":""},{"location":"admin-guide/runbooks/onyxia/#automatic-namespace-creation","title":"Automatic Namespace Creation","text":"<p>Onyxia automatically creates isolated namespaces for users:</p> <p>Namespace Pattern: - Individual Users: <code>user-{username}</code> - Group Projects: <code>project-{groupname}</code> - Username Prefix: <code>oidc-</code> for OpenID Connect users</p>"},{"location":"admin-guide/runbooks/onyxia/#resource-isolation","title":"Resource Isolation","text":"<p>Features: - CPU/Memory Quotas: Configurable per user/group - Storage Quotas: Persistent volume claim limits - Network Policies: Isolation between user workspaces - Pod Security: Security contexts and policies</p>"},{"location":"admin-guide/runbooks/onyxia/#quota-configuration","title":"Quota Configuration","text":"<p>Default quotas can be configured per region: <pre><code>quotas:\n  enabled: true\n  allowUserModification: false\n  default:\n    requests.storage: 1Gi\n    count/pods: \"10\"\n</code></pre></p>"},{"location":"admin-guide/runbooks/onyxia/#integration-with-ubiquity-components","title":"Integration with Ubiquity Components","text":""},{"location":"admin-guide/runbooks/onyxia/#keycloak-authentication","title":"Keycloak Authentication","text":"<p>Single Sign-On Integration: - Unified authentication across Ubiquity services - Role-based access control (RBAC) - Group membership management - Multi-factor authentication support</p>"},{"location":"admin-guide/runbooks/onyxia/#vault-integration","title":"Vault Integration","text":"<p>Secret Management: - Automatic injection of secrets into user environments - Database credentials and API keys - Secure storage of user configurations - Integration with service authentication</p>"},{"location":"admin-guide/runbooks/onyxia/#storage-integration","title":"Storage Integration","text":"<p>Persistent Storage: - Longhorn-backed persistent volumes - NFS shared storage integration - S3-compatible object storage (future) - User home directories and shared project spaces</p>"},{"location":"admin-guide/runbooks/onyxia/#hpc-integration","title":"HPC Integration","text":"<p>SLURM Integration: - Direct job submission from Onyxia services - Resource allocation and scheduling - Job monitoring and management - Interactive and batch job support</p> <p>HTCondor Integration: - High-throughput computing workflows - Container-based job execution - Distributed computing across cluster nodes</p>"},{"location":"admin-guide/runbooks/onyxia/#service-deployment-and-management","title":"Service Deployment and Management","text":""},{"location":"admin-guide/runbooks/onyxia/#launching-services","title":"Launching Services","text":"<p>Via Web Interface: 1. Browse service catalog 2. Select desired application/tool 3. Configure service parameters 4. Deploy to personal namespace 5. Access via generated URL</p> <p>Service Configuration Options: - Resource requests (CPU, memory, GPU) - Storage requirements and persistence - Environment variables and secrets - Network configuration and ingress - Security contexts and policies</p>"},{"location":"admin-guide/runbooks/onyxia/#service-lifecycle-management","title":"Service Lifecycle Management","text":"<p>Operations: - Start/Stop: Pause and resume services - Scale: Adjust resource allocation - Update: Upgrade to new versions - Delete: Clean up unused services - Clone: Duplicate service configurations</p>"},{"location":"admin-guide/runbooks/onyxia/#persistent-data","title":"Persistent Data","text":"<p>Storage Options: - Personal Storage: User-specific persistent volumes - Shared Storage: Project-based shared filesystems - Temporary Storage: Ephemeral storage for compute jobs - External Storage: Integration with external data sources</p>"},{"location":"admin-guide/runbooks/onyxia/#monitoring-and-administration","title":"Monitoring and Administration","text":""},{"location":"admin-guide/runbooks/onyxia/#service-monitoring","title":"Service Monitoring","text":"<p>User Monitoring: - Resource usage dashboards - Service health status - Performance metrics - Cost tracking (resource consumption)</p> <p>Administrator Monitoring: - Platform-wide resource utilization - User activity and service deployments - Catalog usage statistics - Performance and capacity planning</p>"},{"location":"admin-guide/runbooks/onyxia/#log-management","title":"Log Management","text":"<p>User Logs: <pre><code># Access service logs via Onyxia UI\n# Or via kubectl\nkubectl logs -n user-{username} {service-pod-name}\n</code></pre></p> <p>Platform Logs: <pre><code># Onyxia API logs\nkubectl logs -n onyxia deployment/onyxia-api\n\n# Web UI logs (nginx)\nkubectl logs -n onyxia deployment/onyxia-ui\n</code></pre></p>"},{"location":"admin-guide/runbooks/onyxia/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/runbooks/onyxia/#common-issues","title":"Common Issues","text":""},{"location":"admin-guide/runbooks/onyxia/#service-deployment-failures","title":"Service Deployment Failures","text":"<p>Symptoms: Services fail to start or remain in pending state</p> <p>Solutions: 1. Check resource quotas: <code>kubectl describe quota -n user-{username}</code> 2. Verify image pull permissions: <code>kubectl describe pod -n user-{username} {pod-name}</code> 3. Validate storage availability: <code>kubectl get pvc -n user-{username}</code> 4. Review security policies: <code>kubectl describe psp -n user-{username}</code></p>"},{"location":"admin-guide/runbooks/onyxia/#authentication-problems","title":"Authentication Problems","text":"<p>Symptoms: Unable to login or access services</p> <p>Solutions: 1. Verify Keycloak connectivity: <code>curl -I https://keycloak.ubiquitycluster.uk/auth</code> 2. Check OAuth client configuration: Review Keycloak admin console 3. Validate token exchange: Check browser developer tools for API errors 4. Verify user permissions: Check Keycloak user roles and groups</p>"},{"location":"admin-guide/runbooks/onyxia/#resource-exhaustion","title":"Resource Exhaustion","text":"<p>Symptoms: Cannot deploy new services or services are terminated</p> <p>Solutions: 1. Check cluster resources: <code>kubectl top nodes</code> 2. Review user quotas: <code>kubectl get resourcequota -n user-{username}</code> 3. Clean up unused services: Delete stopped services via Onyxia UI 4. Monitor storage usage: <code>kubectl get pvc -n user-{username}</code></p>"},{"location":"admin-guide/runbooks/onyxia/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check Onyxia components\nkubectl get all -n onyxia\n\n# View API configuration\nkubectl get configmap onyxia-api-config -n onyxia -o yaml\n\n# Check user namespaces\nkubectl get namespaces | grep \"user-\\|project-\"\n\n# Monitor resource usage\nkubectl top pods -n onyxia\nkubectl describe node {node-name}\n\n# Check ingress status\nkubectl get ingress -n onyxia\nkubectl describe certificate onyxia-tls-certificate -n onyxia\n</code></pre>"},{"location":"admin-guide/runbooks/onyxia/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"admin-guide/runbooks/onyxia/#service-recovery","title":"Service Recovery","text":"<pre><code># Restart Onyxia API\nkubectl rollout restart deployment/onyxia-api -n onyxia\n\n# Restart Web UI\nkubectl rollout restart deployment/onyxia-ui -n onyxia\n\n# Clean up stuck user services\nkubectl delete pods --field-selector=status.phase=Failed -n user-{username}\n</code></pre>"},{"location":"admin-guide/runbooks/onyxia/#configuration-recovery","title":"Configuration Recovery","text":"<pre><code># Verify Keycloak integration\nkubectl exec -n onyxia deployment/onyxia-api -- curl -s https://keycloak.ubiquitycluster.uk/auth/realms/ubiquity/.well-known/openid_configuration\n\n# Test catalog connectivity\nkubectl exec -n onyxia deployment/onyxia-api -- curl -s https://inseefrlab.github.io/helm-charts-datascience/index.yaml\n</code></pre>"},{"location":"admin-guide/runbooks/onyxia/#security-considerations","title":"Security Considerations","text":""},{"location":"admin-guide/runbooks/onyxia/#access-control","title":"Access Control","text":"<ul> <li>Authentication: Keycloak-based OAuth2/OIDC</li> <li>Authorization: Kubernetes RBAC integration</li> <li>Namespace Isolation: Strict user/group separation</li> <li>Network Policies: Controlled inter-service communication</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/#container-security","title":"Container Security","text":"<ul> <li>Image Scanning: Vulnerability scanning for deployed images</li> <li>Security Contexts: Non-root containers and security policies</li> <li>Resource Limits: Prevent resource exhaustion attacks</li> <li>Secrets Management: Vault integration for sensitive data</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/#data-protection","title":"Data Protection","text":"<ul> <li>Encryption at Rest: Longhorn storage encryption</li> <li>Encryption in Transit: TLS for all communications</li> <li>Data Isolation: User-specific persistent volumes</li> <li>Backup Protection: Encrypted backup storage</li> </ul>"},{"location":"admin-guide/runbooks/onyxia/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/runbooks/onyxia/#service-management","title":"Service Management","text":"<ol> <li>Resource Planning: Set appropriate CPU/memory requests</li> <li>Storage Management: Clean up unused persistent volumes</li> <li>Service Cleanup: Regularly remove stopped services</li> <li>Configuration Backup: Export service configurations</li> </ol>"},{"location":"admin-guide/runbooks/onyxia/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Resource Requests: Set realistic resource requirements</li> <li>Node Affinity: Use node selectors for workload placement</li> <li>Persistent Storage: Use appropriate storage classes</li> <li>Network Optimization: Minimize cross-node communication</li> </ol>"},{"location":"admin-guide/runbooks/onyxia/#user-training","title":"User Training","text":"<ol> <li>Platform Orientation: Provide user onboarding documentation</li> <li>Service Catalog: Maintain updated service descriptions</li> <li>Best Practices: Share resource management guidelines</li> <li>Support Channels: Establish clear support procedures</li> </ol>"},{"location":"admin-guide/runbooks/onyxia/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"admin-guide/runbooks/onyxia/#custom-service-catalogs","title":"Custom Service Catalogs","text":"<p>Adding Custom Catalogs: 1. Create Helm repository 2. Update Onyxia configuration 3. Configure catalog metadata 4. Test service deployments</p> <p>Catalog Structure: <pre><code>catalogs:\n  - id: custom-catalog\n    name: Custom Catalog\n    description: Organization-specific tools\n    maintainer: admin@organization.com\n    location: https://charts.organization.com\n    status: PROD\n    type: helm\n</code></pre></p>"},{"location":"admin-guide/runbooks/onyxia/#integration-extensions","title":"Integration Extensions","text":"<p>Custom Authentication: - LDAP/Active Directory integration - SAML federation - Multi-realm support</p> <p>Storage Backends: - S3-compatible object storage - External NFS mounts - Distributed filesystems</p> <p>Compute Integration: - GPU resource scheduling - External compute clusters - Hybrid cloud resources</p>"},{"location":"admin-guide/runbooks/onyxia/#migration-and-backup","title":"Migration and Backup","text":""},{"location":"admin-guide/runbooks/onyxia/#user-data-migration","title":"User Data Migration","text":"<p>Export User Services: <pre><code># Export user configurations\nkubectl get all -n user-{username} -o yaml &gt; user-backup.yaml\n\n# Backup persistent data\nkubectl exec -n user-{username} {pod-name} -- tar czf - /home/user | gzip &gt; user-data.tar.gz\n</code></pre></p> <p>Import User Services: <pre><code># Restore user configurations\nkubectl apply -f user-backup.yaml\n\n# Restore persistent data\nkubectl exec -n user-{username} {pod-name} -- tar xzf - -C /home/user &lt; user-data.tar.gz\n</code></pre></p>"},{"location":"admin-guide/runbooks/onyxia/#platform-migration","title":"Platform Migration","text":"<p>Configuration Backup: - Helm values and configurations - Keycloak realm export - Custom catalog definitions - User quota configurations</p> <p>Data Backup: - Persistent volume snapshots - User workspace backups - Service configuration exports - Access control policies</p>"},{"location":"admin-guide/runbooks/onyxia/#integration-examples","title":"Integration Examples","text":""},{"location":"admin-guide/runbooks/onyxia/#jupyter-notebook-with-slurm","title":"Jupyter Notebook with SLURM","text":"<p>Service Configuration: <pre><code>jupyter:\n  resources:\n    requests:\n      cpu: \"2\"\n      memory: \"4Gi\"\n  persistence:\n    enabled: true\n    size: \"10Gi\"\n  environment:\n    SLURM_ENDPOINT: \"slurmctld.hpc-ubiq.svc.cluster.local\"\n</code></pre></p>"},{"location":"admin-guide/runbooks/onyxia/#rstudio-with-shared-storage","title":"RStudio with Shared Storage","text":"<p>Service Configuration: <pre><code>rstudio:\n  resources:\n    requests:\n      cpu: \"1\"\n      memory: \"2Gi\"\n  persistence:\n    enabled: true\n    size: \"5Gi\"\n  sharedStorage:\n    nfs:\n      server: \"nfs.ubiquitycluster.uk\"\n      path: \"/shared/projects\"\n</code></pre></p>"},{"location":"admin-guide/runbooks/onyxia/#spark-cluster","title":"Spark Cluster","text":"<p>Service Configuration: <pre><code>spark:\n  master:\n    resources:\n      requests:\n        cpu: \"1\"\n        memory: \"2Gi\"\n  worker:\n    replicas: 3\n    resources:\n      requests:\n        cpu: \"2\"\n        memory: \"4Gi\"\n</code></pre></p> <p>This comprehensive documentation provides administrators and users with everything needed to deploy, manage, and troubleshoot Onyxia within the Ubiquity platform, enabling self-service data science and HPC capabilities.</p>"},{"location":"admin-guide/runbooks/osimages/","title":"Operating System Images for Ubiquity","text":""},{"location":"admin-guide/runbooks/osimages/#on-this-page","title":"On This Page","text":"<ul> <li>Overview</li> <li>Scope</li> <li>Prerequisites</li> <li>Ubiquity Image Building</li> <li>Custom Image Creation</li> <li>Bare Metal Deployment</li> <li>Cloud Deployment</li> <li>Advanced Configurations</li> <li>Troubleshooting</li> </ul>"},{"location":"admin-guide/runbooks/osimages/#overview","title":"Overview","text":"<p>This guide provides comprehensive instructions for creating, customizing, and deploying operating system images within the Ubiquity platform. Ubiquity supports multiple deployment scenarios including bare metal provisioning, cloud deployments, and hybrid environments, each with specific image requirements and customization options.</p>"},{"location":"admin-guide/runbooks/osimages/#scope","title":"Scope","text":"<p>This guide covers: - Base OS Images: Rocky Linux, Ubuntu, and Fedora support for Ubiquity clusters - Bare Metal Images: PXE-bootable images for automated provisioning - Cloud Images: Custom images for cloud providers (AWS, Azure, GCP, OpenStack) - Container Images: Base images for HPC workloads and applications - Specialized Images: HPC-optimized images with NVIDIA drivers, InfiniBand support, and performance tools</p>"},{"location":"admin-guide/runbooks/osimages/#prerequisites","title":"Prerequisites","text":""},{"location":"admin-guide/runbooks/osimages/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Build Host: System capable of running diskimage-builder (minimum 4GB RAM, 20GB storage)</li> <li>Target Platform: Bare metal servers or cloud instances for deployment</li> </ul>"},{"location":"admin-guide/runbooks/osimages/#software-requirements","title":"Software Requirements","text":"<ul> <li>Operating System: Rocky Linux 8/9, Ubuntu 20.04/22.04, or Fedora Server</li> <li>Tools: diskimage-builder, qemu-utils, ansible (for bare metal)</li> <li>Access: Administrative privileges on build host</li> </ul>"},{"location":"admin-guide/runbooks/osimages/#abbreviations-and-acronyms","title":"Abbreviations and Acronyms","text":"<ul> <li>BMO: Bare Metal Operator</li> <li>DIB: Disk Image Builder  </li> <li>HPC: High Performance Computing</li> <li>IPA: Ironic Python Agent</li> <li>MLNX_OFED: NVIDIA Mellanox OpenFabrics Enterprise Distribution</li> <li>PXE: Preboot Execution Environment</li> </ul>"},{"location":"admin-guide/runbooks/osimages/#ubiquity-image-building","title":"Ubiquity Image Building","text":""},{"location":"admin-guide/runbooks/osimages/#built-in-image-builder","title":"Built-in Image Builder","text":"<p>Ubiquity includes a comprehensive image building system located in <code>tools/disk-image/mkimage/</code> that provides:</p> <ul> <li>Automated Building: Script-based image creation with minimal configuration</li> <li>Multiple Formats: Support for qcow2, raw, and other disk formats</li> <li>Custom Elements: Pre-built elements for Ubiquity-specific configurations</li> <li>Multi-Architecture: Support for x86_64 and ARM64 architectures</li> </ul>"},{"location":"admin-guide/runbooks/osimages/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Rocky Linux (Recommended) - Rocky Linux 8.x and 9.x - Default choice for Ubiquity deployments - Optimized kickstart configurations for bare metal - Full HPC stack support</p> <p>Ubuntu Server - Ubuntu 20.04 LTS and 22.04 LTS - Cloud-optimized images - Extensive package ecosystem</p> <p>Fedora Server - Latest stable releases - Cutting-edge kernel features - Development and testing environments</p>"},{"location":"admin-guide/runbooks/osimages/#quick-start","title":"Quick Start","text":"<pre><code># Navigate to image builder\ncd tools/disk-image/mkimage\n\n# Prepare build environment\n./prep.sh\n\n# Build all images\n./build-images.sh\n\n# Build specific images\nimage_filter=\"rocky\" ./build-images.sh\n\n# Specify output format\noutput_type=\"qcow2,raw\" ./build-images.sh\n</code></pre>"},{"location":"admin-guide/runbooks/osimages/#available-custom-elements","title":"Available Custom Elements","text":"<p>Ubiquity Element (<code>custom-elements/ubiquity/</code>) - Core Ubiquity platform integration - Kubernetes node preparation - Longhorn storage optimization</p> <p>MOFED Element (<code>custom-elements/mofed/</code>) - NVIDIA MLNX_OFED network drivers - InfiniBand support for HPC workloads - High-speed interconnect optimization</p> <p>Cloud-init Element (<code>custom-elements/cloud-init-install/</code>) - Cloud environment compatibility - Automated system configuration - User and SSH key management</p> <p>Custom Base Element (<code>custom-elements/custom-base/</code>) - Common configurations across all images - Package installations and system tuning - Security hardening</p>"},{"location":"admin-guide/runbooks/osimages/#custom-image-creation","title":"Custom Image Creation","text":""},{"location":"admin-guide/runbooks/osimages/#setting-up-build-environment","title":"Setting Up Build Environment","text":"<p>Rocky Linux Build Host: <pre><code># Install dependencies\nsudo dnf install -y qemu-img python3-pip git\nsudo pip3 install diskimage-builder\n\n# Clone Ubiquity repository\ngit clone https://github.com/ubiquitycluster/ubiquity.git\ncd ubiquity-open/tools/disk-image/mkimage\n\n# Initialize build environment\n./prep.sh\n</code></pre></p> <p>Ubuntu Build Host: <pre><code># Install dependencies\nsudo apt update\nsudo apt install -y qemu-utils python3-pip git\nsudo pip3 install diskimage-builder\n\n# Setup Ubiquity build environment\ngit clone https://github.com/ubiquitycluster/ubiquity.git\ncd ubiquity-open/tools/disk-image/mkimage\n./prep.sh\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#creating-ubiquity-optimized-images","title":"Creating Ubiquity-Optimized Images","text":"<p>Basic Ubiquity Node Image: <pre><code>export ELEMENTS_PATH=\"custom-elements:elements\"\nexport DIB_RELEASE=\"9\"  # Rocky 9\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  cloud-init-install \\\n  -o ubiquity-node-rocky9\n</code></pre></p> <p>HPC-Optimized Image with InfiniBand: <pre><code>export ELEMENTS_PATH=\"custom-elements:elements\"\nexport DIB_RELEASE=\"9\"\nexport DIB_MOFED_FILE=\"/tmp/MLNX_OFED_LINUX-5.8-1.0.1.1-rhel8.7-x86_64.iso\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  mofed \\\n  cloud-init-install \\\n  -o ubiquity-hpc-rocky9\n</code></pre></p> <p>GPU-Enabled Compute Image: <pre><code>export ELEMENTS_PATH=\"custom-elements:elements\"\nexport DIB_RELEASE=\"9\"\nexport DIB_CUDA_URL=\"https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  nvidia-cuda \\\n  cloud-init-install \\\n  -o ubiquity-gpu-rocky9\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#bare-metal-deployment","title":"Bare Metal Deployment","text":""},{"location":"admin-guide/runbooks/osimages/#kickstart-based-provisioning","title":"Kickstart-Based Provisioning","text":"<p>Ubiquity's bare metal deployment uses automated kickstart installations optimized for HPC environments:</p> <p>Key Features: - Automated Partitioning: Intelligent disk layout based on available storage - LVM Configuration: Optimized for Kubernetes and Longhorn storage - Network Configuration: Support for bonding, VLANs, and complex topologies - Security: Automated SSH key deployment and system hardening</p> <p>Partition Layout (Rocky Linux): <pre><code># System Volume Group (Smallest disk)\n/boot/efi     512MB   vfat\n/boot         2GB     ext4\n/             20%     ext4  (System VG)\n/tmp          5%      ext4  (System VG)\n/var/log      2%      ext4  (System VG)\n/var/crash    10%     ext4  (System VG)\n/var/lib/rancher 10%  ext4  (System VG)\n/home         53%     ext4  (System VG)\n\n# Data Volume Group (Larger disks)\n/var/lib/kubelet    1%   ext4  (Data VG)\n/var/lib/longhorn   60%  ext4  (Data VG)\n/home              39%   ext4  (Data VG)\n</code></pre></p> <p>Network Configuration Examples:</p> <p>Single Interface: <pre><code>network_interfaces:\n  - name: eno1\n    device: eno1\n    ip: 192.168.1.100\n    netmask: 255.255.255.0\n    gateway: 192.168.1.1\n    nameserver: 192.168.1.1\n</code></pre></p> <p>Bonded Interface: <pre><code>network_interfaces:\n  - name: bond0\n    device: bond0\n    slaves: eno1,eno2\n    bond_opts: \"mode=802.3ad miimon=100\"\n    ip: 192.168.1.100\n    netmask: 255.255.255.0\n    gateway: 192.168.1.1\n</code></pre></p> <p>VLAN Configuration: <pre><code>network_interfaces:\n  - name: vlan100\n    device: eno1\n    vlanid: 100\n    ip: 10.0.100.100\n    netmask: 255.255.255.0\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#pxe-boot-process","title":"PXE Boot Process","text":"<p>Ubiquity implements a comprehensive PXE boot system for bare metal provisioning:</p> <pre><code>flowchart TD\n    A[Bare Metal Node] --&gt; B[PXE Boot Request]\n    B --&gt; C[DHCP Server Response]\n    C --&gt; D[TFTP Bootloader Download]\n    D --&gt; E[HTTP Kickstart Download]\n    E --&gt; F[OS Installation]\n    F --&gt; G[Automated Configuration]\n    G --&gt; H[Kubernetes Join]</code></pre> <p>DHCP Configuration: <pre><code># Node receives IP, boot server, and bootloader info\nsubnet 192.168.1.0 netmask 255.255.255.0 {\n    range 192.168.1.100 192.168.1.200;\n    option domain-name-servers 192.168.1.1;\n    option domain-name \"ubiquitycluster.local\";\n    option routers 192.168.1.1;\n    filename \"pxelinux.0\";\n    next-server 192.168.1.10;\n}\n</code></pre></p> <p>Boot Configuration: <pre><code># PXE menu entry for Rocky Linux installation\nLABEL rocky9-install\n    MENU LABEL Install Rocky Linux 9 (Ubiquity)\n    KERNEL vmlinuz-rocky9\n    APPEND initrd=initrd-rocky9.img ks=http://192.168.1.10/kickstart/node01.ks\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#cloud-deployment","title":"Cloud Deployment","text":""},{"location":"admin-guide/runbooks/osimages/#cloud-provider-images","title":"Cloud Provider Images","text":"<p>AWS AMI Creation: <pre><code># Create AMI-compatible image\nexport AWS_DEFAULT_REGION=\"us-west-2\"\nexport DIB_RELEASE=\"9\"\n\ndisk-image-create \\\n  vm \\\n  cloud-init-datasources \\\n  growroot \\\n  install-static \\\n  rocky-container \\\n  ubiquity \\\n  -t ami \\\n  -o ubiquity-aws-rocky9\n</code></pre></p> <p>Azure VHD Creation: <pre><code># Create Azure-compatible VHD\nexport DIB_RELEASE=\"9\"\n\ndisk-image-create \\\n  vm \\\n  azure \\\n  cloud-init-datasources \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  -t vhd \\\n  -o ubiquity-azure-rocky9\n</code></pre></p> <p>GCP Image Creation: <pre><code># Create GCP-compatible image\nexport DIB_RELEASE=\"9\"\n\ndisk-image-create \\\n  vm \\\n  google \\\n  cloud-init-datasources \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  -o ubiquity-gcp-rocky9\n</code></pre></p> <p>OpenStack Image Creation: <pre><code># Create OpenStack-compatible qcow2 image\nexport DIB_RELEASE=\"9\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  -o ubiquity-openstack-rocky9\n\n# Upload to OpenStack\nopenstack image create \\\n  --public \\\n  --disk-format qcow2 \\\n  --container-format bare \\\n  --file ubiquity-openstack-rocky9.qcow2 \\\n  \"Ubiquity Rocky 9\"\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#cloud-init-configuration","title":"Cloud-init Configuration","text":"<p>Ubiquity images include comprehensive cloud-init support:</p> <p>User Configuration: <pre><code>#cloud-config\nusers:\n  - name: admin\n    groups: wheel\n    shell: /bin/bash\n    sudo: ['ALL=(ALL) NOPASSWD:ALL']\n    ssh-authorized-keys:\n      - ssh-rsa AAAAB3NzaC1yc2E... admin@ubiquity\n</code></pre></p> <p>Package Installation: <pre><code>packages:\n  - kubectl\n  - docker\n  - git\n  - htop\n  - iotop\n  - nfs-utils\n</code></pre></p> <p>Service Configuration: <pre><code>runcmd:\n  - systemctl enable --now docker\n  - systemctl enable --now kubelet\n  -  /opt/ubiquity/setup-node.sh\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#advanced-configurations","title":"Advanced Configurations","text":""},{"location":"admin-guide/runbooks/osimages/#hpc-optimized-images","title":"HPC-Optimized Images","text":"<p>InfiniBand and RDMA Support: <pre><code># Enable RDMA networking for HPC workloads\nexport DIB_RELEASE=\"9\"\nexport DIB_MOFED_FILE=\"/path/to/MLNX_OFED_LINUX-5.8-1.0.1.1-rhel9.0-x86_64.iso\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  mofed \\\n  cloud-init-install \\\n  -o ubiquity-hpc-ib-rocky9\n</code></pre></p> <p>Multi-GPU Configurations: <pre><code># Support for multiple GPU configurations\nexport DIB_RELEASE=\"9\"\nexport DIB_CUDA_URL=\"https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run\"\nexport DIB_GPU_DRIVER_VERSION=\"535.54.03\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  nvidia-cuda \\\n  nvidia-fabric-manager \\\n  cloud-init-install \\\n  -o ubiquity-multi-gpu-rocky9\n</code></pre></p> <p>Container Runtime Integration: <pre><code># Docker and containerd optimizations for HPC\nexport DIB_RELEASE=\"9\"\nexport DIB_CONTAINER_RUNTIME=\"containerd\"\nexport DIB_DOCKER_STORAGE_DRIVER=\"overlay2\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  container-runtime \\\n  nvidia-container-toolkit \\\n  cloud-init-install \\\n  -o ubiquity-container-rocky9\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#storage-optimizations","title":"Storage Optimizations","text":"<p>Longhorn Storage Preparation: <pre><code># Optimized for Longhorn distributed storage\nexport DIB_RELEASE=\"9\"\nexport DIB_LONGHORN_VERSION=\"1.5.1\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  longhorn-prep \\\n  cloud-init-install \\\n  -o ubiquity-storage-rocky9\n</code></pre></p> <p>NFS and Distributed Storage: <pre><code># Support for NFS and distributed filesystems\nexport DIB_RELEASE=\"9\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  nfs-client \\\n  cvmfs-client \\\n  cloud-init-install \\\n  -o ubiquity-nfs-rocky9\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#networking-configurations","title":"Networking Configurations","text":"<p>SR-IOV and Hardware Acceleration: <pre><code># Support for SR-IOV and hardware offloading\nexport DIB_RELEASE=\"9\"\nexport DIB_SRIOV_DRIVERS=\"i40e,ixgbe,mlx5_core\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  sriov-support \\\n  dpdk-support \\\n  cloud-init-install \\\n  -o ubiquity-sriov-rocky9\n</code></pre></p> <p>Advanced Security Features: <pre><code># Security hardening and compliance\nexport DIB_RELEASE=\"9\"\nexport DIB_SECURITY_PROFILE=\"hardened\"\n\ndisk-image-create \\\n  vm \\\n  dhcp-all-interfaces \\\n  cloud-init-datasources \\\n  dracut-regenerate \\\n  growroot \\\n  rocky-container \\\n  ubiquity \\\n  security-hardening \\\n  audit-logging \\\n  cloud-init-install \\\n  -o ubiquity-secure-rocky9\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#performance-tuning","title":"Performance Tuning","text":"<p>Kernel and System Optimizations: <pre><code># Custom kernel parameters for HPC workloads\nkernel_parameters:\n  - \"intel_iommu=on\"\n  - \"iommu=pt\"\n  - \"hugepagesz=1G\"\n  - \"hugepages=32\"\n  - \"default_hugepagesz=1G\"\n  - \"isolcpus=1-15,17-31\"\n  - \"rcu_nocbs=1-15,17-31\"\n  - \"nohz_full=1-15,17-31\"\n</code></pre></p> <p>CPU Tuning: <pre><code># CPU governor and frequency scaling\necho \"performance\" &gt; /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n\n# CPU isolation for real-time workloads\necho \"1-15,17-31\" &gt; /sys/devices/system/cpu/isolated\n</code></pre></p> <p>Memory Optimizations: <pre><code># Huge page configuration\necho 1024 &gt; /proc/sys/vm/nr_hugepages\necho \"vm.nr_hugepages = 1024\" &gt;&gt; /etc/sysctl.conf\n\n# NUMA policy optimization\necho \"interleave\" &gt; /proc/sys/kernel/numa_balancing\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/runbooks/osimages/#build-issues","title":"Build Issues","text":"<p>Common Build Failures:</p> <p>Insufficient Disk Space: <pre><code># Check available space\ndf -h /tmp\n\n# Clean up previous builds\nrm -rf /tmp/image.* ~/.cache/diskimage-builder/\n\n# Use alternative temp directory\nexport TMP_DIR=\"/var/tmp\"\nexport DIB_IMAGE_CACHE=\"/var/cache/diskimage-builder\"\n</code></pre></p> <p>Network Connectivity Issues: <pre><code># Verify repository access\ncurl -I http://mirror.centos.org/centos/\n\n# Configure proxy if needed\nexport HTTP_PROXY=\"http://proxy.example.com:8080\"\nexport HTTPS_PROXY=\"https://proxy.example.com:8080\"\n</code></pre></p> <p>Element Dependencies: <pre><code># Verify custom elements path\nls -la custom-elements/\n\n# Check element permissions\nfind custom-elements/ -name \"*.sh\" -exec chmod +x {} \\;\n\n# Debug element execution\nexport DIB_DEBUG_TRACE=1\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#deployment-issues","title":"Deployment Issues","text":"<p>PXE Boot Problems:</p> <p>DHCP Configuration: <pre><code># Check DHCP server status\nsystemctl status dhcpd\n\n# Verify lease file\ntail -f /var/lib/dhcpd/dhcpd.leases\n\n# Test DHCP response\ndhcping -s 192.168.1.10 -c 192.168.1.100\n</code></pre></p> <p>TFTP Issues: <pre><code># Verify TFTP service\nsystemctl status tftp\n\n# Test TFTP connectivity\ntftp 192.168.1.10 -c get pxelinux.0\n\n# Check file permissions\nls -la /var/lib/tftpboot/\n</code></pre></p> <p>Kickstart Problems: <pre><code># Validate kickstart syntax\nksvalidator /var/www/html/kickstart/node01.ks\n\n# Check HTTP access\ncurl -I http://192.168.1.10/kickstart/node01.ks\n\n# Monitor installation logs\ntail -f /var/log/httpd/access_log\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#performance-issues","title":"Performance Issues","text":"<p>Image Size Optimization: <pre><code># Remove unnecessary packages\nexport DIB_MINIMAL_IMAGE=1\n\n# Clean package cache\nexport DIB_APT_CLEAN=1\nexport DIB_YUM_CLEAN=1\n\n# Compress images\nqemu-img convert -c -O qcow2 input.qcow2 output-compressed.qcow2\n</code></pre></p> <p>Boot Time Optimization: <pre><code># Reduce systemd timeout\nsed -i 's/#DefaultTimeoutStartSec=90s/DefaultTimeoutStartSec=30s/' /etc/systemd/system.conf\n\n# Disable unnecessary services\nsystemctl disable NetworkManager-wait-online\nsystemctl disable plymouth-start\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>Build Process Monitoring: <pre><code># Monitor build progress\ntail -f /tmp/dib-build-*.log\n\n# Check system resources during build\nwatch -n 5 'df -h &amp;&amp; free -h &amp;&amp; ps aux | grep disk-image'\n</code></pre></p> <p>Runtime Diagnostics: <pre><code># Check node health\nkubectl get nodes -o wide\n\n# Monitor system metrics\ntop\niostat -x 1\nsar -u 1 5\n\n# Network diagnostics\nss -tuln\nnetstat -i\n</code></pre></p> <p>Log Analysis: <pre><code># System logs\njournalctl -f -u kubelet\njournalctl -f -u docker\n\n# Kubernetes logs\nkubectl logs -n kube-system -l k8s-app=kubelet\n\n# Storage logs\nkubectl logs -n longhorn-system -l app=longhorn-manager\n</code></pre></p>"},{"location":"admin-guide/runbooks/osimages/#recovery-procedures","title":"Recovery Procedures","text":"<p>Failed Deployments: <pre><code># Reset failed node\nansible-playbook -i inventories/production clean.yml --limit failed_node\n\n# Reinstall from PXE\nipmitool -I lanplus -H &lt;bmc_ip&gt; -U admin -P password chassis bootdev pxe\nipmitool -I lanplus -H &lt;bmc_ip&gt; -U admin -P password power reset\n</code></pre></p> <p>Image Corruption: <pre><code># Verify image integrity\nqemu-img check ubiquity-node-rocky9.qcow2\n\n# Repair corrupted image\nqemu-img convert -f qcow2 -O qcow2 corrupted.qcow2 repaired.qcow2\n\n# Rebuild if necessary\nrm -f corrupted.qcow2\n./build-images.sh\n</code></pre></p> <p>Configuration Recovery: <pre><code># Backup configurations\ncp -r custom-elements/ custom-elements.backup.$(date +%Y%m%d)\n\n# Restore from version control\ngit checkout -- custom-elements/\n\n# Reset to working configuration\ngit reset --hard &lt;working_commit_hash&gt;\n</code></pre></p>"},{"location":"admin-guide/runbooks/vault/","title":"Vault","text":"<p>TODO</p>"},{"location":"admin-guide/runbooks/awx/backup/","title":"Backup","text":""},{"location":"admin-guide/runbooks/awx/backup/#back-up-awx-using-awx-operator","title":"Back up AWX using AWX Operator","text":"<p>The AWX Operator has the ability to back up AWX in an easy way - Because... We're good to you!</p> <p>This guide is specifically designed to use with AWX deployed using ubiquity, inside the platform folder.</p> <p>You can also refer to the official instructions for more information.</p>"},{"location":"admin-guide/runbooks/awx/backup/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Instruction</li> <li>Prepare for Backup</li> <li>Back up AWX manually</li> <li>Appendix: Back up AWX using Ansible</li> </ul>"},{"location":"admin-guide/runbooks/awx/backup/#instruction","title":"Instruction","text":""},{"location":"admin-guide/runbooks/awx/backup/#prepare-for-backup","title":"Prepare for Backup","text":"<p>Prepare directories for Persistent Volumes to store backup files that defined in <code>backup/pv.yaml</code>. This guide use the <code>hostPath</code> based PV to make it easy to understand, but you can just as easily back up to anywhere.</p> <pre><code>sudo mkdir -p /data/backup\n</code></pre> <p>Then deploy Persistent Volume and Persistent Volume Claim.</p> <pre><code>kubectl apply -k backup\n</code></pre>"},{"location":"admin-guide/runbooks/awx/backup/#back-up-awx-manually","title":"Back up AWX manually","text":"<p>Modify the name of the AWXBackup object in <code>backup/awxbackup.yaml</code>.</p> <pre><code>...\nkind: AWXBackup\nmetadata:\n  name: awxbackup-2021-06-06     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  namespace: awx\n...\n</code></pre> <p>Then invoke backup by applying this manifest file.</p> <pre><code>kubectl apply -f backup/awxbackup.yaml\n</code></pre> <p>To monitor the progress of the deployment, check the logs of <code>deployments/awx-operator-controller-manager</code>:</p> <pre><code>kubectl -n awx logs -f deployments/awx-operator-controller-manager\n</code></pre> <p>When the backup completes successfully, the logs end with:</p> <pre><code>$ kubectl -n awx logs -f deployments/awx-operator-controller-manager\n...\n----- Ansible Task Status Event StdOut (awx.ansible.com/v1beta1, Kind=AWXBackup, awxbackup-2021-06-06/awx) -----\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=7    changed=0    unreachable=0    failed=0    skipped=9    rescued=0    ignored=0\n</code></pre> <p>This will create AWXBackup object in the namespace and also create backup files in the Persistent Volume. In this example those files are available at <code>/data/backup</code>.</p> <pre><code>$ kubectl -n awx get awxbackup\nNAME                   AGE\nawxbackup-2021-06-06   6m47s\n</code></pre> <pre><code>$ ls -l /data/backup/\ntotal 0\ndrwxr-xr-x. 2 root root 59 Jun  5 06:51 tower-openshift-backup-2021-06-06-10:51:49\n\n$ ls -l /data/backup/tower-openshift-backup-2021-06-06-10\\:51\\:49/\ntotal 736\n-rw-------. 1 1001 root   1093 Jun  6 06:51 awx_object\n-rw-------. 1 1001 root  17085 Jun  6 06:51 secrets.yml\n-rw-rw----. 1 root root 833184 Jun  6 06:51 tower.db\n</code></pre>"},{"location":"admin-guide/runbooks/awx/backup/#appendix-back-up-awx-using-ansible","title":"Appendix: Back up AWX using Ansible","text":"<p>An example simple playbook for Ansible is also provided in this repository. This can be used with <code>ansible-playbook</code>, <code>ansible-runner</code>, and AWX. It can be also used with the scheduling feature on AWX too.</p> <p>Refer \ud83d\udcc1 Appendix: Back up AWX using Ansible for details.</p>"},{"location":"admin-guide/runbooks/awx/backup/ansible/","title":"Index","text":""},{"location":"admin-guide/runbooks/awx/backup/ansible/#appendix-back-up-awx-using-ansible","title":"Appendix: Back up AWX using Ansible","text":"<p>An example simple playbook for Ansible is also provided in this repository. This can be used with <code>ansible-playbook</code>, <code>ansible-runner</code>, and AWX. It can be also used with the scheduling feature on AWX too.</p>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Requirements</li> <li>Variables</li> <li>Preparation</li> <li>Prepare Service Account and API Token</li> <li>Prepare Backup Storage</li> <li>Use with Ansible</li> <li>Use with Ansible Runner</li> <li>Use with AWX</li> </ul>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#requirements","title":"Requirements","text":"<ul> <li>Ansible collections</li> <li><code>kubernetes.core</code></li> <li>Pip modules</li> <li>Refer the <code>kubernetes.core.k8s</code> module documentation</li> </ul>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#variables","title":"Variables","text":"<p>This example playbook is designed to allow you to customize your backup with variables.</p> Variables Description Default <code>awxbackup_namespace</code> The name of the NameSpace where the <code>AWXBackup</code> resource will be created. <code>awx</code> <code>awxbackup_name</code> The name of the <code>AWXBackup</code> resource. Dynamically generated using execution time by default. <code>awxbackup-{{ lookup('pipe', 'date +%Y-%m-%d-%H-%M-%S') }}</code> <code>awxbackup_spec</code> The <code>spec</code> of the <code>AWXBackup</code> resource. Refer official documentation for acceptable fields. <code>deployment_name: awx</code><code>backup_pvc: awx-backup-claim</code><code>clean_backup_on_delete: true</code> <code>awxbackup_timeout</code> Time to wait for backup to complete, in seconds. If exceeded, the playbook will fail. <code>600</code> <code>awxbackup_keep_days</code> Number of days to keep <code>AWXBackup</code> resources. <code>AWXBackup</code> resources older than this value will be deleted by this playbook. Set <code>0</code> to keep forever. <code>30</code> <p>Note that this playbook enables <code>clean_backup_on_delete</code> by default that only works with AWX Operator <code>0.24.0</code> and later. This option makes that your actual backup data in your PVC is deleted at the same time the AWXBackup resource is deleted. You can disable this feature by explicitly specifying <code>clean_backup_on_delete: false</code>. Refer the official documentation for detail.</p>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#preparation","title":"Preparation","text":""},{"location":"admin-guide/runbooks/awx/backup/ansible/#prepare-service-account-and-api-token","title":"Prepare Service Account and API Token","text":"<p>Create a Service Account, Role, and RoleBinding to manage the <code>AWXBackup</code> resource.</p> <pre><code># Specify NameSpace where your AWXBackup resources will be created.\n$ NAMESPACE=awx\n$ kubectl -n ${NAMESPACE} apply -f rbac/sa.yaml\nserviceaccount/awx-backup created\nrole.rbac.authorization.k8s.io/awx-backup created\nrolebinding.rbac.authorization.k8s.io/awx-backup created\n</code></pre>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#prepare-backup-storage","title":"Prepare Backup Storage","text":"<p>Since you have complete control over <code>spec</code> of <code>AWXBackup</code> via <code>awxbackup_spec</code> variables, whether or not this step is required depends on your environment. Check the official documentation and prepare as needed.</p> <p>If your AWX was deployed by using Ubiquity, preparing backup storage by following the basic backup guide is a good starting point.</p>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#use-with-ansible","title":"Use with Ansible","text":"<p>Export required environment variables.</p> <pre><code>export K8S_AUTH_HOST=\"https://&lt;Your K3s Host&gt;:6443/\"\nexport K8S_AUTH_VERIFY_SSL=no\n</code></pre> <p>Obtain and export the API Token which required to authenticate the Kubernetes API.</p> <pre><code># For Kubernetes 1.24 or later:\n# This will generate new token that valid for 1 hours.\n# Of course you can modify the value since \"1h\" is just an example.\n$ export K8S_AUTH_API_KEY=$(kubectl -n awx create token awx-backup --duration=1h)\n\n# For Kubernetes 1.23 or earlier:\n# Obtain and decode token from Secret that automatically generated for the Service Account.\n$ SECRET=$(kubectl -n ${NAMESPACE} get sa awx-backup -o jsonpath='{.secrets[0].name}')\n$ export K8S_AUTH_API_KEY=$(kubectl -n ${NAMESPACE} get secret ${SECRET} -o jsonpath='{.data.token}' | base64 -d)\n</code></pre> <pre><code># Modify variables using \"-e\" as needed\nansible-playbook project/backup.yml \\\n  -e awxbackup_spec=\"{'deployment_name':'awx','backup_pvc':'awx-backup-claim','clean_backup_on_delete':'true'}\" \\\n  -e keep_days=90\n</code></pre>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#use-with-ansible-runner","title":"Use with Ansible Runner","text":"<p>Refer the guide for Ansible Runner for the basic usage.</p> <p>Modify following files as needed. Note that the EE <code>quay.io/ansible/awx-ee:latest</code> contains required modules and collections by default.</p> <ul> <li>\ud83d\udcdd<code>env/settings</code>: Configure your Execution Environment</li> <li>\ud83d\udcdd<code>env/envvars</code>: Specify your K3s host and API Token</li> <li>\ud83d\udcdd<code>env/extravars</code>: Modify variables</li> </ul> <p>Then execute Ansible Runner.</p> <pre><code>ansible-runner run . -p backup.yml\n</code></pre>"},{"location":"admin-guide/runbooks/awx/backup/ansible/#use-with-awx","title":"Use with AWX","text":"<p>This playbook can also be run through Job Templates on AWX. Of course Schedules can be set up in the Job Template to obtain periodic backup.</p> <p>This is the way to make the backup of the AWX itself where the Job Template for the backup is configured.</p> <p>In this case, the PostgreSQL will be dumped while the job is running, so complete logs of the job itself is not part of the backup. Therefore, after restoration, the last backup job will be shown as failed since the AWX can't determine the result of the job, but this can be safely ignored.</p> <ol> <li>Add new Container Group to make the API token usable inside the EE.</li> <li> <p>Enable <code>Customize pod specification</code> and put following YAML string. <code>serviceAccountName</code> and <code>automountServiceAccountToken</code> are important to make the API token usable inside the EE.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: awx\nspec:\n  serviceAccountName: awx-backup\n  automountServiceAccountToken: true\n  containers:\n    - image: quay.io/ansible/awx-ee:latest\n      name: worker\n      args:\n        - ansible-runner\n        - worker\n        - '--private-data-dir=/runner'\n      resources:\n        requests:\n          cpu: 250m\n          memory: 100Mi\n</code></pre> </li> <li> <p>Add new Project including the playbook.</p> </li> <li>You can specify this repository (your git repo) directly, but use with caution. Things could change!</li> <li>Add new Job Template which use the playbook.</li> <li>Select appropriate <code>Inventory</code>. The bundled <code>Demo Inventory</code> is enough to use. If you specify your own inventory, ensure <code>localhost</code> is defined in the inventory and following variables are enabled for <code>localhost</code>.<ul> <li><code>ansible_connection: local</code></li> <li><code>ansible_python_interpreter: '{{ ansible_playbook_python }}'</code></li> </ul> </li> <li>Select appropriate <code>Execution Environment</code>. The default <code>AWX EE (latest)</code> (<code>quay.io/ansible/awx-ee:latest</code>) contains required collections and modules by default, so it's good for the first choice.</li> <li>Select your <code>backup.yml</code> as <code>Playbook</code>.</li> <li>Specify <code>Variables</code> as needed.</li> <li>Select your Container Group created in the above step as <code>Instance Group</code>.</li> <li>(Optional) Add new Schedules for periodic backup.</li> </ol> <p>If you want to make the backup of another AWX on different namespace of different cluster, create new Credential with <code>OpenShift or Kubernetes API Bearer Token</code> type instead of Container Group and then specify the Credential in the Job Template. To obtain the API token, refer \"Use with Ansible\" section.</p>"},{"location":"admin-guide/runbooks/awx/git/","title":"Git Integration","text":""},{"location":"admin-guide/runbooks/awx/git/#deploy-private-git-repository-using-gitea","title":"Deploy Private Git Repository using Gitea","text":"<p>You can deploy your own private Git repository using Gitea to use AWX with the playbooks on SCM.</p> <p>However, it just so happens... We've already done that for you by-default in Ubiquity! Aren't we good to you!</p>"},{"location":"admin-guide/runbooks/awx/git/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Configure AWX to use Git Repository with Self-Signed Certificate</li> </ul>"},{"location":"admin-guide/runbooks/awx/git/#configure-awx-to-use-git-repository-with-self-signed-certificate","title":"Configure AWX to use Git Repository with Self-Signed Certificate","text":"<ol> <li>Add Credentials for SCM</li> <li>Allow Self-Signed Certificate such as this Gitea</li> <li>Open <code>Settings</code> &gt; <code>Jobs settings</code> in AWX</li> <li> <p>Press <code>Edit</code> and scroll down to <code>Extra Environment Variables</code>, then add <code>\"GIT_SSL_NO_VERIFY\": \"True\"</code> in <code>{}</code></p> <pre><code>{\n  \"GIT_SSL_NO_VERIFY\": \"True\"\n}\n</code></pre> </li> <li> <p>Press <code>Save</code></p> </li> </ol>"},{"location":"admin-guide/runbooks/awx/restore/","title":"Restore","text":""},{"location":"admin-guide/runbooks/awx/restore/#restore-awx-using-awx-operator","title":"Restore AWX using AWX Operator","text":"<p>The AWX Operator has the ability to restore AWX in an easy way.</p> <p>This guide is specifically designed to use with the AWX which deployed using Ubiquity from the platform folder.</p> <p>You can also refer to the official instructions for more information.</p>"},{"location":"admin-guide/runbooks/awx/restore/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Instruction</li> <li>Prepare for Restore</li> <li>Restore Manually</li> </ul>"},{"location":"admin-guide/runbooks/awx/restore/#instruction","title":"Instruction","text":"<p>To perform restoration, you need to have AWX Operator running on Kubernetes. If you are planning to restore to a new environment, first prepare Kubernetes and AWX Operator via Ubiquity's ArgoCD environment.</p> <p>It is strongly recommended that the version of AWX Operator is the same as the version when the backup was taken. This is because the structure of the backup files differs between versions and may not be compatible. If you have upgraded AWX Operator after taking the backup, it is recommended to downgrade AWX Operator first before performing the restore. To deploy <code>0.13.0</code> or earlier version of AWX Operator, refer \ud83d\udcddTips: Deploy older version of AWX Operator</p>"},{"location":"admin-guide/runbooks/awx/restore/#prepare-for-restore","title":"Prepare for Restore","text":"<p>If your AWX instance is running, it is recommended that it be deleted along with PVC and PV for the PostgreSQL first, in order to restore to be succeeded.</p> <pre><code># Delete AWX resource, PVC, and PV\nkubectl -n awx delete awx awx\nkubectl -n awx delete pvc postgres-13-awx-postgres-13-0\nkubectl delete pv awx-postgres-13-volume\n\n# Delete any data in the PV\nsudo rm -rf /data/postgres-13\n</code></pre> <p>Then prepare directories for your PVs. <code>/data/projects</code> is required if you are restoring the entire AWX to a new environment.</p> <pre><code>sudo mkdir -p /data/postgres-13\nsudo mkdir -p /data/projects\nsudo chmod 755 /data/postgres-13\nsudo chown 1000:0 /data/projects\n</code></pre> <p>Then deploy PV and PVC. It is recommended that making the size of PVs and PVCs same as the PVs which your AWX used when the backup was taken.</p> <pre><code>kubectl apply -k restore\n</code></pre>"},{"location":"admin-guide/runbooks/awx/restore/#restore-manually","title":"Restore Manually","text":"<p>Modify the name of the AWXRestore object in <code>restore/awxrestore.yaml</code>.</p> <pre><code>...\nkind: AWXRestore\nmetadata:\n  name: awxrestore-2021-06-06     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  namespace: awx\n...\n</code></pre> <p>If you want to restore from AWXBackup object, specify its name in <code>restore/awxrestore.yaml</code>.</p> <pre><code>...\n  # Parameters to restore from AWXBackup object\n  backup_name: awxbackup-2021-06-06     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n...\n</code></pre> <p>If the AWXBackup object no longer exists, place the backup files and specify the name of the PVC and directory in <code>restore/awxrestore.yaml</code>.</p> <pre><code>...\n  # Parameters to restore from existing files on PVC (without AWXBackup object)\n  backup_pvc: awx-backup-claim     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  backup_dir: /backups/tower-openshift-backup-2021-06-06-10:51:49     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n...\n</code></pre> <p>Then invoke restore by applying this manifest file.</p> <pre><code>kubectl apply -f restore/awxrestore.yaml\n</code></pre> <p>To monitor the progress of the deployment, check the logs of <code>deployments/awx-operator-controller-manager</code>:</p> <pre><code>kubectl -n awx logs -f deployments/awx-operator-controller-manager\n</code></pre> <p>When the restore complete successfully, the logs end with:</p> <pre><code>$ kubectl -n awx logs -f deployments/awx-operator-controller-manager\n...\n----- Ansible Task Status Event StdOut (awx.ansible.com/v1beta1, Kind=AWX, awx/awx) -----\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=82   changed=0    unreachable=0    failed=0    skipped=76   rescued=0    ignored=1\n</code></pre> <p>This will create AWXRestore object in the namespace, and now your AWX is restored.</p> <pre><code>$ kubectl -n awx get awxrestore\nNAME                    AGE\nawxrestore-2021-06-06   137m\n</code></pre>"},{"location":"admin-guide/runbooks/awx/runner/","title":"Examples of Ansible Runner","text":"<p>This repository includes ready-to-use files as an example to run Ansible Runner.</p> <ul> <li>ansible/ansible-runner</li> <li>Ansible Runner \u2014 ansible-runner documentation</li> </ul>"},{"location":"admin-guide/runbooks/awx/runner/#environment-in-this-example","title":"Environment in This Example","text":"<ul> <li>CentOS Stream 8 (Minimal)</li> <li>Python 3.9</li> <li>Docker 20.10.17</li> <li>Ansible Runner 2.3.2</li> </ul>"},{"location":"admin-guide/runbooks/awx/runner/#install","title":"Install","text":"<pre><code>python3 -m pip install ansible-runner\n</code></pre>"},{"location":"admin-guide/runbooks/awx/runner/#prepare-required-files","title":"Prepare Required Files","text":"<p>Refer to <code>projects/demo.yml</code> and <code>env/settings</code> to review existing playbook and configuration.</p>"},{"location":"admin-guide/runbooks/awx/runner/#invoke-ansible-runner","title":"Invoke Ansible Runner","text":"<p>The sample playbook in this repository will help you figure out the differences between Execution Environments, such as Ansible version, <code>pip list</code>, etc.</p> <pre><code>cd runner\nansible-runner run . -p demo.yml\n</code></pre> <p>The settings can also be given directly as arguments like <code>--container-image</code> without using <code>env/settings</code>.</p>"},{"location":"admin-guide/runbooks/awx/runner/#tips","title":"Tips","text":"<ul> <li>If <code>process_isolation</code> is set to <code>false</code> as default, <code>ansible-runner</code> will just work as a wrapper for the local <code>ansible-playbook</code> command.</li> <li>If <code>process_isolation</code> is set to <code>true</code>, <code>connection: local</code> points to the container itself, so the playbook cannot affect <code>localhost</code> which means the container host. To avoid this, you need to SSH to local host without using <code>connection: local</code>. Related issue for this is here.</li> <li><code>container_image</code> defaults to <code>quay.io/ansible/ansible-runner:devel</code>. You can check available tags for the public Execution Environment at <code>quay.io/ansible/ansible-runner</code></li> <li>The image from <code>quay.io/ansible/awx-ee</code> which is default Execution Environment for AWX also can be specified.</li> <li>The <code>process_isolation_show_paths</code> described in the documentation does not work with Docker and Podman. Instead, you can use <code>container_volume_mounts</code> as described in <code>env/settings</code>, but this is not documented.</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/","title":"Tips","text":"<ul> <li>\ud83d\udcddCreate \"Manual\" type project</li> <li>\ud83d\udcddTrust custom Certificate Authority</li> <li>\ud83d\udcddExpose <code>/etc/hosts</code> to Pods on K3s</li> <li>\ud83d\udcddDeploy older version of AWX Operator</li> <li>\ud83d\udcddUpgrade AWX Operator and AWX</li> <li>\ud83d\udcddWorkaround for the rate limit on Docker Hub</li> <li>\ud83d\udcddVersion Mapping between AWX Operator and AWX</li> <li>\ud83d\udcddUse Kerberos authentication to connect to Windows hosts</li> <li>\ud83d\udcddTroubleshooting Guide</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/deploy-older-operator/","title":"Deploy older operator","text":""},{"location":"admin-guide/runbooks/awx/tips/deploy-older-operator/#deploy-older-version-of-awx-operator","title":"Deploy older version of AWX Operator","text":"<p>The installation method for AWX Operator has used ArgoCD since version <code>0.14.0</code>. If you want to deploy <code>0.13.0</code> or earlier version of AWX Operator, the old procedure must be followed.</p>"},{"location":"admin-guide/runbooks/awx/tips/deploy-older-operator/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Install AWX Operator</li> <li>Monitor the logs of AWX Operator</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/deploy-older-operator/#install-awx-operator","title":"Install AWX Operator","text":"<p>If you want to deploy <code>0.13.0</code> or earlier version of AWX Operator, you can directly invoke <code>kubectl apply</code> using the manifest file on GitHub instead of using <code>make</code> command. Official old <code>README.md</code> on <code>ansible/awx-operator</code> is also helpful.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/ansible/awx-operator/0.13.0/deploy/awx-operator.yaml\n</code></pre> <p>The AWX Operator will be deployed to the <code>default</code> namespace.</p> <pre><code>$ kubectl -n default get all\nNAME                                READY   STATUS    RESTARTS   AGE\npod/awx-operator-69c646c48f-jmtrs   1/1     Running   0          93s\n\nNAME                           TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)             AGE\nservice/kubernetes             ClusterIP   10.43.0.1     &lt;none&gt;        443/TCP             5m57s\nservice/awx-operator-metrics   ClusterIP   10.43.183.1   &lt;none&gt;        8383/TCP,8686/TCP   70s\n\nNAME                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/awx-operator   1/1     1            1           93s\n\nNAME                                      DESIRED   CURRENT   READY   AGE\nreplicaset.apps/awx-operator-69c646c48f   1         1         1       93s\n</code></pre> <p>Once you have AWX Operator, the rest of the steps are the same as in <code>0.14.0</code> and later.</p>"},{"location":"admin-guide/runbooks/awx/tips/deploy-older-operator/#monitor-the-logs-of-awx-operator","title":"Monitor the logs of AWX Operator","text":"<p>You can monitor the logs of AWX Operator by following command.</p> <pre><code>kubectl logs -f deployment/awx-operator\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/","title":"Dockerhub rate limit","text":""},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#workaround-for-the-rate-limit-on-docker-hub","title":"Workaround for the rate limit on Docker Hub","text":"<p>If your Pod for PostgreSQL is in <code>ErrImagePull</code> and its <code>Events</code> shows following events, this is due to the Rate Limit on Docker Hub.</p> <pre><code>$ kubectl -n awx describe pod awx-postgres-13-0\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  ...\n  Warning  Failed            2s    kubelet            Failed to pull image \"postgres:13\": rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/library/postgres:13\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/library/postgres/manifests/sha256:...: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit\n  ...\n</code></pre> <p>If you just follow the steps in this repository to deploy you AWX, your pull request to Docker Hub will be identified as a free, anonymous account. Therefore, you will be limited to 200 requests in 6 hours. The message \"429 Too Many Requests\" indicates that the limit has been exceeded. To solve this, you should pass your Docker Hub credentials to the Pod as ImagePullSecrets.</p>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Procedure</li> <li>Create <code>base/config.json</code></li> <li>Modify <code>base/kustomization.yaml</code></li> <li>Modify <code>base/awx.yaml</code></li> <li>Next Step</li> <li>Appendix: Create Secret for ImagePullSecrets by Hand</li> <li>Appendix: Modify ImagePullSecrets for the Specific Service Account</li> <li>Appendix: Apply Docker Hub Credential at Cluster Level</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#procedure","title":"Procedure","text":"<p>There are several ways to achieve this, but this guide will show you an example of using Kustomize to save all your configuration as code. In addition to the main guide, a few additional files need to be modified.</p>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#create-baseconfigjson","title":"Create <code>base/config.json</code>","text":"<p>First, prepare JSON file named <code>config.json</code> under your <code>base</code> directory.</p> <pre><code>$ DOCKERHUB_USERNAME=&lt;Your Username for Docker Hub&gt;\n$ DOCKERHUB_PASSWORD=&lt;Your Password or Personal Access Token for Docker Hub&gt;\n$ DOCKERHUB_AUTH=$(echo -n \"${DOCKERHUB_USERNAME}:${DOCKERHUB_PASSWORD}\" | base64)\n$ cat &lt;&lt;EOF &gt; base/config.json\n{\n    \"auths\": {\n        \"docker.io\": {\n            \"auth\": \"${DOCKERHUB_AUTH}\"\n        }\n    }\n}\nEOF\n</code></pre> <p>Ensure your <code>config.json</code> includes base64-encoded string as following. Note that this base64-encoded string includes your password in plain text and easily revealed by <code>echo \"&lt;Encoded String&gt;\" | base64 --decode</code> command.</p> <pre><code>$ cat base/config.json\n{\n    \"auths\": {\n        \"docker.io\": {\n            \"auth\": \"ZXhhbXBsZS...MtdG9rZW4K\"\n        }\n    }\n}\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#modify-basekustomizationyaml","title":"Modify <code>base/kustomization.yaml</code>","text":"<p>Then, add following four lines to under <code>secretGenerator</code> in <code>base/kustomization.yaml</code>.</p> <pre><code>...\nsecretGenerator:\n  ...\n  - name: awx-registry-secret     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n    type: kubernetes.io/dockerconfigjson     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n    files:     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n      - .dockerconfigjson=config.json     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  ...\nresources:\n  ...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#modify-baseawxyaml","title":"Modify <code>base/awx.yaml</code>","text":"<p>Finally, add following line to <code>base/awx.yaml</code>.</p> <pre><code>...\nspec:\n  ...\n  image_pull_secret: awx-registry-secret     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  ...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#next-step","title":"Next Step","text":"<p>Now everything is ready to deploy, go back to the main guide (<code>README.md</code>) and run <code>kubectl apply -k base</code>.</p>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#appendix-create-secret-for-imagepullsecrets-by-hand","title":"Appendix: Create Secret for ImagePullSecrets by Hand","text":"<p>If you want to create Secret manually instead of using Kustomize, you can choose alternative way. If you don't have <code>awx</code> namespace, create it first by <code>kubectl create ns awx</code>.</p> <ul> <li>Method 1: Create Secret by using existing <code>~/.docker/config.json</code></li> </ul> <pre><code># Log in to Docker Hub via docker command first,\n$ docker login\n\n# Then create Secret by specifying ~/.docker/config.json which generated by Docker\n$ kubectl -n awx create secret generic awx-registry-secret \\\n    --from-file=.dockerconfigjson=${HOME}/.docker/config.json \\\n    --type=kubernetes.io/dockerconfigjson\n</code></pre> <ul> <li>Method 2: Create Secret by specifying username and password manually</li> </ul> <pre><code># Create Secret by specifying username and password manually\n$ DOCKERHUB_USERNAME=&lt;Your Username for Docker Hub&gt;\n$ DOCKERHUB_PASSWORD=&lt;Your Password or Personal Access Token for Docker Hub&gt;\n$ kubectl -n awx create secret docker-registry awx-registry-secret \\\n    --docker-server=docker.io \\\n    --docker-username=${DOCKERHUB_USERNAME} \\\n    --docker-password=${DOCKERHUB_PASSWORD}\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#appendix-modify-imagepullsecrets-for-the-specific-service-account","title":"Appendix: Modify ImagePullSecrets for the Specific Service Account","text":"<p>Once create new namespace, the default Service Account named <code>default</code> is also created by default. This <code>default</code> Service Account is used when no Service Account has been specified for the Pod, and you can modify ImagePullSecrets which used by default for this <code>default</code> Service Account too. This can be applied to Private Git Repository and Private Container Registry included in this repository. Additionally, this can also be applied to the Pod for PostgreSQL created by AWX Operator, since the Service Account is not specified.</p> <p>First, you should create Secret for the Service Account by referring Appendix: Create Secret for ImagePullSecrets by Hand. Note that the namespace and the name of the Secret in the command should be changed to suit your environment.</p> <p>Then patch the <code>default</code> service account.</p> <pre><code>kubectl -n &lt;Your Namespace&gt; patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"&lt;Your Secret&gt;\"}]}'\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/dockerhub-rate-limit/#appendix-apply-docker-hub-credential-at-cluster-level","title":"Appendix: Apply Docker Hub Credential at Cluster Level","text":"<p>You can also change the entire K3s configuration so that the specific credential for Docker Hub is always used, regardless of the namespace. Create <code>/etc/rancher/k3s/registries.yaml</code> and restart K3s service as follows.</p> <pre><code># Create /etc/rancher/k3s/registries.yaml including your credential\n$ DOCKERHUB_USERNAME=&lt;Your Username for Docker Hub&gt;\n$ DOCKERHUB_PASSWORD=&lt;Your Password or Personal Access Token for Docker Hub&gt;\n$ sudo tee /etc/rancher/k3s/registries.yaml &lt;&lt;EOF\nconfigs:\n  registry-1.docker.io:\n    auth:\n      username: ${DOCKERHUB_USERNAME}\n      password: ${DOCKERHUB_PASSWORD}\nEOF\n\n# Then restart K3s. The K3s service can be safely restarted without affecting the running resources\n$ sudo systemctl restart k3s\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/expose-hosts/","title":"Expose your <code>/etc/hosts</code> to Pods on K3s","text":"<p>If we don't have a DNS server and are using <code>/etc/hosts</code>, we will need to do some additional tasks to get the Pods on K3s to resolve names according to <code>/etc/hosts</code>.</p> <p>This is necessary for AWX to resolve the hostname for your private Git repository or pull images from the container registry.</p> <p>One easy way to do this is to use <code>dnsmasq</code>.</p> <ol> <li>Add entries to <code>/etc/hosts</code> on your K3s host. Note that the IP addresses have to be replaced with your K3s host's one.</li> </ol> <pre><code>sudo tee -a /etc/hosts &lt;&lt;EOF\n192.168.0.219 awx.example.com\n192.168.0.219 registry.example.com\n192.168.0.219 git.example.com\n192.168.0.219 galaxy.example.com\nEOF\n</code></pre> <ol> <li>Install and start <code>dnsmasq</code> with default configuration.</li> </ol> <pre><code>sudo dnf install dnsmasq\nsudo systemctl enable dnsmasq --now\n</code></pre> <ol> <li>Create new <code>resolv.conf</code> to use K3s. Note that the IP addresses have to be replaced with your K3s host's one.</li> </ol> <pre><code>sudo tee /etc/rancher/k3s/resolv.conf &lt;&lt;EOF\nnameserver 192.168.0.219\nEOF\n</code></pre> <ol> <li>Add <code>--resolv-conf /etc/rancher/k3s/resolv.conf</code> as an argument for <code>k3s server</code> command.</li> </ol> <pre><code># Change configuration using script:\n$ curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --resolv-conf /etc/rancher/k3s/resolv.conf\n\n# If you don't want to use the script, modify /etc/systemd/system/k3s.service manually:\n$ cat /etc/systemd/system/k3s.service\n...\nExecStart=/usr/local/bin/k3s \\\n    server \\\n        '--write-kubeconfig-mode' \\\n        '644' \\\n        '--resolv-conf' \\     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n        '/etc/rancher/k3s/resolv.conf' \\     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <ol> <li>Restart K3s and CoreDNS. The K3s service can be safely restarted without affecting the running resources.</li> </ol> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart k3s\nkubectl -n kube-system delete pod -l k8s-app=kube-dns\n</code></pre> <ol> <li>Ensure that your hostname can be resolved as defined in <code>/etc/hosts</code>.</li> </ol> <pre><code>$ kubectl run -it --rm --restart=Never busybox --image=busybox:1.28 -- nslookup git.example.com\nServer:    10.43.0.10\nAddress 1: 10.43.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      git.example.com\nAddress 1: 192.168.0.219\npod \"busybox\" deleted\n</code></pre> <ol> <li>If you update your <code>/etc/hosts</code>, restarting <code>dnsmasq</code> is required.</li> </ol> <pre><code>sudo systemctl restart dnsmasq\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/manual-project/","title":"Create \"Manual\" type project","text":"<p>The Git repository is the most preferred place to store your playbooks, and the guides and resources to deploy Private Git Repository on K3s are also provided in this repository.</p> <p>However, if you don't need such a rich version control system, or need to make frequent iterations of trial and error to develop new playbooks, creating a project with Manual type is very helpful.</p>"},{"location":"admin-guide/runbooks/awx/tips/manual-project/#concepts","title":"Concepts","text":"<p>The project directories have to exist under <code>/var/lib/awx/projects</code> of AWX, and if you have deployed AWX following the steps described in the main guide on this repository, <code>/data/projects</code> on your K3s host is mounted as <code>/var/lib/awx/projects</code> in AWX.</p> <p>So, to add project directories, simply, just placing it under <code>/data/projects</code> on your K3s host.</p>"},{"location":"admin-guide/runbooks/awx/tips/manual-project/#procedure","title":"Procedure","text":"<p>Create new directory under <code>/data/projects</code> on your K3s host, and place your playbooks under the directory you created. Note that this directory and files under the directory must be readable by the user with UID <code>1000</code>.</p> <pre><code>$ tree /data/projects/\n/data/projects/\n`-- my-first-manual-project     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n    `-- my-playbook.yaml     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Go to <code>Resources</code> &gt; <code>Projects</code> &gt; <code>Add</code> in AWX Web UI, fill <code>Name</code> field and select <code>Manual</code> as <code>Source Control Type</code>.</p> <p>Now you can select your project directory (<code>my-first-manual-project</code> in this example) as <code>Playbook Directory</code>.</p> <p>After <code>Save</code> the project, your playbooks can be selected in the Job Templates.</p>"},{"location":"admin-guide/runbooks/awx/tips/manual-project/#troubleshooting","title":"Troubleshooting","text":"<p>If you got following warning while selecting <code>Playbook Directory</code>, try super-reloading your browser (<code>Shift + F5</code> or <code>Ctrl (Cmd) + Shift + R</code>) to refresh the page without using the cache stored in the browser.</p> <p>\u26a0\ufe0fWARNING: There are no available playbook directories in /var/lib/awx/projects</p>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/","title":"Troubleshooting","text":""},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#troubleshooting-guide","title":"Troubleshooting Guide","text":"<p>Some hints and guides for if you get stuck during the daily use of AWX.</p>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Troubles during Deployment</li> <li>First Step: Investigate your Situation<ul> <li>Investigate Status and Events of the Pods</li> <li>Investigate Logs of the Containers inside the Pods</li> </ul> </li> <li>Reveal \"censored\" output in the AWX Operator's log</li> <li>The Pod is <code>ErrImagePull</code> with \"429 Too Many Requests\"</li> <li>The Pod is <code>Pending</code> with \"1 Insufficient cpu, 1 Insufficient memory.\" event</li> <li>The Pod is <code>Pending</code> with \"1 pod has unbound immediate PersistentVolumeClaims.\" event</li> <li>The Pod is <code>Running</code> but stucked with \"wait-for-migrations\" message</li> <li>The Pod for PostgreSQL is in <code>CrashLoopBackOff</code> state and shows \"Permission denied\" log</li> <li>Troubles during Daily Use</li> <li>Job failed with no output</li> <li>Provisioning Callback does not work</li> <li>The job failed and I got \"ERROR! couldn't resolve module/action\" or \"Failed to import the required Python library\" message</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#troubles-during-deployment","title":"Troubles during Deployment","text":""},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#first-step-investigate-your-situation","title":"First Step: Investigate your Situation","text":"<p>You can start investigating troubles during deployment with following two things.</p> <ul> <li>Status and Events of the Pods</li> <li>Logs of the Containers inside the Pods</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#investigate-status-and-events-of-the-pods","title":"Investigate Status and Events of the Pods","text":"<p>First, check the <code>STATUS</code> for the Pods by this command.</p> <pre><code>kubectl -n awx get pod\n</code></pre> <p>If the Pods are working properly, its <code>STATUS</code> are <code>Running</code>. If your Pods are not in <code>Running</code> state e.g. <code>Pending</code>, <code>ImagePullBackOff</code> or <code>CrashLoopBackOff</code> etc., the Pods might have some problems. In the following example, the Pod <code>awx-84d5c45999-h7xm4</code> is in <code>Pending</code> state.</p> <pre><code>$ kubectl -n awx get pod\nNAME                                               READY   STATUS    RESTARTS   AGE\nawx-operator-controller-manager-68d787cfbd-j6k7z   2/2     Running   0          7m43s\nawx-postgres-13-0                                  1/1     Running   0          4m6s\nawx-84d5c45999-h7xm4                               0/4     Pending   0          3m59s\n</code></pre> <p>If you have the Pods which has the unexpected state instead of <code>Running</code>, the next step is checking <code>Events</code> for the Pod. The command to get <code>Events</code> for the pod is:</p> <pre><code>kubectl -n awx describe pod &lt;Pod Name&gt;\n</code></pre> <p>By this command, you can get the <code>Events</code> for the Pod you specified at the end of the output.</p> <pre><code>$ kubectl -n awx describe pod awx-84d5c45999-h7xm4\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  106s  default-scheduler  0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory.     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  Warning  FailedScheduling  105s  default-scheduler  0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory.     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>In most cases, you can find the reason why the Pod is not <code>Running</code> from <code>Events</code>. In the example above, I can see that it is due to lack of CPU or memory.</p>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#investigate-logs-of-the-containers-inside-the-pods","title":"Investigate Logs of the Containers inside the Pods","text":"<p>The logs also helpful to get the reason why something went wrong. In particular, if the status of the Pod is <code>Running</code> but the Pod does not works as expected, you should check the logs.</p> <p>The commands to get the logs are following. <code>-f</code> is optional, useful to watch the logs as well as get the logs.</p> <pre><code># Get the logs of specific Pod.\n# If the Pod includes multiple containers, container name has to be specified.\nkubectl -n awx logs -f &lt;Pod Name&gt;\nkubectl -n awx logs -f &lt;Pod Name&gt; -c &lt;Container Name&gt;\n\n# Get the logs of specific Pod which is handled by Deployment resource.\n# If the Pod includes multiple containers, container name has to be specified.\nkubectl -n awx logs -f deployment/&lt;Deployment Name&gt;\nkubectl -n awx logs -f deployment/&lt;Deployment Name&gt; -c &lt;Container Name&gt;\n\n# Get the logs of specific Pod which is handled by StatefulSet resource\n# If the Pod includes multiple containers, container name has to be specified.\nkubectl -n awx logs -f statefulset/&lt;Deployment Name&gt;\nkubectl -n awx logs -f statefulset/&lt;Deployment Name&gt; -c &lt;Container Name&gt;\n</code></pre> <p>For AWX Operator and AWX, specifically, the following commands are helpful.</p> <ul> <li>Logs of AWX Operator</li> <li><code>kubectl -n awx logs -f deployment/awx-operator-controller-manager</code></li> <li>Logs of AWX related init containers</li> <li><code>kubectl -n awx logs -f deployment/awx -c init</code></li> <li><code>kubectl -n awx logs -f deployment/awx -c init-projects</code></li> <li>Logs of AWX related containers</li> <li><code>kubectl -n awx logs -f deployment/awx -c awx-web</code></li> <li><code>kubectl -n awx logs -f deployment/awx -c awx-task</code></li> <li><code>kubectl -n awx logs -f deployment/awx -c awx-ee</code></li> <li><code>kubectl -n awx logs -f deployment/awx -c redis</code></li> <li>Logs of PostgreSQL</li> <li><code>kubectl -n awx logs -f statefulset/awx-postgres-13</code></li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#reveal-censored-output-in-the-awx-operators-log","title":"Reveal \"censored\" output in the AWX Operator's log","text":"<p>If you've found the <code>FAILED</code> tasks while investigating AWX Operator's log, sadly sometimes it's marked as <code>censored</code> and you can't get actual log.</p> <pre><code>$ kubectl -n awx logs -f deployments/awx-operator-controller-manager\n...\nTASK [Restore database dump to the new postgresql container] ********************************\nfatal: [localhost]: FAILED! =&gt; {\"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\", \"changed\": true}\n...\n</code></pre> <p>AWX Operator 0.23.0 or later supports making this revealed.</p> <p>To achieve this, you can uncomment <code>no_log: false</code> manually under <code>spec</code> for your <code>awx.yaml</code>, <code>awxbackup.yaml</code>, or <code>awxrestore.yaml</code>, and then re-run your deployment, backup, or restoration.</p> <pre><code>...\nspec:\n  ...\n  # Uncomment to reveal \"censored\" logs\n  no_log: false     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  ...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#the-pod-is-errimagepull-with-429-too-many-requests","title":"The Pod is <code>ErrImagePull</code> with \"429 Too Many Requests\"","text":"<p>If your Pod for PostgreSQL is in <code>ErrImagePull</code> and its <code>Events</code> shows following events, this is due to the rate limit on Docker Hub.</p> <pre><code>$ kubectl -n awx describe pod awx-postgres-13-0\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Normal   Pulling           9s    kubelet            Pulling image \"postgres:13\"\n  Warning  Failed            2s    kubelet            Failed to pull image \"postgres:13\": rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/library/postgres:13\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/library/postgres/manifests/sha256:...: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit\n  Warning  Failed            2s    kubelet            Error: ErrImagePull\n  Normal   BackOff           1s    kubelet            Back-off pulling image \"postgres:13\"\n  Warning  Failed            1s    kubelet            Error: ImagePullBackOff\n</code></pre> <p>If you follow the steps in this repository to deploy you AWX, your pull request to Docker Hub will be identified as a free, anonymous account. Therefore, you will be limited to 200 requests in 6 hours. The message \"429 Too Many Requests\" indicates that it has been exceeded.</p> <p>To solve this, you can simply wait until the limit is freed up, or consider giving your Docker Hub credentials to K3s by following the guide on this page.</p>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#the-pod-is-pending-with-1-insufficient-cpu-1-insufficient-memory-event","title":"The Pod is <code>Pending</code> with \"1 Insufficient cpu, 1 Insufficient memory.\" event","text":"<p>If your Pod is in <code>Pending</code> state and its <code>Events</code> shows following events, the reason is that the node does not have enough CPU and memory to start the Pod. By default AWX requires at least 2 CPUs and 4 GB RAM. In addition more resources are required to run K3s and the OS itself.</p> <pre><code>$ kubectl -n awx describe pod awx-84d5c45999-h7xm4\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  106s  default-scheduler  0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory.     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  Warning  FailedScheduling  105s  default-scheduler  0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory.     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Typical solutions are one of the following:</p> <ul> <li>Add more CPUs or memory to your K3s node.</li> <li>If you have at least 3 CPUs and 5 GB RAM, AWX may work.</li> <li>Reduce resource requests for the containers.</li> <li> <p>The minimum resource requirements can be ignored by adding three lines in <code>base/awx.yml</code>.</p> <pre><code>...\nspec:\n  ...\n  web_resource_requirements: {}     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  task_resource_requirements: {}     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  ee_resource_requirements: {}     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> </li> <li> <p>You can specify more specific value for each containers. Refer official documentation for details.</p> </li> <li>In this way you can run AWX with fewer resources, but you may encounter performance issues.</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#the-pod-is-pending-with-1-pod-has-unbound-immediate-persistentvolumeclaims-event","title":"The Pod is <code>Pending</code> with \"1 pod has unbound immediate PersistentVolumeClaims.\" event","text":"<p>If your Pod is in <code>Pending</code> state and its <code>Events</code> shows following events, the reason is that no usable Persistent Volumes are available.</p> <pre><code>$ kubectl -n awx describe pod awx-84d5c45999-h7xm4\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  24s   default-scheduler  0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims.     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Check the <code>STATUS</code> of your PVs and ensure your PVs doesn't have <code>Available</code> or <code>Bound</code> state.</p> <pre><code>$ kubectl get pv\nNAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                               STORAGECLASS             REASON   AGE\nawx-projects-volume      2Gi        RWO            Retain           Released   awx/awx-projects-claim              awx-projects-volume               17h\nawx-postgres-13-volume   8Gi        RWO            Retain           Released   awx/postgres-13-awx-postgres-13-0   awx-postgres-volume               17h\n</code></pre> <p>Probably this is the second (or more) time to deploy AWX for you. These PVs which have <code>Released</code> state are tied to your old (and probably no longer exists now) PVCs you created in the past.</p> <p>There are a few things you should to know about the PVs in Kubernetes.</p> <ul> <li>Once a PV is bound from a PVC, it keeps the PVC name in its <code>claimRef</code> entry. This will be shown in the <code>CLAIM</code> column in the result of the command <code>kubectl get pv</code>.</li> <li>The <code>Released</code> state of the PV means that the PV was bound by PVC in the <code>claimRef</code> entry in the past but now the PVC does not exist. The PV in this state cannot be bound by any PVC other than the one recorded in <code>claimRef</code>.</li> <li>To allow the PV to bind from a PVC other than the one recorded in <code>claimRef</code>, the <code>claimRef</code> entry must be empty and the PV must has <code>Available</code> state.</li> </ul> <p>To solve this, typical solutions are one of the following:</p> <ul> <li>Patch the PV to empty <code>claimRef</code> entry for the PV.</li> <li> <p>Invoke following commands:</p> <pre><code>kubectl patch pv &lt;PV Name&gt; -p '{\"spec\":{\"claimRef\": null}}'\n</code></pre> </li> <li> <p>Delete the PV and recreate it.</p> </li> <li> <p>Invoke following commands:</p> <pre><code># Delete the PV\nkubectl delete pv &lt;PV Name&gt;\n\n# Recreate the PV\nkubectl apply -k base\n</code></pre> </li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#the-pod-is-running-but-stucked-with-wait-for-migrations-message","title":"The Pod is <code>Running</code> but stucked with \"wait-for-migrations\" message","text":"<p>Sometimes your AWX pod is <code>Running</code> state correctly but not functional at all, and its log shows following message repeatedly.</p> <pre><code>kubectl -n awx logs -f deployment/awx -c awx-web\n[wait-for-migrations] Waiting for database migrations...\n[wait-for-migrations] Attempt 1 of 30\n[wait-for-migrations] Waiting 0.5 seconds before next attempt\n[wait-for-migrations] Attempt 2 of 30\n[wait-for-migrations] Waiting 1 seconds before next attempt\n[wait-for-migrations] Attempt 3 of 30\n[wait-for-migrations] Waiting 2 seconds before next attempt\n[wait-for-migrations] Attempt 4 of 30\n[wait-for-migrations] Waiting 4 seconds before next attempt\n...\n[wait-for-migrations] Attempt 28 of 30\n[wait-for-migrations] Waiting 30 seconds before next attempt\n[wait-for-migrations] Attempt 29 of 30\n[wait-for-migrations] Waiting 30 seconds before next attempt\n...\n</code></pre> <p>This problem occurs when the AWX pod and the PostgreSQL pod cannot communicate properly. In most cases, the cause of this is the network on your K3s.</p> <p>To solve this, check or try the following:</p> <ul> <li>Ensure your PostgreSQL (typically the Pod named <code>awx-postgres-0</code> or <code>awx-postgres-13-0</code>) is in <code>Running</code> state.</li> <li>Ensure <code>host</code> under <code>awx-postgres-configuration</code> in <code>base/kustomization.yaml</code> has correct value.</li> <li>Specify <code>awx-postgres</code> for AWX Operator 0.25.0 or earlier, <code>awx-postgres-13</code> for <code>0.26.0</code> or later.</li> <li>Ensure your <code>firewalld</code>, <code>ufw</code> or any kind of firewall has been disabled on your K3s host.</li> <li>Ensure your <code>nm-cloud-setup</code> service on your K3s host is disabled if exists.</li> <li>Ensure your <code>awx-postgres-configuration</code> has correct values, especially if you're using external PostgreSQL.</li> <li>Uninstall K3s and install it again.</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#the-pod-for-postgresql-is-in-crashloopbackoff-state-and-shows-permission-denied-log","title":"The Pod for PostgreSQL is in <code>CrashLoopBackOff</code> state and shows \"Permission denied\" log","text":"<p>In this situation, your Pod for PostgreSQL is in <code>CrashLoopBackOff</code> state and its log shows following error message.</p> <pre><code>$ kubectl -n awx get pod\nNAME                                               READY   STATUS             RESTARTS   AGE\nawx-operator-controller-manager-68d787cfbd-j6k7z   2/2     Running            0          7m43s\nawx-postgres-13-0                                  1/1     CrashLoopBackOff   3          4m6s\nawx-84d5c45999-h7xm4                               4/4     Running            0          3m59s\n\n$ kubectl -n awx logs statefulset/awx-postgres\nmkdir: cannot create directory '/var/lib/postgresql/data': Permission denied\n</code></pre> <p>You should check the permissions and the owner of directories where used as PV on your K3s host. If you followed my guide, it would be <code>/data/postgres-13</code>. There is additional <code>data</code> directory created by K3s under <code>/data/postgres-13</code>.</p> <pre><code>$ ls -ld /data/postgres-13 /data/postgres-13/data\ndrwxr-xr-x. 2 root root 18 Aug 20 10:09 /data/postgres-13\ndrwxr-xr-x. 3 root root 20 Aug 20 10:09 /data/postgres-13/data\n</code></pre> <p>In my environment, <code>755</code> and <code>root:root</code> (<code>0:0</code>) works correctly. So you can try:</p> <pre><code>sudo chmod 755 /data/postgres-13 /data/postgres-13/data\nsudo chown 0:0 /data/postgres-13 /data/postgres-13/data\n</code></pre> <p>Or, you can also try <code>999:0</code> as owner/group for the directory.</p> <pre><code>sudo chmod 755 /data/postgres-13 /data/postgres-13/data\nsudo chown 999:0 /data/postgres-13 /data/postgres-13/data\n</code></pre> <p><code>999</code> is the UID of the <code>postgres</code> user which used in the container.</p>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#troubles-during-daily-use","title":"Troubles during Daily Use","text":""},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#job-failed-with-no-output","title":"Job failed with no output","text":"<p>If the job is invoked to a large number of hosts or is running long time, sometimes the job is marked as failed and no log will be displayed in the Output tab.</p> <p>This is a problem caused by log rotation on Kubernetes. Refer ansible/awx#10366 for details.</p> <p>In the case of K3s, you can reduce the possibility of this issue by changing the configuration as follows.</p> <pre><code># Change configuration using script:\n$ curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --kubelet-arg \"container-log-max-files=4\" --kubelet-arg \"container-log-max-size=50Mi\"\n\n# If you don't want to use the script, modify /etc/systemd/system/k3s.service manually:\n$ cat /etc/systemd/system/k3s.service\n...\nExecStart=/usr/local/bin/k3s \\\n    server \\\n        '--write-kubeconfig-mode' \\\n        '644' \\\n        '--kubelet-arg' \\     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n        'container-log-max-files=4' \\     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n        '--kubelet-arg' \\     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n        'container-log-max-size=50Mi' \\     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Then restart K3s. The K3s service can be safely restarted without affecting the running resources.</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart k3s\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#provisioning-callback-does-not-work","title":"Provisioning Callback does not work","text":"<p>If you use Traefik which is K3s' Ingress controller as completely default, the Pod may not be able to get the client's IP address (see k3s-io/k3s#2997 for details). Therefore, the feature called Provisioning Callback in AWX does not work properly since AWX can't determine actual IP address of the remote host who request callback.</p> <p>For this reason, you should fix the Traefik configuration. For a single node like doing in this repository, reconfiguring your Traefik by creating YAML file is the easy way.</p> <pre><code>sudo tee /var/lib/rancher/k3s/server/manifests/traefik-config.yaml &lt;&lt;EOF\n---\napiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\n  name: traefik\n  namespace: kube-system\nspec:\n  valuesContent: |-\n    hostNetwork: true\nEOF\n</code></pre> <p>Then wait until your <code>traefik</code> by the following command is <code>1/1</code> <code>READY</code>.</p> <pre><code>kubectl -n kube-system get deployment traefik\n</code></pre> <p>Now your client's IP address can be passed correctly through <code>X-Forwarded-For</code> and <code>X-Real-Ip</code> headers.</p> <p>The last step is modifying AWX. By default, AWX uses only <code>REMOTE_ADDR</code> and <code>REMOTE_HOST</code> headers to determine the remote host (means HTTP client). Therefore, you have to make AWX to use <code>X-Forwarded-For</code> header.</p> <p>This can be achieved by modifying your <code>base/awx.yaml</code> and apply it, or simply configure <code>Remote Host Headers</code> via AWX UI.</p> <p>If you want to use <code>base/awx.yaml</code> to achieve this, add following three lines to your <code>base/awx.yaml</code>.</p> <pre><code>...\nspec:\n  ...\n  extra_settings:     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n    - setting: REMOTE_HOST_HEADERS     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n      value: \"['HTTP_X_FORWARDED_FOR', 'REMOTE_ADDR', 'REMOTE_HOST']\"     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Then apply this change and wait for your AWX will be reconfigured.</p> <pre><code>kubectl apply -k base\n</code></pre> <p>You can watch its progress by following command as did when you deploy AWX at the first time.</p> <pre><code>kubectl -n awx logs -f deployments/awx-operator-controller-manager\n</code></pre> <p>Alternatively you can modify this settings via AWX UI. Move on to <code>Settings</code> &gt; <code>Miscellaneous System settings</code> &gt; <code>Edit</code> page, then and put following JSON strings as <code>Remote Host Headers</code>.</p> <pre><code>[\n  \"HTTP_X_FORWARDED_FOR\",\n  \"REMOTE_ADDR\",\n  \"REMOTE_HOST\"\n]\n</code></pre> <p>Now your Provisioning Callback should work. In my environment, the name of the host in the inventory have to be defined using IP address instead of DNS hostname.</p>"},{"location":"admin-guide/runbooks/awx/tips/troubleshooting/#the-job-failed-and-i-got-error-couldnt-resolve-moduleaction-or-failed-to-import-the-required-python-library-message","title":"The job failed and I got \"ERROR! couldn't resolve module/action\" or \"Failed to import the required Python library\" message","text":"<p>When you launch the Job Template, it may fail and you will see an error like the following:</p> <pre><code>ERROR! couldn't resolve module/action 'community.postgresql.postgresql_info'. This often indicates a misspelling, missing collection, or incorrect module path.\n\nThe error appears to be in '/runner/project/site.yml': line 6, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n  tasks:\n    - community.postgresql.postgresql_info:\n      ^ here\n</code></pre> <p>Alternatively, the import of Python modules may fail.</p> <pre><code>...\nTASK [community.postgresql.postgresql_info] ************************************\nfatal: [localhost]: FAILED! =&gt; {\"changed\": false, \"msg\": \"Failed to import the required Python library (psycopg2) on automation-job-12-v2gvf's Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter\"}\n...\n</code></pre> <p>To solve this, please contact the Ubiquity community via GitHub Issues and we can work to build another container.</p>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/","title":"Trust custom ca","text":""},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#trust-custom-certificate-authority","title":"Trust custom Certificate Authority","text":"<p>If your AWX has to trust custom Certificate Authority, you can pass the CA certificates to AWX. This is helpful in cases:</p> <ul> <li>Use private Git repository via SSL, without ignoring SSL verification.</li> <li>Use LDAPS to authenticate users.</li> </ul> <p>Refer the official documentation for more information.</p>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Prepare required CA certificatess</li> <li>Modify <code>base/kustomization.yaml</code></li> <li>Modify <code>base/awx.yaml</code></li> <li>Apply configuration</li> <li>Troubleshooting</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#overview","title":"Overview","text":"<p>Trusting custom Certificate Authority can be achieved by following steps:</p> <ol> <li>Creating new Secret which includes your certificates</li> <li>Passing it to your AWX by specifying the name of the Secret as your AWX's specification</li> </ol> <p>There are two kinds of certificate, one is used to trust LDAP server, and the other is used as the CA bundle.</p> Fields in the specification for AWX Keys in Secret Containers in AWX pod that the certificate will be mounted Paths that the certificate will be mounted as <code>ldap_cacert_secret</code> <code>ldap-ca.crt</code> <code>awx-web</code> <code>/etc/openldap/certs/ldap-ca.crt</code> <code>bundle_cacert_secret</code> <code>bundle-ca.crt</code> <code>awx-web</code>, <code>awx-task</code>, and <code>awx-ee</code> <code>/etc/pki/ca-trust/source/anchors/bundle-ca.crt</code> <p>Note that the <code>awx-ee</code> container is used to run management jobs only, not EE which runs your playbooks. If the EE running your playbook needs a certificate, you will need to customize the pod specification.</p> <p>If you want to mount the certificate to the additional containers in AWX pod or the additional path other than above, you shoud add extra volumes and extra mounts using <code>extra_volumes</code> and <code>_extra_volume_mounts</code> field, but this is not covered in this guide. Refer to the official documentation.</p>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#prepare-required-ca-certificatess","title":"Prepare required CA certificatess","text":"<p>Place your certificates under <code>base</code> directory.</p> <pre><code>$ ls -l base\ntotal 32\n-rw-rw-r--. 1 kuro kuro  801 Feb 27 00:23 awx.yaml\n-rw-rw-r--. 1 kuro kuro 1339 Feb 27 00:44 cacert.pem     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n-rw-rw-r--. 1 kuro kuro  610 Feb 27 00:23 kustomization.yaml\n...\n</code></pre> <p>Note that your certificates have to have PEM format. You can check the format of the certificates depending on which of the following commands succeeds.</p> <pre><code># Works for PEM format\nopenssl x509 -in cacert.crt -text\n\n# Works for DER format\nopenssl x509 -in cacert.crt -inform DER -text\n\n# Works for PKCS #7 format\nopenssl pkcs7 -in cacert.crt -text\n\n# Works for PKCS #12 format\nopenssl pkcs12 -in cacert.crt -info\n</code></pre> <p>If your certificate doesn't have PEM format, you can convert it by followings:</p> <pre><code># Convert DER to PEM\nopenssl x509 -in cacert.crt -inform DER -out cacert.pem -outform PEM\n\n# Convert PKCS #7 to PEM\nopenssl pkcs7 -print_certs -in cacert.crt -out cacert.pem -outform PEM\n\n# Convert PKCS #12 to PEM\nopenssl pkcs12 -in cacert.crt -out cacert.pem -nokeys -nodes\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#modify-basekustomizationyaml","title":"Modify <code>base/kustomization.yaml</code>","text":"<p>Add following lines under <code>secretGenerator</code> in <code>base/kustomization.yaml</code>.</p> <p>Note that this example provides both <code>ldap-ca.crt</code> and <code>bundle-ca.crt</code>, but you can remove unnecessary line if you don't need both of them. <code>ldap-ca.crt</code> will be used as the CA certificate for LDAP server, and <code>bundle-ca.crt</code> will be used as the CA bundle.</p> <pre><code>...\nsecretGenerator:\n  ...\n  - name: awx-custom-certs     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n    type: Opaque     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n    files:     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n      - ldap-ca.crt=&lt;Name Of Your Certificate File&gt;     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n      - bundle-ca.crt=&lt;Name Of Your Certificate File&gt;     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  ...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#modify-baseawxyaml","title":"Modify <code>base/awx.yaml</code>","text":"<p>Add following lines under <code>secretGenerator</code> in <code>base/kustomization.yaml</code>.</p> <p>Note that this example provides both <code>ldap_cacert_secret</code> (should have <code>ldap-ca.crt</code>) and <code>bundle_cacert_secret</code> (should have <code>bundle-ca.crt</code>), but you can remove unnecessary line if you don't need both of them.</p> <pre><code>...\nspec:\n  ...\n  ldap_cacert_secret: awx-custom-certs     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  bundle_cacert_secret: awx-custom-certs     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  ...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#apply-configuration","title":"Apply configuration","text":"<p>Invoke <code>apply</code> command. This will start re-deployment of your AWX.</p> <pre><code>kubectl apply -k base\n</code></pre> <p>You can monitor the progress of the re-deployment by following command:</p> <pre><code>kubectl -n awx logs -f deployments/awx-operator-controller-manager\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/trust-custom-ca/#troubleshooting","title":"Troubleshooting","text":"<p>If you have problem with SSL connection such as LDAPS, you can verify your certificates inside the pod.</p> <pre><code># Open Bash shell of the \"awx-web\" container\n$ kubectl -n awx exec -it deployment/awx -c awx-web -- bash\nbash-5.1$\n</code></pre> <p>First of all, you should ensure your CA certificate is mounted and has PEM format. The certificate should be be dumped as readable plain text by following command, without any error.</p> <pre><code># The secret ldap_cacert_secret is mounted as /etc/openldap/certs/ldap-ca.crt\nbash-5.1$ openssl x509 -in /etc/openldap/certs/ldap-ca.crt -text\n\n# The secret bundle_cacert_secret is mounted as /etc/pki/ca-trust/source/anchors/bundle-ca.crt\nbash-5.1$ openssl x509 -in /etc/pki/ca-trust/source/anchors/bundle-ca.crt\n</code></pre> <p>Note that your certificate file should contain both intermediate CA and root CA, if your server certificate is signed by intermediate CA.</p> <pre><code># Example output of concatenated CA cert; one for intermediate CA, one for root CA\nbash-5.1$ cat /etc/openldap/certs/ldap-ca.crt\n-----BEGIN CERTIFICATE-----\nMIIDizCCAnOgAwIBAgIUftINZYmeHvcovY0qBHp+SqZWrlswDQYJKoZIhvcNAQEL\n...\n3Eyhv0l7mJw/86twDMFFax+cKOCRFV6NoPOpzK1mzAXmxth6vk8DeRm0ipVpQVQ=\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\nMIIDizCCAnOgAwIBAgIUftINZYmeHvcovY0qBHp+SqZWrlwwDQYJKoZIhvcNAQEL\n...\nlVsDxZfbZVpRGkDr8odNurNmz0Xcttr+ZVRkoTy5KUxqIZhQuS6ySJj7yoLawWY=\n-----END CERTIFICATE-----\n</code></pre> <p>Now you can test SSL connection.</p> <pre><code># This is an example to test connection to LDAP server over SSL using /etc/openldap/certs/ldap-ca.crt\nbash-5.1$ openssl s_client -connect ldap.example.com:636 -no-CAfile -CAfile /etc/openldap/certs/ldap-ca.crt\nCONNECTED(00000003)\ndepth=2 C = JP, ST = Example State, O = EXAMPLE.COM, CN = rca.example.com\nverify return:1\ndepth=1 C = JP, ST = Example State, O = EXAMPLE.COM, CN = ica.example.com\nverify return:1\ndepth=0 C = JP, ST = Example State, O = EXAMPLE.COM, CN = ldap.example.com\nverify return:1\n---\nCertificate chain     \ud83d\udc48\ud83d\udc48\ud83d\udc48 Ensure that the full certificate chain is recognized\n 0 s:C = JP, ST = Example State, O = EXAMPLE.COM, CN = ldap.example.com\n   i:C = JP, ST = Example State, O = EXAMPLE.COM, CN = ica.example.com\n   ...\n 1 s:C = JP, ST = Example State, O = EXAMPLE.COM, CN = ica.example.com\n   i:C = JP, ST = Example State, O = EXAMPLE.COM, CN = rca.example.com\n   ...\n 2 s:C = JP, ST = Example State, O = EXAMPLE.COM, CN = rca.example.com\n   i:C = JP, ST = Example State, O = EXAMPLE.COM, CN = rca.example.com\n   ...\n---\n...\n---\nSSL handshake has read 3210 bytes and written 413 bytes\nVerification: OK     \ud83d\udc48\ud83d\udc48\ud83d\udc48 Ensure there is no verification error\n---\n...\nSSL-Session:\n    ...\n    Verify return code: 0 (ok)     \ud83d\udc48\ud83d\udc48\ud83d\udc48 Ensure there is no verification error\n    ...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/","title":"Upgrade operator","text":""},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#upgrade-awx-operator-and-awx","title":"Upgrade AWX Operator and AWX","text":"<p>This guide provides the procedure for the following three types of upgrading AWX Operator.</p> <ul> <li>Upgrade from <code>0.14.0</code> or later (e.g. from <code>0.14.0</code> to <code>0.15.0</code>)</li> <li>Upgrade from <code>0.13.0</code> (e.g. from <code>0.13.0</code> to <code>0.14.0</code>)</li> <li>Upgrade from <code>0.12.0</code> or earlier (e.g. from <code>0.12.0</code> to <code>0.13.0</code>)</li> </ul> <p>Note that once you upgrade AWX Operator, your AWX will also be upgraded automatically to the version bundled with the upgraded AWX Operator as shown in \ud83d\udcddVersion Mapping between AWX Operator and AWX.</p> <p>There is <code>image_version</code> parameter for AWX resource to change which image will be used, but it appears that using a version of AWX other than the one bundled with the AWX Operator is currently not supported. Conversely, if you want to upgrade AWX, you need to plan to upgrade AWX Operator first.</p>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#table-of-contents","title":"Table of Contents","text":"<ul> <li>\u2705 Take a backup of the old AWX instance</li> <li>\ud83d\udcdd Upgrade from <code>0.14.0</code> or later (e.g. from <code>0.14.0</code> to <code>0.15.0</code>)</li> <li>\ud83d\udcdd Upgrade from <code>0.13.0</code> (e.g. from <code>0.13.0</code> to <code>0.14.0</code>)</li> <li>\ud83d\udcdd Upgrade from <code>0.12.0</code> or earlier (e.g. from <code>0.12.0</code> to <code>0.13.0</code>)</li> <li>\u2753 Troubleshooting</li> <li>New Pod gets stuck in <code>Pending</code> state</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#take-a-backup-of-the-old-awx-instance","title":"\u2705 Take a backup of the old AWX instance","text":"<p>Before performing the upgrade, make sure that you have a backup of your old AWX.</p> <p>Refer \ud83d\udcddREADME: Backing up using AWX Operator to take backup using AWX Operator.</p>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#upgrade-from-0140-or-later-eg-from-0140-to-0150","title":"\ud83d\udcdd Upgrade from <code>0.14.0</code> or later (e.g. from <code>0.14.0</code> to <code>0.15.0</code>)","text":"<p>If you are using AWX Operator <code>0.14.0</code> or later and want to upgrade to newer version, basically upgrade is done by deploying the new version of AWX Operator to the same namespace where the old AWX Operator is running.</p> <p>Note that only when upgrading from <code>0.25.0</code> or earlier to <code>0.26.0</code> or later, since the bundled PostgreSQL version will be changed to 13, so the following additional tasks are required.</p> <pre><code># Required only when upgrading from 0.25.0 or earlier to 0.26.0 or later\nsudo mkdir -p /data/postgres-13\nsudo chmod 755 /data/postgres-13\ncat &lt;&lt;EOF &gt; pv-postgres-13.yaml\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: awx-postgres-13-volume\nspec:\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  capacity:\n    storage: 8Gi\n  storageClassName: awx-postgres-volume\n  hostPath:\n    path: /data/postgres-13\nEOF\nkubectl apply -f pv-postgres-13.yaml\n</code></pre> <p>To upgrade your AWX Operator, perform following steps.</p> <pre><code># Prepare required files\ncd ~\ngit clone https://github.com/ansible/awx-operator.git\ncd awx-operator\ngit checkout 0.15.0  # Checkout the version to upgrade to\n\n# Deploy AWX Operator\nexport NAMESPACE=awx  # Specify the namespace where the old AWX Operator exists\nmake deploy\n</code></pre> <p>This will upgrade the AWX Operator first, after that, AWX will be also upgraded as well.</p> <p>To monitor the progress of the deployment, check the logs of <code>deployments/awx-operator-controller-manager</code>:</p> <pre><code>kubectl -n awx logs -f deployments/awx-operator-controller-manager -c awx-manager\n</code></pre> <p>When the deployment completes successfully, the logs end with:</p> <pre><code>$ kubectl -n awx logs -f deployments/awx-operator-controller-manager -c awx-manager\n...\n----- Ansible Task Status Event StdOut (awx.ansible.com/v1beta1, Kind=AWX, awx/awx) -----\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=56   changed=0    unreachable=0    failed=0    skipped=35   rescued=0    ignored=0\n</code></pre> <p>If your AWX Operator has upgraded from <code>0.25.0</code> or earlier to <code>0.26.0</code> or later, old PV for PostgreSQL 12 can be removed since new AWX is running with new PV for PostgreSQL 13.</p> <pre><code># Recommended only when upgraded from 0.25.0 or earlier to 0.26.0 or later\nkubectl delete pv awx-postgres-volume\nsudo rm -rf /data/postgres\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#upgrade-from-0130-eg-from-0130-to-0140","title":"\ud83d\udcdd Upgrade from <code>0.13.0</code> (e.g. from <code>0.13.0</code> to <code>0.14.0</code>)","text":"<p>If you are using AWX Operator <code>0.13.0</code> and want to upgrade to newer version, you should consider the big changes in AWX Operator in <code>0.14.0</code>. As described in the documentation, in <code>0.14.0</code>, AWX Operator changed from cluster scope to namespace scope. Also, the Operator SDK <code>1.x</code> is used.</p> <p>This means that upgrading from <code>0.13.0</code> to <code>0.14.0</code> or later requires a bit of finesse, such as cleaning the old AWX Operator. If you are using <code>0.12.0</code> or earlier and want to upgrade to <code>0.14.0</code> or later, I recommend you to upgrade to <code>0.13.0</code> first and then come back to here to avoid unintended issue.</p> <p>In this guide, for example, perform upgrading from <code>0.13.0</code> to <code>0.14.0</code>. The AWX Operator <code>0.13.0</code> or earlier resides in the <code>default</code> namespace by default and the related AWX instance resides in the <code>awx</code> namespace, as described in this repository. After the upgrade, everything related to the AWX Operator <code>0.14.0</code> will reside in the <code>awx</code> namespace.</p> Phase AWX Operator AWX Instance Before Upgrade <code>0.13.0</code> in <code>default</code> namespace <code>19.3.0</code> in <code>awx</code> namespace After Upgrade <code>0.14.0</code> in <code>awx</code> namespace <code>19.4.0</code> in <code>awx</code> namespace <p>To upgrade AWX Operator, remove the old AWX Operator that is running in the <code>default</code> namespace first. In addition, remove Service Account, Cluster Role, and Cluster Role Binding that are required for old AWX Operator to work.</p> <pre><code>kubectl -n default delete deployment awx-operator\nkubectl -n default delete serviceaccount awx-operator\nkubectl -n default delete clusterrolebinding awx-operator\nkubectl -n default delete clusterrole awx-operator\n</code></pre> <p>Since we removed only old AWX Operator, the old CRDs are still exist. Therefore, the old <code>awx</code> resource which means old AWX instance is still running in the <code>awx</code> namespace.</p> <p>Finally, deploy the new AWX Operator to the <code>awx</code> namespace.</p> <pre><code># Prepare required files\ncd ~\ngit clone https://github.com/ansible/awx-operator.git\ncd awx-operator\ngit checkout 0.14.0  # Checkout the version to upgrade to\n\n# Deploy AWX Operator\nexport NAMESPACE=awx  # Specify the namespace where the old AWX instance exists\nmake deploy\n</code></pre> <p>This will update the CRDs in the cluster and create the required Service Account, Roles, etc. in the <code>awx</code> namespace. Also, AWX Operator will start working. Once AWX Operator is up and running, it will start rolling out a new version of the AWX instance automatically based on the old <code>awx</code> resource definition.</p> <p>To monitor the progress of the deployment, check the logs of <code>deployments/awx-operator-controller-manager</code>:</p> <pre><code>kubectl -n awx logs -f deployments/awx-operator-controller-manager -c awx-manager\n</code></pre> <p>When the deployment completes successfully, the logs end with:</p> <pre><code>$ kubectl -n awx logs -f deployments/awx-operator-controller-manager -c awx-manager\n...\n----- Ansible Task Status Event StdOut (awx.ansible.com/v1beta1, Kind=AWX, awx/awx) -----\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=56   changed=0    unreachable=0    failed=0    skipped=35   rescued=0    ignored=0\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#upgrade-from-0120-or-earlier-eg-from-0120-to-0130","title":"\ud83d\udcdd Upgrade from <code>0.12.0</code> or earlier (e.g. from <code>0.12.0</code> to <code>0.13.0</code>)","text":"<p>If you are using <code>0.12.0</code> or earlier and want to upgrade to newer version, simply, deploy the new version of AWX Operator. This procedure can be applicable for upgrading to up to <code>0.13.0</code>. If you want to upgrade to <code>0.14.0</code> or later, I recommend you to upgrade to <code>0.13.0</code> by following this procedure first and then perform upgrading to <code>0.14.0</code> or later.</p> <pre><code># Specify the version to upgrade to in the URL\nkubectl apply -f https://raw.githubusercontent.com/ansible/awx-operator/0.13.0/deploy/awx-operator.yaml\n</code></pre> <p>This will upgrade the AWX Operator first, after that, AWX will be also upgraded as well.</p> <p>To monitor the progress of the deployment, check the logs of <code>deployment/awx-operator</code>:</p> <pre><code>kubectl logs -f deployment/awx-operator\n</code></pre> <p>When the deployment completes successfully, the logs end with:</p> <pre><code>$ kubectl logs -f deployment/awx-operator\n...\n--------------------------- Ansible Task Status Event StdOut  -----------------\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=54   changed=0    unreachable=0    failed=0    skipped=37   rescued=0    ignored=0 \n-------------------------------------------------------------------------------\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#troubleshooting","title":"\u2753 Troubleshooting","text":"<p>Some hints for when you got stuck during upgrade.</p>"},{"location":"admin-guide/runbooks/awx/tips/upgrade-operator/#new-pod-gets-stuck-in-pending-state","title":"New Pod gets stuck in <code>Pending</code> state","text":"<p>If the K3s node does not have enough free resources to deploy a new AWX instance, the new Pod for AWX gets stuck in <code>Pending</code> state.</p> <pre><code>$ kubectl -n awx get pods\nNAME                                               READY   STATUS    RESTARTS   AGE\nawx-7d74496d7d-d66dw                               4/4     Running   0          19d\nawx-84d5c45999-55gb4                               0/4     Pending   0          10s     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Try running <code>kubectl -n awx describe pod &lt;Pod Name&gt;</code> and check the <code>Events</code> section at the end for the cause.</p> <pre><code>$ kubectl -n awx describe pod awx-84d5c45999-55gb4\n...\nEvents:\n  Type     Reason            Age   From               Message\n  ----     ------            ----  ----               -------\n  Warning  FailedScheduling  106s  default-scheduler  0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory.     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n  Warning  FailedScheduling  105s  default-scheduler  0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory.     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>This means that the node does not have enough CPU or memory resources to start the Pod.</p> <p>During the AWX upgrade, a rollout of the Deployment resource will be performed and temporarily two AWX Pods will be running. This means that the required Resource Requests for CPU and memory will be doubled.</p> <p>For this reason, if we do not have enough free resources on our K3s node, we can manually delete the old AWX instance beforehand in order to free up resources. Note that the rollout history will be lost with this step.</p> <pre><code>$ kubectl -n awx delete deployment awx\ndeployment.apps \"awx\" deleted\n\n$ kubectl -n awx get all\nNAME                    READY   STATUS    RESTARTS   AGE\npod/awx-postgres-13-0   1/1     Running   0          8m57s\n\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/awx-postgres   ClusterIP   None            &lt;none&gt;        5432/TCP   8m57s\nservice/awx-service    ClusterIP   10.43.248.150   &lt;none&gt;        80/TCP     8m51s\n\nNAME                            READY   AGE\nstatefulset.apps/awx-postgres   1/1     8m58s\n</code></pre> <p>Ensure that it is not the <code>awx</code> resource that should be deleted, but the <code>deployment</code> resource. If we accidentally delete the <code>awx</code> resource or any Secrets, we will not be able to upgrade successfully.</p> <p>After a few minutes of waiting, our AWX Operator will successfully launch the new Deployment and the Pod for AWX.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/","title":"Use kerberos","text":""},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#use-kerberos-authentication-to-connect-to-windows-hosts","title":"Use Kerberos authentication to connect to Windows hosts","text":"<p>If you use Active Directory users to run job templates against Windows hosts that are domain members, Kerberos authentication is usually required for WinRM.</p> <p>There is an official documentation to use Kerberos authentication in Ansible Automation Controller, but it is not suitable for AWX.</p> <p>This page shows you how to use Kerberos authentication for running job templates in AWX.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Example environment for this guide</li> <li>Procedure</li> <li>Setting up your Windows host<ul> <li>Enable WinRM</li> <li>Configure group or permissions for the domain user</li> </ul> </li> <li>Setting up Kubernetes<ul> <li>Create <code>krb5.conf</code></li> <li>Create ConfigMap for <code>krb5.conf</code></li> </ul> </li> <li>Setting up AWX<ul> <li>Create Container Group</li> <li>Create Credential</li> <li>Create Inventory</li> <li>Configure Job Template</li> </ul> </li> <li>Testing</li> <li>Troubleshooting</li> <li>Playbook for investigation</li> <li>Ensure your <code>krb5.conf</code> is mounted</li> <li>Ensure your KDC and target host are accessible from the EE</li> <li>Ensure <code>kinit</code> can be succeeded manually</li> <li>Gather trace logs for your playbook</li> <li>Common issues and workarounds<ul> <li>Error creating pod</li> <li>kinit: Cannot find KDC for realm \"\\&lt;DOMAINNAME&gt;\" while getting initial credentials</li> <li>kerberos: the specified credentials were rejected by the server</li> <li>kerberos: Access is denied. Bad HTTP response returned from server. Code 500</li> </ul> </li> <li>Alternative solution (not recommended)</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#example-environment-for-this-guide","title":"Example environment for this guide","text":"<p>This is my example environment for this guide. Replace these values in this guide as appropriate for your environment.</p> Key Value Domain <code>kurokobo.internal</code> Domain Controller <code>kuro-ad01.kurokobo.internal</code> KDC Server <code>kuro-ad01.kurokobo.internal</code> Ansible Target Host <code>kuro-win01.kurokobo.internal</code> Ansible User <code>awx@kurokobo.internal</code>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#procedure","title":"Procedure","text":"<p>To use Kerberos authentication in your AWX, following tasks are required.</p> <ol> <li>Setting up your Windows host</li> <li>Enable WinRM that uses Kerberos authentication</li> <li>Allow specific domain user to connect via WinRM</li> <li>Setting up your Kubernetes</li> <li>Create <code>krb5.conf</code> as ConfigMap on your Kubernetes cluster</li> <li>Setting up your AWX</li> <li>Create Container Group with custom pod spec that mounts <code>krb5.conf</code> to make Kerberos authentication to be used in your EE</li> <li>Create Credential for the domain user</li> <li>Create Inventory for the Windows hosts</li> <li>Configure your Job Template</li> </ol>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#setting-up-your-windows-host","title":"Setting up your Windows host","text":"<p>Enable WinRM on your Windows host and allow specific domain user to connect to your host via WinRM.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#enable-winrm","title":"Enable WinRM","text":"<p>Refer the Ansible documentation and enable WinRM on your Windows host. Running <code>winrm quickconfig</code> on your Windows host is the simplest way, but GPO can also be used to enable WinRM.</p> <p>Ensure that your WinRM Listener is enabled for HTTP. Note that HTTP is safe to use with Kerberos authentication since Kerberos has its own encryption method and all messages will be encrypted over HTTP. Therefore WinRM Listener for HTTPS is not mandatory.</p> <pre><code>&gt; winrm enumerate winrm/config/Listener\nListener\n    Address = *\n    Transport = HTTP\n    Port = 5985\n    Hostname\n    Enabled = true\n    URLPrefix = wsman\n    CertificateThumbprint\n    ListeningOn = 127.0.0.1, ...\n</code></pre> <p>Then ensure Kerberos authentication is enabled for WinRM.</p> <pre><code>&gt; winrm get winrm/config/Service\nService\n    RootSDDL = ...\n    MaxConcurrentOperations = 4294967295\n    MaxConcurrentOperationsPerUser = 1500\n    EnumerationTimeoutms = 240000\n    MaxConnections = 300\n    MaxPacketRetrievalTimeSeconds = 120\n    AllowUnencrypted = false\n    Auth\n        Basic = true\n        Kerberos = true     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n        Negotiate = true\n        Certificate = false\n        CredSSP = false\n        CbtHardeningLevel = Relaxed\n    DefaultPorts\n        HTTP = 5985\n        HTTPS = 5986\n    IPv4Filter = *\n    IPv6Filter = *\n    EnableCompatibilityHttpListener = false\n    EnableCompatibilityHttpsListener = false\n    CertificateThumbprint\n    AllowRemoteAccess = true\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#configure-group-or-permissions-for-the-domain-user","title":"Configure group or permissions for the domain user","text":"<p>Additionally, you have to care about the group or permissions of the domain user that used for WinRM.</p> <p>WinRM is configured by default to only allow connections by users in the local <code>Administrators</code> group. In this guide, a domain user <code>awx@kurokobo.internal</code> will be used to connect to Windows hosts via WinRM.</p> <p>Therefore this user have to be joined local <code>Administrators</code>, or have permissions for <code>Read</code> and <code>Execute</code> for WinRM. Default permissions for WinRM can be changed by <code>winrm configSDDL default</code>. Refer the Ansible documentation for more detail.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#setting-up-kubernetes","title":"Setting up Kubernetes","text":"<p>Create <code>krb5.conf</code> and add it as ConfigMap to your Kubernetes cluster.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#create-krb5conf","title":"Create <code>krb5.conf</code>","text":"<p>Create new file <code>krb5.conf</code> on the host that <code>kubectl</code> for your Kubernetes cluster can be used. This file will be added as ConfigMap in your Kubernetes cluster in the later step.</p> <p>There are some official documentation about <code>krb5.conf</code>:</p> <ul> <li>Ansible documentation</li> <li>Windows Remote Management - Configuring Host Kerberos</li> <li>Ansible Automation Controller documentation</li> <li>23. User Authentication with Kerberos</li> </ul> <p>This is my example. Note that some domain names under <code>[realms]</code> and <code>[domain_realm]</code> are capitalized.</p> <pre><code>[libdefaults]\n  dns_lookup_realm = false\n  dns_lookup_kdc = false\n  rdns = false\n\n[realms]\n  KUROKOBO.INTERNAL = {\n    kdc = kuro-ad01.kurokobo.internal\n    admin_server = kuro-ad01.kurokobo.internal\n  }\n\n[domain_realm]\n  .kurokobo.internal = KUROKOBO.INTERNAL\n  kurokobo.internal = KUROKOBO.INTERNAL\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#create-configmap-for-krb5conf","title":"Create ConfigMap for <code>krb5.conf</code>","text":"<p>Create new ConfigMap on your Kubernetes cluster using <code>krb5.conf</code> that you have created.</p> <pre><code>kubectl -n awx create configmap awx-kerberos-config --from-file=krb5.conf\n</code></pre> <p>This command creates new ConfigMap called <code>awx-kerberos-config</code> in the namespace <code>awx</code>. Specify the path to your <code>krb5.conf</code> as <code>--from-file</code>.</p> <p>Note that the namespace has to be the name of the namespace that your EE will be launched on. If you've deployed your AWX using to my guide, the namespace is <code>awx</code>.</p> <p>Ensure new ConfigMap is created on your Kubernetes cluster.</p> <pre><code>$ kubectl -n awx get configmap awx-kerberos-config -o yaml\napiVersion: v1\ndata:\n  krb5.conf: |-\n    [libdefaults]\n      dns_lookup_realm = false\n      dns_lookup_kdc = false\n      rdns = false\n\n    [realms]\n      KUROKOBO.INTERNAL = {\n        kdc = kuro-ad01.kurokobo.internal\n        admin_server = kuro-ad01.kurokobo.internal\n      }\n\n    [domain_realm]\n      .kurokobo.internal = KUROKOBO.INTERNAL\n      kurokobo.internal = KUROKOBO.INTERNAL\nkind: ConfigMap\nmetadata:\n  ...\n  name: awx-kerberos-config\n  namespace: awx\n  ...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#setting-up-awx","title":"Setting up AWX","text":"<p>Create new Container Group, Credential, and Inventory in your AWX.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#create-container-group","title":"Create Container Group","text":"<p>Create Container Group with custom pod spec that mounts <code>krb5.conf</code> to make Kerberos authentication to be used in your EE.</p> <ol> <li>Open AWX UI and open <code>Instance Groups</code> under <code>Administration</code>, then press <code>Add</code> &gt; <code>Add container group</code>.</li> <li>Enter <code>Name</code> as you like (e.g. <code>kerberos</code>) and toggle <code>Customize pod specification</code>.</li> <li>Put following YAML string to <code>Custom pod spec</code> and press <code>Save</code></li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: awx\nspec:\n  serviceAccountName: default\n  automountServiceAccountToken: false\n  containers:\n    - image: 'quay.io/ansible/awx-ee:latest'\n      name: worker\n      args:\n        - ansible-runner\n        - worker\n        - '--private-data-dir=/runner'\n      resources:\n        requests:\n          cpu: 250m\n          memory: 100Mi\n      volumeMounts:\n        - name: awx-kerberos-volume\n          mountPath: /etc/krb5.conf\n          subPath: krb5.conf\n  volumes:\n    - name: awx-kerberos-volume\n      configMap:\n        name: awx-kerberos-config\n</code></pre> <p>This pod spec means that your ConfigMap including your <code>krb5.conf</code> will be mounted as <code>/etc/krb5.conf</code> in the EE based on this Container Group. Of course this pod spec is just a working example and you can modify this to suit your requirements. Refer my guide for Container Group for detail.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#create-credential","title":"Create Credential","text":"<p>Create new Credential for your domain user. In this guide, the domain user <code>awx@kurokobo.internal</code> will be used to connect to Windows hosts via WinRM, so the Credential for <code>awx@kurokobo.internal</code> have to be created.</p> <ol> <li>Open AWX UI and open <code>Credentials</code> under <code>Resources</code>, then press <code>Add</code>.</li> <li>Enter <code>Name</code> as you like (e.g. <code>Domain User</code>) and select <code>Machine</code> as <code>Credential Type</code>.</li> <li>Enter <code>Username</code> and <code>Password</code> for your domain user in <code>&lt;username&gt;@&lt;DOMAINNAME&gt;</code> format. Note that the domain name have to be capitalized like <code>awx@KUROKOBO.INTERNAL</code>.</li> </ol> <p>It's very important that the domain name in the <code>Username</code> have to be capitalized, since this username will be passed as-is to <code>kinit</code>. If you specify your domain name in lower-case, <code>kinit</code> will fail because <code>kinit</code> cannot find KDC for realm in lower-case.</p> <p>Alternatively the name of the realm can be passed through <code>ansible_winrm_realm</code> variable, but my recommendation is specify realm in Credential as the part of <code>Username</code> in upper-case.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#create-inventory","title":"Create Inventory","text":"<p>Create Inventory in the standard way. The important points are as follows.</p> <ul> <li>The name of the <code>Host</code> in your Inventory have to be specified as FQDN, like <code>kuro-win01.kurokobo.internal</code>, instead of IP address. This is mandatory requirement.</li> <li>You should add following variables in the Inventory as host variables or group variables.</li> </ul> <pre><code>---\nansible_connection: winrm\nansible_winrm_transport: kerberos\nansible_port: 5985\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#configure-job-template","title":"Configure Job Template","text":"<p>Create or configure your Job Template. The important points are as follows.</p> <ul> <li>Specify Inventory that includes your Windows hosts in FQDN.</li> <li>Specify Credential that includes the username in <code>&lt;username&gt;@&lt;DOMAINNAME&gt;</code> format. The domain name in the username have to be capitalized.</li> <li>Specify Instance Group that has custom pod spec that mounts your ConfigMap as <code>/etc/krb5.conf</code>.</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#testing","title":"Testing","text":"<p>You can test connectivity via WinRM using Kerberos by using <code>ansible.windows.win_ping</code> module. This is an example playbook.</p> <pre><code>---\n- name: Test Kerberos Authentication\n  hosts: kuro-win01.kurokobo.internal\n  gather_facts: false\n  tasks:\n\n    - name: Ensure windows host is reachable\n      ansible.windows.win_ping:\n</code></pre> <p>If the <code>Verbosity</code> for the Job Template is configured <code>4 (Connection Debug)</code> and if your Kerberos authentication is successfully established, the job ends with success and the log shows that <code>kinit</code> is called to connect to Windows hosts.</p> <pre><code>TASK [Ensure windows host is reachable] ****************************************\n...\n&lt;kuro-win01.kurokobo.internal&gt; ESTABLISH WINRM CONNECTION FOR USER: awx@KUROKOBO.INTERNAL on PORT 5985 TO kuro-win01.kurokobo.internal\ncalling kinit with pexpect for principal awx@KUROKOBO.INTERNAL     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n...\nok: [kuro-win01.kurokobo.internal] =&gt; {\n    \"changed\": false,\n    \"invocation\": {\n        \"module_args\": {\n            \"data\": \"pong\"\n        }\n    },\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#troubleshooting","title":"Troubleshooting","text":"<p>The Kerberos authentication including <code>kinit</code> will be invoked on every Job Template running. This means that <code>kinit</code> will be invoked in EE, so if we want to investigate Kerberos related issues, we have to dig into the EE.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#playbook-for-investigation","title":"Playbook for investigation","text":"<p>Run this playbook as a Job on the Container Group.</p> <pre><code>---\n- name: Debug Kerberos Authentication\n  hosts: localhost\n  gather_facts: false\n  tasks:\n\n    - name: Ensure /etc/krb5.conf is mounted\n      ansible.builtin.debug:\n        msg: \"{{ lookup( 'file', '/etc/krb5.conf' ) }}\"\n\n    - name: Pause for specified minutes for debugging\n      ansible.builtin.pause:\n        minutes: 10\n</code></pre> <p>You can dig into the EE during <code>ansible.builtin.pause</code> is working by following commands.</p> <ol> <li>Launch the Job with the playbook above.</li> <li>If your Job exits with <code>Failed</code> immediately, your custom pod spec for your Container Group or ConfigMap for your <code>krb5.conf</code> might be wrong.</li> <li>Invoke <code>kubectl -n &lt;namespace&gt; get pod</code> on your Kubernetes cluster</li> <li>Gather the pod name that starts with <code>automation-job-*</code></li> </ol> <pre><code>$ kubectl -n awx get pod\nNAME                                               READY   STATUS    RESTARTS   AGE\nawx-postgres-0                                     1/1     Running   0          41h\nawx-76445c946f-btfzz                               4/4     Running   0          41h\nawx-operator-controller-manager-7594795b6b-565wm   2/2     Running   0          41h\nautomation-job-42-tdvs5                            1/1     Running   0          4s     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Now you can access <code>bash</code> inside the EE by <code>kubectl -n &lt;namespace&gt; exec -it &lt;pod name&gt; -- bash</code>:</p> <pre><code>$ kubectl -n awx exec -it automation-job-42-tdvs5 -- bash\nbash-5.1$     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n</code></pre> <p>Then proceed investigation.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#ensure-your-krb5conf-is-mounted","title":"Ensure your <code>krb5.conf</code> is mounted","text":"<p>If your Container Group and ConfigMap are configured correctly, you can get your <code>krb5.conf</code> as <code>/etc/krb5.conf</code> inside the EE.</p> <pre><code>bash-5.1$ cat /etc/krb5.conf\n[libdefaults]\n  dns_lookup_realm = false\n  dns_lookup_kdc = false\n  rdns = false\n\n[realms]\n  KUROKOBO.INTERNAL = {\n    kdc = kuro-ad01.kurokobo.internal\n    admin_server = kuro-ad01.kurokobo.internal\n  }\n\n[domain_realm]\n  .kurokobo.internal = KUROKOBO.INTERNAL\n  kurokobo.internal = KUROKOBO.INTERNAL\n</code></pre> <p>If your <code>krb5.conf</code> is missing, ensure your custom pod spec for Container Group and ConfigMap for your <code>krb5.conf</code> are correct.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#ensure-your-kdc-and-target-host-are-accessible-from-the-ee","title":"Ensure your KDC and target host are accessible from the EE","text":"<p>Ensure your KDC server can be reachable. There is no command such as <code>ping</code> or <code>nslookup</code>, downloading and using  Busybox is helpful.</p> <pre><code># Download busybox and make it executable\nbash-5.1$ curl -o busybox https://busybox.net/downloads/binaries/1.35.0-x86_64-linux-musl/busybox\nbash-5.1$ chmod +x busybox\n</code></pre> <p>Then test name resolution and network reachability.</p> <pre><code># Ensure your domain name can be resolved\nbash-5.1$ ./busybox nslookup kurokobo.internal\nServer:         10.43.0.10\nAddress:        10.43.0.10:53\n\nName:   kurokobo.internal\nAddress: ***.***.***.***\n</code></pre> <pre><code># Ensure hostname of your KDC can be resolved\nbash-5.1$ ./busybox nslookup kuro-ad01.kurokobo.internal\nServer:         10.43.0.10\nAddress:        10.43.0.10:53\n\nName:   kuro-ad01.kurokobo.internal\nAddress: ***.***.***.***\n</code></pre> <pre><code># Ensure the port 88 on your KDC can be opened\nbash-5.1$ ./busybox nc -v -w 1 kuro-ad01.kurokobo.internal 88\nkuro-ad01.kurokobo.internal (***.***.***.***:88) open\n</code></pre> <pre><code># Ensure hostname of your target host can be resolved\nbash-5.1$ ./busybox nslookup kuro-win01.kurokobo.internal\nServer:         10.43.0.10\nAddress:        10.43.0.10:53\n\nName:   kuro-win01.kurokobo.internal\nAddress: ***.***.***.***\n\n# If you don't have \"rdns = false\" in your \"krb5.conf\",\n# and the IP address of your target host can be reverse looked up,\n# the result must match the hostname of your target host.\nbash-5.1$ ./busybox nslookup ***.***.***.***\nServer:         10.43.0.10\nAddress:        10.43.0.10:53\n\n***.***.***.***.in-addr.arpa      name = kuro-win01.kurokobo.internal\n</code></pre> <pre><code># Ensure the port 88 on your KDC is reachable\nbash-5.1$ ./busybox nc -v -w 1 kuro-ad01.kurokobo.internal 88\nkuro-ad01.kurokobo.internal ***.***.***.***:88) open\n</code></pre> <pre><code># Ensure the port 5985 (or 5986 for HTTPS) on your target host respond 404 error to HTTP request\nbash-5.1$ curl -I http://kuro-win01.kurokobo.internal:5985\nHTTP/1.1 404 Not Found\nContent-Length: 315\nContent-Type: text/html; charset=us-ascii\nServer: Microsoft-HTTPAPI/2.0\nDate: Wed, 15 Mar 2023 16:47:03 GMT\nConnection: close\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#ensure-kinit-can-be-succeeded-manually","title":"Ensure <code>kinit</code> can be succeeded manually","text":"<p>You can test Kerberos authentication by using <code>kinit</code> manually inside the EE.</p> <pre><code># Ensure that there is no error while passing &lt;username&gt;@&lt;DOMAINNAME&gt; and password\n# Note that the domain name for kinit have to be capitalized\nbash-5.1$ kinit awx@KUROKOBO.INTERNAL\nPassword for awx@KUROKOBO.INTERNAL:\n</code></pre> <pre><code># Ensure new ticket has been issued after kinit\nbash-5.1$ klist\nTicket cache: FILE:/tmp/krb5cc_1000\nDefault principal: awx@KUROKOBO.INTERNAL\n\nValid starting     Expires            Service principal\n07/02/22 12:32:28  07/02/22 22:32:28  krbtgt/KUROKOBO.INTERNAL@KUROKOBO.INTERNAL\n        renew until 07/03/22 12:32:21\n</code></pre> <p>If you have faced any errors, you can investigate by getting trace logs for <code>kinit</code> by exporting <code>KRB5_TRACE</code> environment variable.</p> <pre><code># The command to destroy cached tickets if exists\nbash-5.1$ kdestroy\n\n# Make trace logs to be displayed on stdout\nbash-5.1$ export KRB5_TRACE=/dev/stdout\n\n# Example trace logs for the case that the ticket has been issued successfully\nbash-5.1$ kinit awx@KUROKOBO.INTERNAL\n[38] 1678897813.575763: Getting initial credentials for awx@KUROKOBO.INTERNAL\n[38] 1678897813.575765: Sending unauthenticated request\n[38] 1678897813.575766: Sending request (190 bytes) to KUROKOBO.INTERNAL\n[38] 1678897813.575767: Resolving hostname kuro-ad01.kurokobo.internal\n[38] 1678897813.575768: Sending initial UDP request to dgram ***.***.***.***:88\n[38] 1678897813.575769: Received answer (198 bytes) from dgram ***.***.***.***:88\n[38] 1678897813.575770: Response was not from primary KDC\n[38] 1678897813.575771: Received error from KDC: -1765328359/Additional pre-authentication required\n[38] 1678897813.575774: Preauthenticating using KDC method data\n[38] 1678897813.575775: Processing preauth types: PA-PK-AS-REQ (16), PA-PK-AS-REP_OLD (15), PA-ETYPE-INFO2 (19), PA-ENC-TIMESTAMP (2)\n[38] 1678897813.575776: Selected etype info: etype aes256-cts, salt \"KUROKOBO.INTERNALawx\", params \"\"\n[38] 1678897813.575777: PKINIT client has no configured identity; giving up\n[38] 1678897813.575778: Preauth module pkinit (16) (real) returned: -1765328174/No pkinit_anchors supplied\nPassword for awx@KUROKOBO.INTERNAL:\n[38] 1678897823.104967: AS key obtained for encrypted timestamp: aes256-cts/1836\n[38] 1678897823.104969: Encrypted timestamp (for 1678897818.79834): plain 301AA...137DA, encrypted 303E8...E278E\n[38] 1678897823.104970: Preauth module encrypted_timestamp (2) (real) returned: 0/Success\n[38] 1678897823.104971: Produced preauth for next request: PA-ENC-TIMESTAMP (2)\n[38] 1678897823.104972: Sending request (270 bytes) to KUROKOBO.INTERNAL\n[38] 1678897823.104973: Resolving hostname kuro-ad01.kurokobo.internal\n[38] 1678897823.104974: Sending initial UDP request to dgram ***.***.***.***:88\n[38] 1678897823.104975: Received answer (106 bytes) from dgram ***.***.***.***:88\n[38] 1678897823.104976: Response was not from primary KDC\n[38] 1678897823.104977: Received error from KDC: -1765328332/Response too big for UDP, retry with TCP\n[38] 1678897823.104978: Request or response is too big for UDP; retrying with TCP\n[38] 1678897823.104979: Sending request (270 bytes) to KUROKOBO.INTERNAL (tcp only)\n[38] 1678897823.104980: Resolving hostname kuro-ad01.kurokobo.internal\n[38] 1678897823.104981: Initiating TCP connection to stream ***.***.***.***:88\n[38] 1678897823.104982: Sending TCP request to stream ***.***.***.***:88\n[38] 1678897823.104983: Received answer (1639 bytes) from stream ***.***.***.***:88\n[38] 1678897823.104984: Terminating TCP connection to stream ***.***.***.***:88\n[38] 1678897823.104985: Response was not from primary KDC\n[38] 1678897823.104986: Processing preauth types: PA-ETYPE-INFO2 (19)\n[38] 1678897823.104987: Selected etype info: etype aes256-cts, salt \"KUROKOBO.INTERNALawx\", params \"\"\n[38] 1678897823.104988: Produced preauth for next request: (empty)\n[38] 1678897823.104989: AS key determined by preauth: aes256-cts/1836\n[38] 1678897823.104990: Decrypted AS reply; session key is: aes256-cts/8188\n[38] 1678897823.104991: FAST negotiation: unavailable\n[38] 1678897823.104992: Resolving unique ccache of type MEMORY\n[38] 1678897823.104993: Initializing MEMORY:9h1LvJw with default princ awx@KUROKOBO.INTERNAL\n[38] 1678897823.104994: Storing config in MEMORY:9h1LvJw for krbtgt/KUROKOBO.INTERNAL@KUROKOBO.INTERNAL: pa_type: 2\n[38] 1678897823.104995: Storing awx@KUROKOBO.INTERNAL -&gt; krb5_ccache_conf_data/pa_type/krbtgt\\/KUROKOBO.INTERNAL\\@KUROKOBO.INTERNAL@X-CACHECONF: in MEMORY:9h1LvJw\n[38] 1678897823.104996: Storing awx@KUROKOBO.INTERNAL -&gt; krbtgt/KUROKOBO.INTERNAL@KUROKOBO.INTERNAL in MEMORY:9h1LvJw\n[38] 1678897823.104997: Moving ccache MEMORY:9h1LvJw to FILE:/tmp/krb5cc_1000\n[38] 1678897823.104998: Destroying ccache MEMORY:9h1LvJw\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#gather-trace-logs-for-your-playbook","title":"Gather trace logs for your playbook","text":"<p>If manually invoked <code>kinit</code> succeeds but the task in your playbook such as <code>ansible.windows.win_ping</code> fails, it may be possible to investigate the cause from the trace logs of the <code>kinit</code> that invoked internally during runtime of playbook.</p> <ol> <li>Append <code>env</code> to your custom pod spec for Container Group</li> </ol> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: awx\nspec:\n  ...\n  containers:\n    - image: 'quay.io/ansible/awx-ee:latest'\n      name: worker\n      env:     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n        - name: KRB5_TRACE     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n          value: /tmp/krb5.log     \ud83d\udc48\ud83d\udc48\ud83d\udc48\n      args:\n        - ansible-runner\n        - worker\n        - '--private-data-dir=/runner'\n      ...\n</code></pre> <ol> <li>Run playbook that contains following tasks</li> </ol> <pre><code>...\n  tasks:\n    - name: Ensure windows host is reachable\n      ansible.windows.win_ping:\n      ignore_unreachable: true\n\n    - name: Get /tmp/krb5.log\n      ansible.builtin.debug:\n        msg: \"{{ lookup( 'file', '/tmp/krb5.log' ) }}\"\n</code></pre> <p>You can get the contents of <code>/tmp/krb5.log</code> from your job output. Below is example with success. Note that <code>\\\\n</code> should be replaced with line break to make it readable.</p> <pre><code>...\nTASK [Ensure windows host is reachable] ****************************************\nok: [kuro-win01.kurokobo.internal]\n\nTASK [Get /tmp/krb5.log] *******************************************************\nok: [kuro-win01.kurokobo.internal] =&gt; {\n    \"msg\": \"[25] 1678899932.599110: Matching awx@KUROKOBO.INTERNAL in collection with result: 0/Success\n            [25] 1678899932.599111: Matching awx@KUROKOBO.INTERNAL in collection with result: 0/Success\n            [25] 1678899932.599112: Getting credentials awx@KUROKOBO.INTERNAL -&gt; HTTP/kuro-win01.kurokobo.internal@KUROKOBO.INTERNAL using ccache FILE:/tmp/tmprrrrpt1c\n            [25] 1678899932.599113: Retrieving awx@KUROKOBO.INTERNAL -&gt; krb5_ccache_conf_data/start_realm@X-CACHECONF: from FILE:/tmp/tmprrrrpt1c with result: -1765328243/Matching credential not found (filename: /tmp/tmprrrrpt1c)\n            [25] 1678899932.599114: Retrieving awx@KUROKOBO.INTERNAL -&gt; HTTP/kuro-win01.kurokobo.internal@KUROKOBO.INTERNAL from FILE:/tmp/tmprrrrpt1c with result: -1765328243/Matching credential not found (filename: /tmp/tmprrrrpt1c)\n            [25] 1678899932.599115: Retrieving awx@KUROKOBO.INTERNAL -&gt; krbtgt/KUROKOBO.INTERNAL@KUROKOBO.INTERNAL from FILE:/tmp/tmprrrrpt1c with result: 0/Success\n            [25] 1678899932.599116: Starting with TGT for client realm: awx@KUROKOBO.INTERNAL -&gt; krbtgt/KUROKOBO.INTERNAL@KUROKOBO.INTERNAL\n            [25] 1678899932.599117: Requesting tickets for HTTP/kuro-win01.kurokobo.internal@KUROKOBO.INTERNAL, referrals on\n            [25] 1678899932.599118: Generated subkey for TGS request: aes256-cts/7945\n            [25] 1678899932.599119: etypes requested in TGS request: aes256-cts, aes128-cts, aes256-sha2, aes128-sha2, rc4-hmac, camellia128-cts, camellia256-cts\n            [25] 1678899932.599121: Encoding request body and padata into FAST request\n            [25] 1678899932.599122: Sending request (1827 bytes) to KUROKOBO.INTERNAL\n            [25] 1678899932.599123: Resolving hostname kuro-ad01.kurokobo.internal\n            [25] 1678899932.599124: Initiating TCP connection to stream ***.***.***.***:88\n            [25] 1678899932.599125: Sending TCP request to stream ***.***.***.***:88\n            [25] 1678899932.599126: Received answer (1752 bytes) from stream ***.***.***.***:88\n            [25] 1678899932.599127: Terminating TCP connection to stream ***.***.***.***:88\n            [25] 1678899932.599128: Response was not from primary KDC\n            [25] 1678899932.599129: Decoding FAST response\n            [25] 1678899932.599130: FAST reply key: aes256-cts/DEB0\n            [25] 1678899932.599131: TGS reply is for awx@KUROKOBO.INTERNAL -&gt; HTTP/kuro-win01.kurokobo.internal@KUROKOBO.INTERNAL with session key aes256-cts/93B1\n            [25] 1678899932.599132: TGS request result: 0/Success\n            [25] 1678899932.599133: Received creds for desired service HTTP/kuro-win01.kurokobo.internal@KUROKOBO.INTERNAL\n            [25] 1678899932.599134: Storing awx@KUROKOBO.INTERNAL -&gt; HTTP/kuro-win01.kurokobo.internal@KUROKOBO.INTERNAL in FILE:/tmp/tmprrrrpt1c\n            [25] 1678899932.599135: Creating authenticator for awx@KUROKOBO.INTERNAL -&gt; HTTP/kuro-win01.kurokobo.internal@KUROKOBO.INTERNAL, seqnum 655929156, subkey aes256-cts/1F31, session key aes256-cts/93B1\n            [25] 1678899932.599137: Read AP-REP, time 1678899927.599136, subkey aes256-cts/8266, seqnum 1460012097\"\n}\n...\n</code></pre>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#common-issues-and-workarounds","title":"Common issues and workarounds","text":"<p>Some common issues during this guide and workaround for those errors.</p> <p>The following official documentations are also be helpful.</p> <ul> <li>\"Troubleshooting Kerberos\" section in Ansible documentation</li> <li>Red Hat's KB article for the error \"Server not found in Kerberos database\"</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#error-creating-pod","title":"Error creating pod","text":"<p>The job had been failed immediately after running the job. The log shows following.</p> <pre><code>Error creating pod: container failed with exit code 128: failed to create containerd task: ...\n</code></pre> <p>This is usually caused by misconfigured custom pod spec of your Container Group or ConfigMap for your <code>krb5.conf</code>.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#kinit-cannot-find-kdc-for-realm-domainname-while-getting-initial-credentials","title":"kinit: Cannot find KDC for realm \"\\&lt;DOMAINNAME&gt;\" while getting initial credentials","text":"<p><code>kinit</code> inside the EE or job failed with following error.</p> <pre><code>bash-5.1$ kinit &lt;username&gt;@&lt;DOMAINNAME&gt;\nkinit: Cannot find KDC for realm \"&lt;DOMAINNAME&gt;\" while getting initial credentials\n</code></pre> <pre><code>TASK [Ensure windows host is reachable] ****************************************\nfatal: [...]: UNREACHABLE! =&gt; {\n    \"changed\": false,\n    \"msg\": \"Kerberos auth failure for principal awx@kurokobo.internal with pexpect: Cannot find KDC for realm \\\"&lt;DOMAINNAME&gt;\\\" while getting initial credentials\",\n    \"unreachable\": true\n}\n</code></pre> <p>If this occurred, ensure:</p> <ul> <li><code>/etc/krb5.conf</code> is correctly configured</li> <li>Your KDC hostname can be resolved</li> <li>Your KDC can be accessed from EE</li> <li>The username for <code>kinit</code> is correct. Especially, note that the domain name in the username have to be capitalized like <code>awx@KUROKOBO.INTERNAL</code></li> <li>If manually invoked <code>kinit</code> is succeeded but <code>kinit</code> inside the job failed, ensure the username in your Credential in AWX is correct. Note that the domain name in the username have to be capitalized like <code>awx@KUROKOBO.INTERNAL</code></li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#kerberos-the-specified-credentials-were-rejected-by-the-server","title":"kerberos: the specified credentials were rejected by the server","text":"<p>The job failed with following error.</p> <pre><code>TASK [Ensure windows host is reachable] ****************************************\nfatal: [...]: UNREACHABLE! =&gt; {\n    \"changed\": false,\n    \"msg\": \"kerberos: the specified credentials were rejected by the server\",\n    \"unreachable\": true\n}\n</code></pre> <p>Ensure your domain user that used to connect to WinRM on the target host is the member of local <code>Administrators</code> group on the target host, or has permissions for <code>Read</code> and <code>Execute</code> for WinRM.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#kerberos-access-is-denied-bad-http-response-returned-from-server-code-500","title":"kerberos: Access is denied. Bad HTTP response returned from server. Code 500","text":"<p>The job failed with following error.</p> <pre><code>TASK [Ensure windows host is reachable] ****************************************\nfatal: [...]: UNREACHABLE! =&gt; {\n    \"changed\": false,\n    \"msg\": \"kerberos: Access is denied.  (extended fault data: {'transport_message': 'Bad HTTP response returned from server. Code 500', 'http_status_code': 500, 'wsmanfault_code': '5', 'fault_code': 's:Sender', 'fault_subcode': 'w:AccessDenied'})\",\n    \"unreachable\": true\n}\n</code></pre> <p>Ensure your domain user that used to connect to WinRM on the target host is the member of local <code>Administrators</code> group on the target host, or has permissions for <code>Read</code> and <code>Execute</code> for WinRM. In this case, <code>Execute</code> might be missing.</p>"},{"location":"admin-guide/runbooks/awx/tips/use-kerberos/#alternative-solution-not-recommended","title":"Alternative solution (not recommended)","text":"<p>To replace <code>/etc/krb5.conf</code> in EE with your customized <code>krb5.conf</code>, you can also use <code>AWX_ISOLATION_SHOW_PATHS</code> settings in AWX. This is a setting to expose any path on the host to EE. If this setting is activated, it's no longer required to create a Container Group on AWX or ConfigMap on Kubernetes, that described in this guide.</p> <p>However, this feature will internally mount <code>krb5.conf</code> via <code>hostPath</code>, so a customized <code>krb5.conf</code> must be placed on all Kubernetes nodes where the EE will run.</p> <p>Also, side-effects and security concerns must be taken into consideration, as all EE jobs running on AWX will mount <code>krb5.conf</code> via <code>hostPath</code>, weather the job is for Windows hosts or not.</p> <p>Therefore, I don't recommend this method in this guide.</p> <p>If you want to use this feature, you can do so by following these steps.</p> <ol> <li>Place your <code>krb5.conf</code> on any path on your Kubernetes node, e.g. <code>/data/kerberos/krb5.conf</code>, instead of creating a Container Group on AWX or ConfigMap on Kubernetes</li> <li>Enable <code>Expose host paths for Container Groups</code> in AWX under <code>Settings</code> &gt; <code>Job settings</code>.</li> <li>This equals to set <code>AWX_MOUNT_ISOLATED_PATHS_ON_K8S</code> to <code>true</code>.</li> <li>Add <code>/data/kerberos/krb5.conf:/etc/krb5.conf:O</code> to <code>Paths to expose to isolated jobs</code> in AWX under <code>Settings</code> &gt; <code>Job settings</code>.</li> <li>This equals to append string to <code>AWX_ISOLATION_SHOW_PATHS</code>.</li> <li>Run your Job Template that without any Container Group.</li> </ol>"},{"location":"admin-guide/runbooks/awx/tips/version-mapping/","title":"Version mapping","text":""},{"location":"admin-guide/runbooks/awx/tips/version-mapping/#version-mapping-between-awx-operator-and-awx","title":"Version Mapping between AWX Operator and AWX","text":"<ul> <li>Default version mapping between AWX Operator and AWX</li> <li>Appendix: Gather bundled AWX version from AWX Operator</li> </ul>"},{"location":"admin-guide/runbooks/awx/tips/version-mapping/#default-version-mapping-between-awx-operator-and-awx","title":"Default version mapping between AWX Operator and AWX","text":"<p>The table below maps the AWX Operator versions and bundled AWX versions.</p> AWX Operator AWX 1.3.0 21.13.0 1.2.0 21.12.0 1.1.4 21.11.0 1.1.3 21.10.2 1.1.2 21.10.1 1.1.1 21.10.0 1.1.0 21.9.0 1.0.0 21.8.0 0.30.0 21.7.0 0.29.0 21.6.0 0.28.0 21.5.0 0.27.0 21.5.0 0.26.0 21.4.0 0.25.0 21.3.0 0.24.0 21.3.0 0.23.0 21.2.0 0.22.0 21.1.0 0.21.0 21.0.0 0.20.2 21.0.0 0.20.1 21.0.0 0.20.0 20.1.0 0.19.0 20.0.1 0.18.0 20.0.1 0.17.0 20.0.0 0.16.1 19.5.1 0.16.0 19.5.1 0.15.0 19.5.0 0.14.0 19.4.0 0.13.0 19.3.0 0.12.0 19.2.2 0.11.0 19.2.1 0.10.0 19.2.0 0.9.0 19.1.0 0.8.0 19.0.0 0.7.0 18.0.0 0.6.0 15.0.0 <p>In the current version of AWX Operator, there is <code>image_version</code> parameter for AWX resource to change which image will be used, but it appears that using a version of AWX other than the one bundled with the AWX Operator is currently not supported.</p>"},{"location":"admin-guide/runbooks/awx/tips/version-mapping/#appendix-gather-bundled-awx-version-from-awx-operator","title":"Appendix: Gather bundled AWX version from AWX Operator","text":"<p>For AWX Operator 0.23.0 or later (AWX 21.2.0 or later), you can find the bundled version in the release notes on GitHub. See release notes for AWX Operator or release notes for AWX.</p> <p>If you want to get the bundled version for older versions or by means other than accessing the release notes, try following commands.</p> <ul> <li>For AWX Operator 0.15.0 or later</li> </ul> <pre><code># Using Docker\ndocker run -it --rm --entrypoint /usr/bin/bash quay.io/ansible/awx-operator:${OPERATOR_VERSION} -c env | grep DEFAULT_AWX_VERSION\n\n# Using Kubernetes\nkubectl run awx-operator --restart=Never -it --rm --command /usr/bin/bash --image=quay.io/ansible/awx-operator:${OPERATOR_VERSION} -- -c \"env\" | grep DEFAULT_AWX_VERSION\n</code></pre> <ul> <li>For AWX Operator 0.10.0 to 0.14.0</li> </ul> <pre><code>curl -sf https://raw.githubusercontent.com/ansible/awx-operator/${OPERATOR_VERSION}/roles/installer/defaults/main.yml | egrep \"^image_version:\"\n</code></pre> <ul> <li>For AWX Operator 0.9.0</li> </ul> <pre><code>curl -sf https://raw.githubusercontent.com/ansible/awx-operator/${OPERATOR_VERSION}/roles/installer/defaults/main.yml | egrep \"^tower_image_version:\"\n</code></pre> <ul> <li>For AWX Operator 0.7.0 and 0.8.0</li> </ul> <pre><code>curl -sf https://raw.githubusercontent.com/ansible/awx-operator/${OPERATOR_VERSION}/roles/installer/defaults/main.yml | egrep \"^tower_image:\"\n</code></pre> <ul> <li>For AWX Operator 0.6.0</li> </ul> <pre><code>curl -sf https://raw.githubusercontent.com/ansible/awx-operator/${OPERATOR_VERSION}/roles/awx/defaults/main.yml | egrep \"^tower_image:\"\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/","title":"BareMetalHost Provisioning Error Resolution","text":""},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#overview","title":"Overview","text":"<p>This troubleshooting guide addresses bare metal hosts that enter a 'provisioned registration error' state after Ubiquity cluster updates. This typically occurs when the image URL references become invalid due to IP address changes during the update process.</p>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#problem-description","title":"Problem Description","text":"<p>After updating a Ubiquity cluster, bare metal hosts may show as \"Provisioned\" in the status but actually be in an error state. The BareMetalHost resource will display an error message similar to:</p> <pre><code>status:\n  errorCount: 1\n  errorMessage: 'Host adoption failed: Error while attempting to adopt node 7a8d8aa7-e39d-48ec-98c1-ed05eacc354f:\n    Validation of image href http://10.10.10.10/images/ubiquity-node-rocky9.qcow2 failed,\n    reason: Got HTTP code 404 instead of 200 in response to HEAD request..'\n  errorType: provisioned registration error\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#root-cause","title":"Root Cause","text":"<p>This issue occurs when: - The Ubiquity cluster's internal networking configuration changes during updates - Image URLs contain hardcoded IP addresses instead of stable DNS names - The httpd-http service serving images becomes unreachable at the old endpoint</p>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#resolution-steps","title":"Resolution Steps","text":"<p>Follow these steps to resolve the provisioning error and restore proper BareMetalHost functionality:</p>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the Ubiquity cluster kubectl command line</li> <li>Administrative privileges on the cluster</li> <li>Knowledge of the affected BareMetalHost resource name</li> </ul> <p>Note: In the examples below, we update <code>master-2</code> as the BareMetalHost name. Replace this with your actual BareMetalHost resource name.</p>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-1-configure-cluster-access","title":"Step 1: Configure Cluster Access","text":"<p>Ensure you have proper access to the affected Ubiquity cluster:</p> <pre><code># Verify cluster access\nkubectl get nodes\n\n# Confirm BareMetalHost resources\nkubectl get bmh -A\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-2-start-kubectl-proxy","title":"Step 2: Start kubectl proxy","text":"<p>Start the Kubernetes API proxy to enable direct API access:</p> <pre><code>kubectl proxy &amp;\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-3-pause-baremetalhost-reconciliation","title":"Step 3: Pause BareMetalHost Reconciliation","text":"<p>Temporarily pause the Bare Metal Operator reconciliation for the affected host:</p> <pre><code>kubectl patch bmh master-2 --type=merge --patch '{\"metadata\":{\"annotations\":{\"baremetalhost.metal3.io/paused\": \"true\"}}}'\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-4-create-payload-files","title":"Step 4: Create Payload Files","text":"<p>Create the necessary payload files with corrected image URLs that use the stable httpd-http service endpoint.</p> <p>Create <code>status_payload.json</code>:</p> <pre><code>{\n   \"status\": {\n      \"errorCount\": 0,\n      \"errorMessage\": \"\",\n      \"provisioning\": {\n         \"image\": {\n            \"checksum\": \"http://httpd-http/images/ubiquity-node-rocky9.qcow2.md5sum\",\n            \"url\": \"http://httpd-http/images/ubiquity-node-rocky9.qcow2\"\n         },\n         \"state\": \"provisioned\"\n      }\n   }\n}\n</code></pre> <p>Create <code>spec_payload.json</code>:</p> <pre><code>{\n   \"spec\": {\n      \"image\": {\n         \"checksum\": \"http://httpd-http/images/ubiquity-node-rocky9.qcow2.md5sum\",\n         \"url\": \"http://httpd-http/images/ubiquity-node-rocky9.qcow2\"\n      }\n   }\n}\n</code></pre> <p>Image Reference Note: The example above uses <code>ubiquity-node-rocky9.qcow2</code>, which is the standard Ubiquity Rocky Linux 9 image. Replace this with your actual image name if different. Common Ubiquity images include: - <code>ubiquity-node-rocky9.qcow2</code> - Standard Ubiquity node image - <code>ubiquity-hpc-rocky9.qcow2</code> - HPC-optimized image with InfiniBand support - <code>ubiquity-gpu-rocky9.qcow2</code> - GPU-enabled compute image</p>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-5-validate-payload-files","title":"Step 5: Validate Payload Files","text":"<p>Verify that the payload files are valid JSON:</p> <pre><code>cat status_payload.json | jq\ncat spec_payload.json | jq\n</code></pre> <p>The command output should display the JSON structure without errors, confirming valid syntax.</p>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-6-apply-status-patch","title":"Step 6: Apply Status Patch","text":"<p>Update the BareMetalHost status using the Kubernetes API:</p> <pre><code>curl -k -v -XPATCH \\\n  -H \"Accept: application/json\" \\\n  -H \"Content-Type: application/merge-patch+json\" \\\n  --data-binary \"@status_payload.json\" \\\n  127.0.0.1:8001/apis/metal3.io/v1alpha1/namespaces/default/baremetalhosts/master-2/status\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-7-apply-spec-patch","title":"Step 7: Apply Spec Patch","text":"<p>Update the BareMetalHost specification:</p> <pre><code>kubectl patch bmh master-2 --type=merge --patch \"$(cat spec_payload.json)\"\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-8-resume-reconciliation","title":"Step 8: Resume Reconciliation","text":"<p>Re-enable the Bare Metal Operator reconciliation:</p> <pre><code>kubectl patch bmh master-2 --type=merge --patch '{\"metadata\":{\"annotations\":{\"baremetalhost.metal3.io/paused\":null}}}'\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#step-9-clean-up","title":"Step 9: Clean Up","text":"<p>Stop the kubectl proxy process:</p> <pre><code># Stop the background kubectl proxy\npkill -f \"kubectl proxy\"\n\n# Or bring the background process to foreground and stop with Ctrl+C\nfg\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#verification","title":"Verification","text":"<p>After completing the resolution steps, verify that the issue has been resolved:</p> <pre><code># Check BareMetalHost status\nkubectl get bmh master-2 -o yaml\n\n# Verify the error count is 0 and errorMessage is empty\nkubectl get bmh master-2 -o jsonpath='{.status.errorCount}'\nkubectl get bmh master-2 -o jsonpath='{.status.errorMessage}'\n\n# Confirm the image URLs are updated\nkubectl get bmh master-2 -o jsonpath='{.spec.image.url}'\n</code></pre>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#prevention","title":"Prevention","text":"<p>To prevent this issue in future Ubiquity cluster updates:</p> <ol> <li>Use Stable DNS Names: Always configure image URLs with stable DNS service names like <code>httpd-http</code> instead of IP addresses</li> <li>Image Management: Use Ubiquity's image building tools in <code>tools/disk-image/mkimage/</code> to create standardized images</li> <li>Update Procedures: Follow Ubiquity's recommended update procedures that preserve service endpoint stability</li> </ol>"},{"location":"admin-guide/troubleshooting/bmh-provisioning-error/#related-documentation","title":"Related Documentation","text":"<ul> <li>Operating System Images for Ubiquity - Comprehensive guide for building and managing Ubiquity images</li> <li>Metal3 API Reference - BareMetalHost resource specification</li> <li>Bare Metal Operator - Advanced BareMetalHost configuration options</li> </ul>"},{"location":"architecture/ingress/","title":"Ingress Steering and sticky sessions","text":"<p>We implement the following ingress steering and sticky sessions in Ubiquity:</p> <p><pre><code>nginx.ingress.kubernetes.io/affinity: \"cookie\"\nnginx.ingress.kubernetes.io/session-cookie-name: \"route\"\nnginx.ingress.kubernetes.io/session-cookie-expires: \"172800\"\nnginx.ingress.kubernetes.io/session-cookie-max-age: \"172800\"\n</code></pre> The reason for this is that we use a high-availability ingress instance spread over multiple nodes. This means that the ingress instance can be on a different node for each request. This can cause problems with the session. The above configuration ensures that the session is always on the same node.</p>"},{"location":"architecture/ingress/#here-is-the-explanation-for-the-code-above","title":"Here is the explanation for the code above:","text":"<ol> <li>Annotations to set up the sticky session are described in the official documentation.</li> <li>The sticky session is implemented using the cookie method.</li> <li>The cookie name is set to route.</li> <li>The cookie expires after 172800 seconds, which is 48 hours.</li> <li>The cookie max age is set to 172800 seconds, which is 48 hours.</li> <li>The sticky session is enabled for the ingress defined.</li> </ol>"},{"location":"architecture/ingress/#to-test-the-sticky-session-using-the-curl-command-heres-the-command-that-were-going-to-use","title":"To test the sticky session using the curl command. Here's the command that we're going to use:","text":"<p><code>curl -v -b cookie.txt -c cookie.txt http://&lt;INGRESS_IP&gt;/hello</code></p> <p>The command above will send a GET request to the Ingress resource. Let's break down the command above: - The <code>-v</code> flag is used to enable the verbose mode. - The <code>-b</code> flag is used to read cookies from the file. - The <code>-c</code> flag is used to write cookies to the file. - The <code>http://&lt;INGRESS_IP&gt;/hello</code> part is the URL that we want to send the request to. Replace  with the IP address of your Ingress resource. <p>Let's test the sticky session using the curl command: <code>curl -v -b cookie.txt -c cookie.txt http://&lt;INGRESS_IP&gt;/hello</code></p> <p>Here's an example output:</p> <pre><code>* Added cookie route=\"route1\" for domain &lt;INGRESS_IP&gt;, path /, expire 1641010396\n* Added cookie route=\"route1\" for domain &lt;INGRESS_IP&gt;, path /, expire 1641010396\n*   Trying &lt;INGRESS_IP&gt;...\n* TCP_NODELAY set\n* Connected to &lt;INGRESS_IP&gt; (&lt;INGRESS_IP&gt;) port 80\n</code></pre>"},{"location":"architecture/networking/","title":"Networking","text":"<p>As an example of how the networking works (because each system is different), this is an example of how you can use cloudflared to setup a tunnel, that goes to and from the cluster. <pre><code>flowchart TD\n  subgraph LAN\n    laptop/desktop/phone &lt;--&gt; LoadBalancer\n    subgraph k8s[Kubernetes cluster]\n      Pod --&gt; Service\n      Service --&gt; Ingress\n\n      LoadBalancer\n\n      cloudflared\n      cloudflared &lt;--&gt; Ingress\n    end\n    LoadBalancer &lt;--&gt; Ingress\n  end\n\n  cloudflared -- outbound --&gt; Cloudflare\n  Internet -- inbound --&gt; Cloudflare</code></pre></p> <p>TODO</p>"},{"location":"architecture/overview/","title":"Overview","text":""},{"location":"architecture/overview/#components","title":"Components","text":"<pre><code>+--------------+\n|    ./apps    |\n|--------------|\n|  ./platform  |\n|--------------|       +------------+\n|   ./system   |- - - -| ./external |\n|--------------|       +------------+\n| ./bootstrap  |\n|--------------|\n|   ./metal    |\n|--------------|\n|   ./cloud    |\n|--------------|\n|   HARDWARE   |\n+--------------+\n</code></pre>"},{"location":"architecture/overview/#main-components","title":"Main Components","text":"<ul> <li><code>./metal</code> - Bare metal management: install Linux, drivers, and Kubernetes</li> <li><code>./bootstrap</code> - GitOps bootstrap with ArgoCD</li> <li><code>./system</code> - Critical system components for the cluster (load balancer, storage, ingress, operation tools)</li> <li><code>./platform</code> - Essential components for service hosting platform (Vault, Git, SSO)</li> <li><code>./cloud</code> - Cloud platforms triggered at make time based on configuration to build on cloud provider</li> <li><code>./apps</code> - User-facing applications</li> <li><code>./external</code> (optional) - Externally managed services</li> </ul>"},{"location":"architecture/overview/#support-components","title":"Support Components","text":"<ul> <li><code>./tools</code> - Tools container with all utilities needed to manage the cluster and troubleshoot issues, including disk image building</li> <li><code>./docs</code> - All documentation that generates a searchable web UI</li> <li><code>./ubiq-playbooks</code> - Bootstrapping Ansible playbooks for Ansible AWX (separate sub-repository at <code>https://github.com/ubiquitycluster/ubiq-playbooks</code>)</li> <li><code>./scripts</code> - Scripts to automate common tasks such as configuring cluster deployment, retrieving passwords, etc.</li> </ul>"},{"location":"architecture/overview/#provisioning-flow","title":"Provisioning Flow","text":"<p>Everything is automated. After you edit the configuration files, you just need to run a single <code>make</code> command and it will:</p> <p>Option 1: Build the <code>./cloud</code> layer: - Bootstrap OS and drivers via Terraform on chosen cloud provider - Set up network policies to open a secure channel between cloud provider and bootstrap/on-premises environment - Build a Kubernetes cluster (based on k3s)</p> <p>Option 2: Build the <code>./metal</code> layer: - Download OS and drivers - Create an ephemeral, stateless PXE server - Install Linux on all servers in parallel - Build a Kubernetes cluster (based on k3s)</p> <p>Step 3: Build the <code>./bootstrap</code> layer: - Install ArgoCD - Configure the root app to manage other layers (and also manage itself)</p> <p>ArgoCD takes over and completes the deployment: - Step 4: Build the <code>./system</code> layer (storage, networking, monitoring, etc.) - Step 5: Build the <code>./platform</code> layer (Gitea, Vault, SSO, etc.) - Step 6: Build the <code>./apps</code> layer (Onyxia, Matrix, Slurm, etc.)</p>"},{"location":"architecture/overview/#architecture-flow-diagram","title":"Architecture Flow Diagram","text":"<pre><code>flowchart TD\n    subgraph metal[./metal]\n        pxe[PXE Server] -.-&gt; linux[Rocky Linux] --&gt; k3s\n    end\n\n    subgraph bootstrap[./bootstrap]\n        argocd[ArgoCD] --&gt; rootapp[Root app]\n    end\n\n    subgraph system[./system]\n        metallb[MetalLB]\n        nginx[NGINX]\n        longhorn[Longhorn]\n        cert-manager\n        external-dns[External DNS]\n        cloudflared\n    end\n\n    subgraph external[./external]\n        letsencrypt[Let's Encrypt]\n        cloudflare[Cloudflare]\n    end\n\n    letsencrypt -.-&gt; cert-manager\n    cloudflare -.-&gt; cert-manager\n    cloudflare -.-&gt; external-dns\n    cloudflare -.-&gt; cloudflared\n\n    subgraph platform[./platform]\n        gitea[Gitea]\n        awx[AWX]\n        dex[Dex]\n        harbor[Harbor]\n        hpc-ubiq[HPC-Ubiq]\n        keycloak[Keycloak]\n        argo-events[Argo Events]\n        argo-workflows[Argo Workflows]\n        vault[Vault]\n    end\n\n    subgraph apps[./apps]\n        onyxia[Onyxia]\n        matrix[Matrix]\n        slurm[Slurm]\n    end\n\n    make[Run make] -- 1 --&gt; metal\n    metal -- 2 --&gt; bootstrap\n    bootstrap -. 3 .-&gt; system\n    system -. 4 .-&gt; platform\n    platform -. 5 .-&gt; apps</code></pre>"},{"location":"architecture/overview/#detailed-provisioning-flow","title":"Detailed Provisioning Flow","text":"<p>Below is the pseudo code for the entire process. You don't have to read it right now, but it will be handy for debugging.</p> Detailed provisioning flow <pre><code>Human runs make:\n    Option - build ./cloud:\n        Start opus container\n        Cloud build-out:\n            Option - Azure:\n                Install azcli (will get folded into opus)\n                Do az login if first-time running to configure Azure credentials\n                Prompt via Ansible playbook for tenant and subscription IDs for project\n                Save in ./cloud/azure/terraform.tfvars\n                Run terraform init\n                Run terraform apply -auto-approve\n                Run az command to find tunnel information\n                Run Ansible playbook to setup tunnel\n            Option - AWS:\n                Install awscli (will get folded into opus)\n                Prompt via Ansible playbook for subscriptions\n                Save in ./cloud/aws/terraform.tfvars\n        Build a Kubernetes cluster:\n            Download k3s binary\n            Generate cluster token\n            Copy k3s config files\n            Enable k3s service and form a cluster\n            Create KUBECONFIG file\n            Create MetalLB config:\n                Use the last /27 subnet of the network\n                Apply the config\n\n    Option - build ./metal:\n        Start opus container\n        Install the OS/drivers:\n            Download the installer image and extract it\n            Download any driver packages and extract them\n            Create a PXE server on the controller using Docker Compose:\n                DHCP server\n                TFTP server\n                HTTP server\n            Create init config for each machine (in the form of a kickstart file)\n            Turn the machines on via WoL or IPMI (node-dependent)\n            The machines boot:\n                Select network boot automatically\n                Broadcast DHCP request\n                DHCP server reply:\n                    Machine IP\n                    TFTP server (next-server) IP\n                Get boot files from TFTP server:\n                    GRUB\n                    GRUB config with URL to init config based on MAC address\n                    Kernel\n                    Initrd\n                Boot to the kernel\n                Download from HTTP server:\n                    Init config from the URL in GRUB config\n                    Remaining files required to boot\n                Install the OS based on the init config:\n                    Configure the system\n                    Remaining files required to install\n                Install drivers for OS based on the init config:\n                    Configure drivers\n                Reboot to the new OS\n            Controller sees the machines are ready\n        Build a Kubernetes cluster:\n            Download k3s binary\n            Generate cluster token\n            Copy k3s config files\n            Enable k3s service and form a cluster\n            Create KUBECONFIG file\n            Create MetalLB config:\n                Use the last /27 subnet of the network\n                Apply the config\n\n    Build ./bootstrap:\n        Install ArgoCD:\n            Apply Helm chart\n            Wait for status\n        Install root app:\n            Select values file:\n                If Gitea unreachable (first install):\n                    Get data from GitHub\n                Else:\n                    Get data from Gitea\n            Apply Helm chart\n            Wait for status\n\nArgoCD applies the rest:\n    Clone git repo\n    Install components based on directories:\n        ./bootstrap (it manages itself):\n            ArgoCD\n            Root\n        ./system:\n            Storage\n            Load balancer\n            Ingress\n            Etc.\n        ./platform (depends on ./system):\n            Git:\n                Migrate the Ubiquity repository from GitHub\n                ArgoCD switches the source from GitHub to Gitea\n            CI\n            Vault\n            HPC-Ubiq\n            Onyxia\n            Etc.\n        ./apps (depends on ./system and ./platform):\n            Homepage\n            Cloud shell\n            Cloud CMD\n            Etc.\n</code></pre>"},{"location":"architecture/cloud/design/","title":"Design","text":""},{"location":"architecture/cloud/design/#ubiquity-terraform-structure","title":"Ubiquity Terraform Structure","text":"<p>Figure 1 (below) illustrates how Ubiquity is structured to provide a unified interface between multiple cloud providers. Each blue block is a file or a module, while white blocks are variables or resources. Arrows indicate variables or resources that contribute to the definition of the linked variables or resources. The figure can be read as a flow-chart from top to bottom. Some resources and variables have been left out of the chart to avoid cluttering it further.</p> <p>Ubiquity Terraform Structure Figure 1. Ubiquity Terraform Project Structure</p> <ol> <li><code>main.tf</code>: User provides the instances and volumes structure they want as _map_s.     <pre><code>instances = {\n  mgmt  = { type = \"p4-7.5gb\", tags = [\"ansible\", \"mgmt\", \"nfs\"] }\n  ctrl  = { type = \"p4-7.5gb\", tags = [\"master\", \"k8s\"] }\n  login = { type = \"p2-3.75gb\", tags = [\"login\", \"public\", \"proxy\"] }\n  node  = { type = \"p2-3.75gb\", tags = [\"node\", \"worker\"], count = 2 }\n}\n\nvolumes = {\n  nfs = {\n    home     = { size = 100 }\n    project  = { size = 500 }\n    scratch  = { size = 500 }\n  }\n}\n</code></pre></li> <li> <p><code>common/design</code>: </p> <ol> <li>the <code>instances</code> map is expanded to form a new map where each entry represents a single host.     <pre><code>instances = {\n  mgmt1 = {\n    type = \"p2-3.75gb\"\n    tags = [\"ansible\", \"mgmt\", \"nfs\"]\n  }\n  ctrl1 = {\n    type = \"p4-7.5gb\"\n    tags = [\"master\", \"k8s\"]\n  }\n  ctrl2 = {\n    type = \"p4-7.5gb\"\n    tags = [\"master\", \"k8s\"]\n  }\n  ctrl3 = {\n    type = \"p4-7.5gb\"\n    tags = [\"master\", \"k8s\"]\n  }\n  login1 = {\n    type = \"p2-3.75gb\"\n    tags = [\"login\", \"public\", \"proxy\"]\n  }\n  node1 = {\n    type = \"p2-3.75gb\"\n    tags = [\"node\", \"worker\"]\n  }\n  node2 = {\n    type = \"p2-3.75gb\"\n    tags = [\"node\", \"worker\"]\n  }\n}\n</code></pre></li> <li>the <code>volumes</code> map is expanded to form a new map where each entry represent a single volume     <pre><code>volumes = {\n  mgmt1-nfs-home    = { size = 100 }\n  mgmt1-nfs-project = { size = 100 }\n  mgmt1-nfs-scratch = { size = 500 }\n}\n</code></pre></li> </ol> </li> <li> <p><code>network.tf</code>: the <code>instances</code> map from <code>common/design</code> is used to generate a network interface (nic) for each host, and a public ip address for each host with the <code>public</code> tag. The local ip address retrieved from the nic of the instance tagged <code>ansible</code> is outputted as <code>ansibleserver_ip</code>.     <pre><code>resource \"provider_network_interface\" \"nic\" {\n  for_each = module.design.instances\n  ...\n}\n</code></pre></p> </li> <li> <p><code>common/instance_config</code>: for each host in <code>instances</code>, a cloud-init yaml config that includes <code>ansibleserver_ip</code> is generated. These configs are outputted to a <code>user_data</code> map where the keys are the hostnames.     <pre><code>user_data = {\n  for key, values in var.instances :\n    key =&gt; templatefile(\"${path.module}/inventory.yaml\", { ... })\n}\n</code></pre></p> </li> <li> <p><code>infrastructure.tf</code>: for each host in <code>instances</code>, an instance resource as defined by the selected cloud provider is generated. Each instance is initially configured by its <code>user_data</code> cloud-init yaml config.     <pre><code>resource \"provider_instance\" \"instances\" {\n  for_each  = module.design.instance\n  user_data = module.instance_config.user_data[each.key]\n  ...\n}\n</code></pre></p> </li> <li> <p><code>infrastructure.tf</code>: for each volume in <code>volumes</code>, a block device as defined by the selected cloud provider is generated and attached it to its matching instance using an <code>attachment</code> resource.     <pre><code>resource \"provider_volume\" \"volumes\" {\n  for_each = module.design.volumes\n  size     = each.value.size\n  ...\n}\nresource \"provider_attachment\" \"attachments\" {\n  for_each    = module.design.volumes\n  instance_id = provider_instance.instances[each.value.instance].id\n  volume_id   = provider_volume.volumes[each.key].id\n  ...\n}\n</code></pre></p> </li> <li> <p><code>infrastructure.tf</code>: the created instances' information are consolidated in a map output as <code>all_instances</code>.     <pre><code>all_instances = {\n  mgmt1 = {\n    public_ip = \"\"\n    local_ip  = \"10.0.0.1\"\n    id        = \"abc1213-123-1231\"\n    hostkey   = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAB\"\n    tags      = [\"mgmt\", \"ansible\", \"nfs\"]\n  }\n  ...\n}\n</code></pre></p> </li> <li> <p><code>common/cluster_config</code>: the information from created instances is consolidated in <code>all_instances</code> and written in a yaml file that is uploaded on the Ansible server as part of the configuration.     <pre><code>resource \"null_resource\" \"deploy_cluster_config\" {\n  ...\n  provisioner \"file\" {\n    content     = local.ansible_vars\n    destination = \"terraform_data.yaml\"\n  }\n  ...\n}\n</code></pre></p> </li> <li> <p><code>outputs.tf</code>: the information of all instances that have a public address are output as a map named <code>public_instances</code>.</p> </li> </ol>"},{"location":"architecture/cloud/design/#resource-per-provider","title":"Resource per provider","text":"<p>In the previous section, we have used generic resource name when writing HCL code that defines these resources. The following table indicate what resource is used for each provider based on its role in the cluster.</p> Resource AWS Azure Google Cloud Platform OpenStack OVH network aws_vpc azurerm_virtual_network google_compute_network prebuilt openstack_networking_network_v2 subnet aws_subnet azurerm_subnet google_compute_subnetwork prebuilt openstack_networking_subnet_v2 router aws_route not used google_compute_router built-in not used nat aws_internet_gateway not used google_compute_router_nat built-in not used firewall aws_security_group azurerm_network_security_group google_compute_firewall openstack_compute_secgroup_v2 openstack_compute_secgroup_v2 nic aws_network_interface azurerm_network_interface google_compute_address openstack_networking_port_v2 openstack_networking_port_v2 public ip aws_eip azurerm_public_ip google_compute_address openstack_networking_floatingip_v2 openstack_networking_network_v2 instance aws_instance azurerm_linux_virtual_machine google_compute_instance openstack_compute_instance_v2 openstack_compute_instance_v2 volume aws_ebs_volume azurerm_managed_disk google_compute_disk openstack_blockstorage_volume_v3 openstack_blockstorage_volume_v3 attachment aws_volume_attachment azurerm_virtual_machine_data_disk_attachment google_compute_attached_disk openstack_compute_volume_attach_v2 openstack_compute_volume_attach_v2"},{"location":"architecture/cloud/design/#using-reference-design-to-extend-for-a-new-cloud-provider","title":"Using reference design to extend for a new cloud provider","text":"<p>Ubiquity currently supports five cloud providers, but its design makes it easy to add new providers. This section presents a step-by-step guide to add a new cloud provider support to Ubiquity.</p> <ol> <li> <p>Identify the resources. Using the Resource per provider table, read the cloud provider Terraform documentation, and identify the name for each resource in the table.</p> </li> <li> <p>Check minimum requirements. Once all resources have been identified, you should be able to determine if the cloud provider can be used to deploy Ubiquity. If you found a name for each resource listed in table, the cloud provider can be supported. If some resources are missing, you will need to do read the provider's documentation to determine if the absence of the resource can be compensated for somehow.</p> </li> <li> <p>Initialize the provider folder. Create a folder named after the provider. In this folder, create two symlinks, one pointing to <code>common/variables.tf</code> and the other to <code>common/outputs.tf</code>. These files define the interface common to all providers supported by Ubiquity.</p> </li> <li> <p>Define cloud provider specifics variables. Create a file named after your provider <code>provider_name.tf</code> and define variables that are required by the provider but not common to all providers, for example the availability zone or the region.</p> </li> <li> <p>Initialize the infrastructure. Create a file named  <code>infrastructure.tf</code>. In this file, define the provider if it requires input parameters (for example the region) and include the <code>common/design</code> module.     <pre><code>provider \"provider_name\" {\n  region = var.region\n}\n\nmodule \"design\" {\n  source       = \"../common/design\"\n  cluster_name = var.cluster_name\n  domain       = var.domain\n  instances    = var.instances\n  volumes      = var.volumes\n}\n</code></pre></p> </li> <li> <p>Create the networking infrastructure. Create a file named <code>network.tf</code> and define the network, subnet, router, nat, firewall, nic and public ip resources using the <code>module.design.instances</code> map.</p> </li> <li> <p>Create the instance configurations. In <code>infrastructure.tf</code>, include the <code>common/instance_config</code> module and provide the required input parameters.   <pre><code>module \"instance_config\" {\n  source = \"../common/instance_config\"\n  ...\n}\n</code></pre></p> </li> <li> <p>Create the instances. In <code>infrastructure.tf</code>, define the <code>instances</code> resource using <code>module.design.instances</code> for the instance attributes and <code>module.instance_config.user_data</code> for the initial configuration.</p> </li> <li> <p>Create the volumes. In <code>infrastructure.tf</code>, define the <code>volumes</code> resource using <code>module.design.volumes</code>.</p> </li> <li> <p>Attach the volumes. In <code>infrastructure.tf</code>, define the <code>attachments</code> resource using <code>module.design.volumes</code> and refer to the attribute <code>each.value.instance</code> to retrieve the instance's id to which the volume needs to be attached.</p> </li> <li> <p>Consolidate the instances' information.  In <code>infrastructure.tf</code>, define a local variable named <code>all_instances</code> that will be a map containing the following keys (for each created instance): <code>id</code>, <code>public_ip</code>, <code>local_ip</code>, <code>tags</code>, <code>hostkeys</code>, where <code>hostkeys</code> is also a map with a key named <code>rsa</code> that correspond to the instance hostkey.</p> </li> <li> <p>Consolidate the volume device information. In <code>infrastructure.tf</code>, define a local variable named <code>volume_devices</code> implementing the following logic in HCL. Replace the line starting by <code>/dev/disk/by-id</code> with the proper logic that would match the volume resource to its device path from within the instance to which it is attached.   <pre><code>volume_devices = {\n  for ki, vi in var.volumes :\n  ki =&gt; {\n    for kj, vj in vi :\n    kj =&gt; [for key, volume in module.design.volumes :\n      \"/dev/disk/by-id/*${substr(provider_volume.volumes[\"${volume[\"instance\"]}-${ki}-${kj}\"].id, 0, 20)}\"\n      if key == \"${volume[\"instance\"]}-${ki}-${kj}\"\n    ]\n  }\n}\n</code></pre></p> </li> <li> <p>Create the cluster configuration and upload. In <code>infrastructure.tf</code>, include the <code>common/cluster_config</code> module and provide the required input parameters.</p> </li> </ol>"},{"location":"architecture/cloud/design/#an-example","title":"An example","text":"<ol> <li> <p>Identify the resources. For Digital Ocean, Oracle Cloud and Alibaba Cloud, we get the following resource mapping:     | Resource    | Digital Ocean | Oracle Cloud | Alibaba Cloud |     | ----------- | :-------------------- |  :-------------------- |  :-------------------- |     | network     | digitalocean_vpc | oci_core_vcn | alicloud_vpc |     | subnet      | built in vpc | oci_subnet | alicloud_vswitch |     | router      | n/a          | oci_core_route_table | built in vpc |     | nat         | n/a          | oci_core_internet_gateway | alicloud_nat_gateway |     | firewall    | digitalocean_firewall | oci_core_security_list | alicloud_security_group |     | nic         | n/a | built in instance | alicloud_network_interface |     | public ip   | digitalocean_floating_ip | built in instance | alicloud_eip |     | instance    | digitalocean_droplet | oci_core_instance | alicloud_instance |     | volume      | digitalocean_volume | oci_core_volume | alicloud_disk |     | attachment  | digitalocean_volume_attachment | oci_core_volume_attachment | alicloud_disk_attachment |</p> </li> <li> <p>Check minimum requirements. In the preceding table, we can see Digital Ocean does not have the ability to define a network interface. The documentation also leads us to conclude that it is not possible to define the private ip address of the instances before creating them. Because the Ansible server ip address is required before generating the cloud-init YAML config for all instances, including the Ansible server itself, this means it impossible to use Digital Ocean to spawn a Ubiquity cluster without significant extra effort outside of Terraform.  Oracle Cloud presents the same issue, however, after reading the instance documentation, we find that it is possible to define a static ip address as a string in the instance attribute. It would therefore be possible to create a datastructure in Terraform that would associate each instance hostname with an ip address in the subnet CIDR.  Alibaba cloud has an answer for each resource, so we will use this provider in the following steps.</p> </li> <li> <p>Initialize the provider folder. In a terminal:   <pre><code>git clone https://github.com/ubiquitycluster/ubiquity.git\ncd cloud\nmkdir alicloud\ncd aliclcoud\nln -s ../common/{variables,outputs}.tf .\n</code></pre></p> </li> <li> <p>Define cloud provider specifics variables. Add the following to a new file <code>alicloud.tf</code>:   <pre><code>variable \"region\" { }\nlocals {\n  cloud_provider  = \"alicloud\"\n  cloud_region    = var.region\n}\n</code></pre></p> </li> <li> <p>Initialize the infrastructure. Add the following to a new file <code>infrastructure.tf</code>:   <pre><code>provider \"alicloud\" {\n  region = var.region\n}\n\nmodule \"design\" {\n  source       = \"../common/design\"\n  cluster_name = var.cluster_name\n  domain       = var.domain\n  instances    = var.instances\n  volumes      = var.volumes\n}\n</code></pre></p> </li> <li> <p>Create the networking infrastructure. <code>network.tf</code> base template:   <pre><code>resource \"alicloud_vpc\" \"network\" { }\nresource \"alicloud_vswitch\" \"subnet\" { }\nresource \"alicloud_nat_gateway\" \"nat\" { }\nresource \"alicloud_security_group\" \"firewall\" { }\nresource \"alicloud_security_group_rule\" \"allow_in_services\" { }\nresource \"alicloud_security_group\" \"allow_any_inside_vpc\" { }\nresource \"alicloud_security_group_rule\" \"allow_ingress_inside_vpc\" { }\nresource \"alicloud_security_group_rule\" \"allow_egress_inside_vpc\" { }\nresource \"alicloud_network_interface\" \"nic\" { }\nresource \"alicloud_eip\" \"public_ip\" { }\nresource \"alicloud_eip_association\" \"eip_asso\" { }\n\nlocals {\n  ansibleserver_ip = [\n      for x, values in module.design.instances : alicloud_network_interface.nic[x].private_ip\n      if contains(values.tags, \"ansible\")\n  ]\n}\n</code></pre></p> </li> <li> <p>Create the instance configuration. Add the following to <code>infrastructure.tf</code>:   <pre><code>module \"instance_config\" {\n  source           = \"../common/instance_config\"\n  instances        = module.design.instances\n  config_git_url   = var.config_git_url\n  config_version   = var.config_version\n  ansibleserver_ip  = local.ansibleserver_ip \n  sudoer_username  = var.sudoer_username\n  public_keys      = var.public_keys\n  generate_ssh_key = var.generate_ssh_key\n}\n</code></pre></p> </li> <li> <p>Create the instances. Add and complete the following snippet to <code>infrastructure.tf</code>:   <pre><code>resource \"alicloud_instance\" \"instances\" {\n  for_each = module.design.instances\n}\n</code></pre></p> </li> <li> <p>Create the volumes. Add and complete the following snippet to <code>infrastructure.tf</code>:   <pre><code>resource \"alicloud_disk\" \"volumes\" {\n  for_each = module.design.volumes\n}\n</code></pre></p> </li> <li> <p>Attach the volumes. Add and complete the following snippet to <code>infrastructure.tf</code>:   <pre><code>resource \"alicloud_disk_attachment\" \"attachments\" {\n  for_each = module.design.volumes\n}\n</code></pre></p> </li> <li> <p>Consolidate the instances' information. Add the following snippet to <code>infrastructure.tf</code>:   <pre><code>locals {\n  all_instances = { for x, values in module.design.instances :\n    x =&gt; {\n      public_ip   = contains(values[\"tags\"], \"public\") ? alicloud_eip.public_ip[x].public_ip : \"\"\n      local_ip    = alicloud_network_interface.nic[x].private_ip\n      tags        = values[\"tags\"]\n      id          = alicloud_instance.instances[x].id\n      hostkeys    = {\n        rsa = module.instance_config.rsa_hostkeys[x]\n      }\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Consolidate the volume devices' information. Add the following snippet to <code>infrastructure.tf</code>:   <pre><code>volume_devices = {\n  for ki, vi in var.volumes :\n  ki =&gt; {\n    for kj, vj in vi :\n    kj =&gt; [for key, volume in module.design.volumes :\n      \"/dev/disk/by-id/virtio-${replace(alicloud_disk.volumes[\"${volume[\"instance\"]}-${ki}-${kj}\"].id, \"d-\", \"\")}\"\n      if key == \"${volume[\"instance\"]}-${ki}-${kj}\"\n    ]\n  }\n}\n</code></pre></p> </li> <li> <p>Create the cluster configuration and upload. Add the following snippet to <code>infrastructure.tf</code>.   <pre><code>module \"cluster_config\" {\n  source          = \"../common/cluster_config\"\n  instances       = local.all_instances\n  nb_users        = var.nb_users\n  ansible_vars    = var.ansible_vars\n  software_stack  = var.software_stack\n  cloud_provider  = local.cloud_provider\n  cloud_region    = local.cloud_region\n  sudoer_username = var.sudoer_username\n  guest_passwd    = var.guest_passwd\n  domain_name     = module.design.domain_name\n  cluster_name    = var.cluster_name\n  volume_devices  = local.volume_devices\n  private_ssh_key = module.instance_config.private_key\n}\n</code></pre></p> </li> </ol> <p>Once your new provider is written, you can write an example that will use the module to spawn a Ubiquity cluster with that provider.   <pre><code>module \"alicloud\" {\n  source         = \"./alicloud\"\n  config_git_url = \"https://github.com/ubiquitycluster/ubiquity.git\"\n  config_version = \"main\"\n\n  cluster_name = \"new\"\n  domain       = \"my.cloud\"\n  image        = \"centos_7_9_x64_20G_alibase_20210318.vhd\"\n  nb_users     = 10\n\n  instances = {\n    mgmt   = { type = \"ecs.g6.large\", tags = [\"puppet\", \"mgmt\", \"nfs\"] }\n    login  = { type = \"ecs.g6.large\", tags = [\"login\", \"public\", \"proxy\"] }\n    node   = { type = \"ecs.g6.large\", tags = [\"node\"], count = 1 }\n  }\n\n  volumes = {\n    nfs = {\n      home     = { size = 10 }\n      project  = { size = 50 }\n      scratch  = { size = 50 }\n    }\n  }\n\n  public_keys = [file(\"~/.ssh/id_rsa.pub\")]\n\n  # Alicloud specifics\n  region  = \"us-west-1\"\n}\n</code></pre></p>"},{"location":"architecture/on-prem/openstack-bmo-node-discovery/","title":"Bare metal provisioning (PXE boot)","text":"<pre><code>flowchart TD\n  subgraph controller[Initial controller]\n    Ansible\n    dhcp[DHCP server]\n    tftp[TFTP server]\n    http[HTTP server]\n  end\n\n  machine[Bare metal machine]\n\n  Ansible -. 1 .-&gt; machine\n  machine &lt;-. 2, 3 .-&gt; dhcp\n  machine &lt;-. 4, 5 .-&gt; tftp\n  machine &lt;-. 6, 7 .-&gt; http</code></pre> <ol> <li>create-nodelist generates a nodelist, with expected IPs, settings, etc. </li> <li>node-data-generate.sh then creates the node data for the nodes and host including finding mac addresses <code>xx:xx:xx:xx:xx:xx</code>, into the output directory. This requires golang to be installed.</li> <li>DHCP server: I hear you, here's your IP address, proceed to the next server to obtain your bootloader.</li> <li>Machine: Hello, could you please send me my bootloader?</li> <li>TFTP server: Here you go. Grab your boot configuration, kernel, and initial ramdisk as well.</li> <li>Machine: Hi, I just booted into my bootloader, and my boot parameters instructed me to get the installation instructions, packages, etc. from this site.</li> <li>HTTP server: It's all yours.</li> <li>Machine: Great, now I can install the OS and reboot!</li> </ol>"},{"location":"developers/developers/","title":"Ubiquity Developer Documentation","text":""},{"location":"developers/developers/#table-of-content","title":"Table of Content","text":"<ol> <li>Setup</li> <li>Where to start</li> <li>Troubleshooting</li> <li>Release</li> </ol>"},{"location":"developers/developers/#1-setup","title":"1. Setup","text":"<p>To develop for Ubiquity you will need: * Terraform (&gt;= 1.2.1) * git * Access to a Cloud provider (AWS, GCP, Azure, OpenStack, etc.) OR hardware to develop on - See the development rig that we use here * Ability to communicate with the cloud provider API from your computer * A cloud project with enough room for the resource described in section Ubiquity Doc 1.1.</p>"},{"location":"developers/developers/#2-where-to-start","title":"2. Where to start","text":"<p>The Ubiquity project is defined by an infrastructure-as-code component that is responsible for generating a cluster architecture in a cloud and a Ansible environment that configures the cluster instances based on their role.</p> <p>If you wish to add device, an instance, add a new networking interface or a filesystem, you will most likely need to develop some Terraform code if you're working on a cloud provider.  The project structure for Terraform code is described in the reference design document.  The document also describes how one could work with current Ubiquity code to add support for another cloud provider.</p> <p>If you wish to add a service to one of the Ansible roles, install new software, modify an instance configuration or role, you will most likely need to develop some Ansible code. The following section provides more details on the Ansible environments available and how to develop them.</p>"},{"location":"developers/developers/#3-ansible-environment","title":"3. Ansible environment","text":"<p>Ubiquity Terraform code initialises every instance to be a cloud-init agent and an instance with the tag <code>awx.ubiquitycluster.uk</code> (by default, configurable) as the AWX main server. </p> <p>On the AWX main server, there is a folder containing the configuration code for the instances of the cluster, this folder is called an Ansible environment and it is pulled from GitHub  during the initial configuration of the AWX main server.</p> <p>The source of that environment is provided to Terraform using the variable <code>config_git_url</code>.</p> <p>A repository describing an AWX environment must contain at the least the following files  and folders: <pre><code>ubiq-playbooks\n\u2517 collections\n  \u2517 requirements.yml\n\u2523 ansible.cfg\n\u2523 ubiquity.yml\n\u2517 inventory\n  \u2517 mail.yml\n\u2517 vars/\n  \u2517 main.yml\n</code></pre></p> <ul> <li><code>collections/requirements.yml</code> specifies the Ansible collections that need to be installed in the environment.</li> <li><code>ansible.cfg</code> overrides the primary server default settings for the environment. Note that these are often present per-role as well for role-specific overrides.</li> <li><code>ubiquity.yml</code> configures a playbook that runs an ordered list of playbooks with their own roles inside to be applied to the instances. This structure is to enable individual playbooks to be applied to instances as needed (and allow testing of individual playbooks).</li> <li><code>vars/main.yml</code> is common data source to apply variables as an override to the roles. You can also apply this inside AWX.</li> <li><code>inventory/main.yml</code> defines how each instance will be configured based on their hostname and/or groups.</li> </ul> <p>An example of a bare-bone Ubiquity environment is available on GitHub: Ubiquity/ubiq-playbooks, while the Ubiquity AWX environment that replicates a Ubiquity HPC cluster is named ubiquitycluster/ubiq-playbooks.</p>"},{"location":"developers/developers/#terraform_datayaml-a-bridge-between-terraform-and-awx","title":"terraform_data.yaml: a bridge between Terraform and AWX","text":"<p>To provide information on the deployed resources and the value of the input parameters, Ubiquity Terraform code uploads to the AWX main server a file named <code>terraform_data.yaml</code>, in the folder <code>/etc/puppetlabs/data/</code>. There is also a symlink created in <code>/etc/puppetlabs/code/environment/production/data/</code> to ease its usage inside the Puppet environment.</p> <p>When included in the data hierarchy (<code>hiera.yaml</code>), <code>terraform_data.yaml</code> can provide information about the instances, the volumes and the variables set by the user through the <code>main.tf</code> file. The file has the following structure: <pre><code>---\nterraform:\n  instances:\n    hostname1:\n      local_ip: \"10.0.0.x\"\n      public_ip: \"\"\n      tags: [\"tag_1\"]\n      hostkeys:\n        rsa: \"\"\n  volumes:\n    volume_tag1:\n      volume_1:\n        - \"/dev/disk/by-id/123-*\"\n      volume_2:\n        - \"/dev/disk/by-id/123-abc-*\"\n  tag_ip:\n    tag_1:\n      - 10.0.0.x\n  data:\n    cluster_name: \"\"\n    consul_token: \"\"\n    domain_name: \"\"\n    freeipa_passwd: \"\"\n    guest_passwd: \"\"\n    munge_key: \"\"\n    nb_users: \"\"\n    sudoer_username: \"\"\n</code></pre></p> <p>The values provided by <code>terraform_data.yaml</code> can be accessed in Puppet by using the <code>lookup()</code> function. For example, to access an instance's list of tags: <pre><code>lookup(\"terraform.instances.${::hostname}.tags\")\n</code></pre> The data source can also be used to define a key in another data source YAML file by using the <code>alias()</code> function. For example, to define the number of guest accounts using the value of <code>nb_user</code>, we could add this to <code>common.yaml</code> <pre><code>profile::accounts::guests::nb_accounts: \"%{alias('terraform.data.nb_users')}\"\n</code></pre></p>"},{"location":"developers/developers/#configuring-instances-sitepp-and-classes","title":"Configuring instances: site.pp and classes","text":"<p>The configuration of each instance is defined in <code>manifests/site.pp</code> file of the Puppet environment. In this file, it is possible to define a configuration based on an instance hostname <pre><code>node \"mgmt1\" { }\n</code></pre> or using the instance tags by defining the configuration for the <code>default</code> node : <pre><code>node default {\n  $instance_tags = lookup(\"terraform.instances.${::hostname}.tags\")\n  if 'tag_1' in $instances_tags { }\n}\n</code></pre></p> <p>It is possible to define Puppet resource directly in <code>site.pp</code>. However, above a certain level of complexity, which can be reach fairly quickly, it is preferable to define classes and include these classes in <code>site.pp</code> based on the node hostname or tags.</p> <p>Classes can be defined in the Puppet environment under the following path: <code>site/profile/manifests</code>. These classes are named profile classes and the philosophy behind it is explained in Puppet documentation. Because these classes are defined in <code>site/profile</code>, their name has to start with the prefix <code>profile::</code>.</p> <p>It is also possible to include classes defined externally and installed using the <code>Puppetfile</code>. These classes installed by librarian-puppet can be found in the <code>modules</code> folder of the Puppet environment.</p>"},{"location":"developers/developers/#4-troubleshooting","title":"4. Troubleshooting","text":""},{"location":"developers/developers/#41-cloud-init","title":"4.1 cloud-init","text":"<p>To test new additions to OS images and Ansible playbooks, it is possible to execute cloud-init phases manually. There are four steps that can be executed sequentially: init local, init modules config and modules final. Here are the corresponding commands to execute each step: <pre><code>cloud-init init --local\ncloud-init init\ncloud-init modules --mode=config\ncloud-init modules --mode=final\n</code></pre></p> <p>It is also possible to clean a cloud-init execution and have it execute again at next reboot. To do so, enter the following command: <pre><code>cloud-init clean\n</code></pre> Add <code>-r</code> to the previous command to reboot the instance once cloud-init has finishing cleaning.</p>"},{"location":"developers/developers/#42-selinux","title":"4.2 SELinux","text":"<p>SELinux is enabled on every instances of a Ubiquity cluster. Some applications do not provide SELinux policies which can lead to their malfunctionning when SELinux is enabled. It is possible to track down the reasons why SELinux is preventing an application to work properly using the command-line tool <code>ausearch</code>.</p> <p>If you suspect application <code>app-a</code> to be denied by SELinux to work properly, run the following command as root: <pre><code>ausearch -c app-a --raw | grep denied\n</code></pre></p> <p>To see all requests denied by SELinux: <pre><code>ausearch --raw | grep denied\n</code></pre></p> <p>Sometime, the denials are hidden from regular logging. To display all denials, run the following command as root: <pre><code>semodule --disable_dontaudit --build\n</code></pre> then re-execute the application that is not working properly.</p> <p>Once you have found the denials that are the cause of the problem, you can create a new policy to allow the requests that were previously denied with the following command: <pre><code>ausearch -c app-a --raw | grep denied | audit2allow -a -M app-a\n</code></pre></p> <p>Finally, you can install the generated policy using the command provided by <code>auditallow</code>.</p>"},{"location":"developers/developers/#building-the-policy-package-file-pp-from-the-enforcement-file-te","title":"Building the policy package file (.pp) from the enforcement file (.te)","text":"<p>If you need to tweak an existing enforcement file and you want to recompile the policy package, you can with the following commands: <pre><code>checkmodule -M -m -o my_policy.mod my_policy.te\nsemodule_package -o my_policy.pp -m my_policy.mod\n</code></pre></p>"},{"location":"developers/developers/#references","title":"References","text":"<ul> <li>https://wiki.gentoo.org/wiki/SELinux</li> <li>https://wiki.gentoo.org/wiki/SELinux/Tutorials/Where_to_find_SELinux_permission_denial_details</li> </ul>"},{"location":"developers/developers/#5-release","title":"5. Release","text":"<p>To build a release, use the script <code>release.sh</code> located in the scripts folder of the Ubiquity git repo. <pre><code>Usage: release.sh VERSION [provider ...]\n</code></pre> The script creates a folder named <code>releases</code> where it was called.</p> <p>The <code>VERSION</code> argument is expected to correspond to git tag in the <code>ubiq-playbooks</code> repo. It could also be a branch name or a commit. If the provider optional argument is left blank, release files will be built for all providers currently supported by Ubiquity.</p> <p>Examples:</p> <ul> <li>Building a release for OpenStack with the ansible repo main branch:     <pre><code>$ ./release.sh main openstack\n</code></pre></li> <li>Building a release for GCP with the latest Terraform and cloud-init, and version 5.8 of ansible playbooks Ubiquity:     <pre><code>$ ./release.sh 5.8 gcp\n</code></pre></li> <li>Building a release for Azure and OVH with the latest Terraform and cloud-init, and version 5.7 of ansible playbooks Ubiquity:     <pre><code>$ ./release.sh 5.7 azure ovh\n</code></pre></li> </ul>"},{"location":"developers/releasing/","title":"Ubiquity releasing","text":"<p>This document details the steps to create a release for <code>ubiquity</code> aka UbiQ.</p>"},{"location":"developers/releasing/#before-making-a-release","title":"Before making a release","text":"<p>Things you should do before making a release:</p> <ul> <li>Check the   Ubiquity release process   for high-level process and possible follow-up actions</li> <li>Uplift controller Go modules to use latest corresponding modules</li> <li>Uplift any other direct/indirect dependency to close any public   vulnerabilities</li> </ul>"},{"location":"developers/releasing/#permissions","title":"Permissions","text":"<p>Creating a release requires repository <code>write</code> permissions for:</p> <ul> <li>Tag pushing</li> <li>GitHub Release publishing</li> </ul> <p>These permissions are implicit for the org admins and repository admins. This GitHub team has the required permissions in each repository required to release Ubiquity. Adding person to the team gives him/her the necessary rights in all relevant repositories in the organisation. Individual persons should not be given permissions directly.</p>"},{"location":"developers/releasing/#process","title":"Process","text":"<p>Ubiquity uses semantic versioning.</p>"},{"location":"developers/releasing/#repository-setup","title":"Repository setup","text":"<p>Clone the repository: <code>git clone git@github.com:ubiquitycluster/ubiquity</code></p> <p>or if using existing repository, verify your intended remote is set to <code>ubiquity</code>: <code>git remote -v</code>. For this document, we assume it is <code>origin</code>.</p>"},{"location":"developers/releasing/#tags","title":"Tags","text":"<p>Ubiquity does not have release branches. Release is created by tagging a commit off the main branch. First we create a primary release tag, that triggers release note creation and image building processes.</p> <ul> <li>Create a signed, annotated tag with: <code>git tag -s -a v0.x.y -m v0.x.y</code></li> <li>Push the tags to the GitHub repository: <code>git push origin v0.x.y</code></li> </ul> <p>This triggers two things:</p> <ul> <li>GitHub action workflow for automated release process creates a draft release   in GitHub repository with correct content, comparing the pushed tag to   previous tag</li> <li>Quay starts building release image for opus with the release tag</li> </ul> <p>We also need to create one or more tags for the Go modules ecosystem:</p> <ul> <li>For any subdirectory with <code>go.mod</code> in it (excluding <code>hack/tools</code>), create   another Git tag with directory prefix, ie.   <code>git tag -s api/v0.x.y -m api/v0.x.y</code>.   NOTE: Do not create annotated tags for go modules.</li> </ul>"},{"location":"developers/releasing/#release-artifacts","title":"Release artifacts","text":"<p>We need to verify all release artifacts are correctly built or generated by the release workflow. For a release, we should have the following artifacts:</p> <p>Git tags pushed:</p> <ul> <li>Primary release tag: <code>v0.x.y</code></li> <li>Go module tags: <code>api/v0.x.y</code> and <code>pkg/hardwareutils/v0.x.y</code></li> </ul> <p>Container images built and tagged at Quay registry:</p> <ul> <li>ubiquity-opus:v0.x.y</li> </ul> <p>Ubiquity release in GitHub will only contain source code as artifact.</p>"},{"location":"developers/releasing/#release-notes","title":"Release notes","text":"<p>Next step is to clean up the release note manually.</p> <ul> <li>Check for duplicates, reverts, and incorrect classifications of PRs, and   whatever release creation tagged to be manually checked.</li> <li>For any superseded PRs (like same dependency uplifted multiple times, or   commit revertions) that provide no value to the release, create a summary   line in <code>Superseded</code> section with the PR ids and summary title. This way the   changes are acknowledged to be part of the release, but not overwhelming the   important changes contained by the release.</li> <li>If the release you're making is not a new major release, new minor release,   or a new patch release from the latest release branch, uncheck the box for   latest release.</li> <li>If it is a release candidate (RC) or a pre-release, tick pre-release box.</li> <li>Publish the release.</li> </ul>"},{"location":"developers/roadmap/","title":"Roadmap","text":"<p>Info</p> <p>Current status: BETA</p>"},{"location":"developers/roadmap/#alpha-requirements","title":"Alpha requirements","text":"<p>Literally anything that works.</p>"},{"location":"developers/roadmap/#beta-requirements","title":"Beta requirements","text":"<p>Good enough for standard usage, and secure.</p> <ul> <li> Automated bare metal provisioning<ul> <li> Controller set up (Docker)</li> <li> OS installation (PXE boot)</li> </ul> </li> <li> Automated cluster creation (k3s)</li> <li> Automated application deployment (ArgoCD)</li> <li> Automated DNS management</li> <li> Initialise GitOps repository on Gitea automatically</li> <li> Observability<ul> <li> Monitoring</li> <li> Logging</li> <li> Alerting</li> </ul> </li> <li> SSO</li> <li> Secure<ul> <li> Automated certificate management</li> <li> Declarative secret management</li> <li> Replace all default passwords with randomly generated ones</li> <li> Expose services to the internet securely with Cloudflare Tunnel</li> </ul> </li> <li> Only use open-source technologies (except external managed services in <code>./external</code>)</li> <li> Everything is defined as code</li> <li> Backup solution (3 copies, 2 seperate devices, 1 offsite)</li> <li> Define SLOs:<ul> <li> 70% availability (might break in the weekend due to new experimentation)</li> </ul> </li> <li> Core applications<ul> <li> Gitea</li> <li> Tekton</li> <li> Vault</li> <li> Private container registry</li> <li> Homepage</li> </ul> </li> </ul>"},{"location":"developers/roadmap/#stable-requirements","title":"Stable requirements","text":"<p>Can be used in production.</p> <ul> <li> A single command to deploy everything</li> <li> Fast deployment time (from empty hard drive to running services in under 1 hour)</li> <li> Fully automatic, not just automated<ul> <li> Bare-metal OS rolling upgrade</li> <li> Kubernetes version rolling upgrade</li> <li> Application version upgrade</li> <li> Encrypted backups</li> <li> Secrets rotation</li> <li> Self healing</li> </ul> </li> <li> Secure by default<ul> <li> SELinux</li> <li> Network policies</li> </ul> </li> <li> Static code analysis</li> <li> Chaos testing</li> <li> Minimal dependency on external services</li> <li> Complete documentation<ul> <li> Diagram as code</li> <li> Book (this book)</li> <li> Walkthrough tutorial and feature demo (video)</li> </ul> </li> <li> Configuration script for new users</li> <li> SLOs:<ul> <li> 99,9% availability (less than 9 hours of downtime per year)</li> <li> 99,99% data durability</li> </ul> </li> <li> Clear upgrade path</li> <li> Additional applications<ul> <li> Matrix with bridges</li> <li> VPN server</li> <li> Slurm</li> <li> Onyxia</li> <li> Development dashboard</li> </ul> </li> </ul>"},{"location":"developers/roadmap/#unplanned","title":"Unplanned","text":"<p>Nice to have</p> <ul> <li> Addition applications<ul> <li> Mail server</li> </ul> </li> <li> Air-gap install</li> <li> Automated testing</li> <li> Security audit</li> <li> Serverless (Knative)</li> <li> Cluster API (last attempt)</li> <li> Split DNS (requires a better router)</li> </ul>"},{"location":"processes/roadmap/","title":"Ubiquity Roadmap","text":"<p>The Ubiquity Roadmap is maintained as a Github project and can be found here.</p>"},{"location":"processes/roadmap/#description","title":"Description","text":"<p>Each column in the project represents the work items for a specific release of Ubiquity. In addition there is a <code>Feature requests</code> column that contains items  that have not yet been accepted and the <code>Backlog</code> column that contains items  that have been accepted but not yet planned for a specific release.</p> <p>An issue can be planned for a specific release if someone volunteers to take ownership of the feature. The owner will then be assigned the issue. An owner does not have to carry the whole design and implementation processes on her own but must instead make sure that the feature is being worked on and will be completed by the planned release date.</p>"},{"location":"processes/roadmap/#proposing-a-feature","title":"Proposing a feature","text":"<p>Proposing a new feature for a specific release of one of the components is done by opening an issue in the Ubiquity repository, describing the feature and which components and release are targeted. The new issue will automatically appear in the feature request column of the roadmap.</p>"},{"location":"processes/roadmap/#updating-the-roadmap","title":"Updating the Roadmap","text":"<p>Updating the roadmap is done during a community meeting, with a discussion within the members of the projects, alternatively through an email thread. The update is performed by one of the approvers of the Ubiquity project.</p> <p>A new feature proposal is moved from the <code>feature requests</code> column to a component release column if agreed within the community, and a member volunteers to take ownership of that feature. If a feature is seen as necessary in the long-term without being planned for the releases defined, it is then placed in the <code>Backlog</code> column. An issue from the <code>backlog</code> can be moved to a specific release when someone volunteers to take ownership of the issue.</p> <p>An inactive issue in one of the releases (marked as stale) can be moved back to the <code>Backlog</code> column, and issues in the <code>feature requests</code> column that are not actual feature proposals but issues related to Ubiquity repository can be removed from the project.</p>"},{"location":"processes/triage/","title":"Title","text":"<p>Ubiquity Issue Triage Process</p>"},{"location":"processes/triage/#status","title":"Status","text":"<p>provisional</p>"},{"location":"processes/triage/#summary","title":"Summary","text":"<p>In order to ensure that issues reported by Ubiquity users are reviewed on a consistent basis, we should meet on a regular schedule in a live meeting to review newly submitted issues, and on some recurring basis look at potentially stale issues for consideration whether it should be closed, increase priority, etc.</p>"},{"location":"processes/triage/#proposal","title":"Proposal","text":"<p>During the triage process, the moderator should go through each of the subcategories listed below and apply the process to each issue.</p>"},{"location":"processes/triage/#new-issue-triage","title":"New Issue Triage","text":"<p>GitHub Search Query: <code>archived:false no:label is:issue sort:created-asc is:open</code></p> <ul> <li>Evalulate if the issue is still relevant.</li> <li>If not, close the issue.</li> <li>Determine the kind, and apply the right label. For example: bug, feature, etc.</li> <li>Make a best guess at priority, if the issue isn't actively being   worked on</li> <li>If needed, ask for more information from the reporter or a   developer. Label this issue <code>priority/awaiting-evidence</code>.</li> <li>Mark trivial issues as <code>good first issue</code></li> </ul>"},{"location":"processes/triage/#awaiting-evidence","title":"Awaiting Evidence","text":"<p>GitHub Search Query:<code>archived:false  is:issue sort:created-asc is:open label:priority/awaiting-more-evidence</code></p> <ul> <li>Review if the required evidence has been provided, if so, change the   priority/kind as needed, or close the issue if resolved.</li> </ul>"},{"location":"processes/triage/#stale-issues","title":"Stale Issues","text":"<p>GitHub Search Query: <code>archived:false is:issue sort:created-asc is:open label:lifecycle/stale</code></p>"},{"location":"reference/K3s/","title":"K3s Lightweight Kubernetes made ready for production","text":"<p>This repo contains the playbook needed to set up an on-premises K3s cluster and securize it.</p> <p>The accompanying blog for this project is found here: https://digitalis.io/blog/kubernetes/k3s-lightweight-kubernetes-made-ready-for-production-part-1/</p>"},{"location":"reference/K3s/#included-roles","title":"Included roles","text":""},{"location":"reference/K3s/#hardening","title":"Hardening","text":"<p>There is an hardening role, that will target your nodes and securize them using best pratices from CIS Benchmark and STIG Guidelines. You can use your own role or one of the official ansible available roles like:</p> <ul> <li>https://github.com/ansible-lockdown/RHEL8-STIG/</li> <li>https://github.com/ansible-lockdown/RHEL8-CIS/</li> </ul> <p>It is possible to tweak the hardening using host-vars, mainly packages names (that depends on your distribution of choiche):</p> <pre><code>aide_package: 'aide'\nauditd_package: 'audit'\nmodprobe_package: 'kmod'\nunwanted_pkg:\n  - mcstrans\n  - rsh\n  - rsh-server\n  - setroubleshoot\n  - telnet-server\n  - talk\n  - tftp\n  - tftp-server\n  - xinetd\n  - ypserv\n\nkernel_packages:\n  - kernel\n  - kernel-headers\n  - kernel-devel\n</code></pre> <p>And various tweaks on password aging, sessions timeout and so on.</p>"},{"location":"reference/K3s/#k3s-dependencies","title":"K3s Dependencies","text":"<p>This one will take care of setting up your cluster for K3s. Also here, adjust packages names accoring to your distribution of choiche:</p> <pre><code>k3s_dependencies:\n  - conntrack-tools\n  - curl\n  - ebtables\n  - epel-release\n  - ethtool\n  - gawk\n  - grep\n  - ipvsadm\n  - iscsi-initiator-utils\n  - libseccomp\n  - socat\n  - util-linux\n</code></pre>"},{"location":"reference/K3s/#dual-network","title":"Dual Network","text":"<p>It is highly recommended to follow an internal/external network layout for your cluster, as showed in this little diagram</p> <p></p> <p>To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme</p> <pre><code>external_interface: eth0\ninternal_interface: eth1\n</code></pre> <p>Also you can decide here what CIDR should your cluster use</p> <pre><code>cluster_cidr: 10.43.0.0/16\nservice_cidr: 10.44.0.0/16\n</code></pre>"},{"location":"reference/K3s/#k3s-deploy","title":"K3s Deploy","text":"<p>Lots of customization here, you can configure your Kubernetes cluster version</p> <pre><code>k3s_version: v1.20.5+k3s1\n</code></pre> <p>You can configure your ingress hostnames, if not specified (default) it will use nip.io to resolve your IPs</p> <pre><code>ingress_hostname: your.dns.name.io\n</code></pre> <p>You can (really have to) configure your MetalLB ip ranges here</p> <pre><code>metallb_external_ip_range: 192.168.1.200-192.168.1.240\nmetallb_internal_ip_range: 10.10.90.100-10.10.90.240\n</code></pre> <p>Still referring to a dual-network layout. Just leave the internal one empty if not using a dual-layout one</p> <p>You can then customize the keepalive VIP and interface</p> <pre><code>keepalived_interface: eth0\nkeepalived_addr_cidr: 192.168.122.100/24\nkeepalived_ip: 192.168.122.100\n</code></pre> <p>And then a plethora of configs possible for falco sidekick. Refer to their docs (https://github.com/falcosecurity/falcosidekick) to customize your integrations</p> <pre><code>falco_security_enabled: yes\nfalco_sidekick_slack: \"\"\nfalco_sidekick_slack_priority: \"warning\"\nfalco_sidekick_alertmanager: \"...\"\nfalco_sidekick_alertmanager_priority: \"...\"\nfalco_sidekick_discord: \"...\"\nfalco_sidekick_discord_priority: \"...\"\nfalco_sidekick_googlechat: \"...\"\nfalco_sidekick_googlechat_priority: \"...\"\nfalco_sidekick_kubeless_function: \"...\"\nfalco_sidekick_kubeless_namespace: \"...\"\nfalco_sidekick_kubeless_priority: \"...\"\nfalco_sidekick_mattermost: \"...\"\nfalco_sidekick_mattermost_priority: \"...\"\nfalco_sidekick_rocketchat: \"...\"\nfalco_sidekick_rocketchat_priority: \"...\"\nfalco_sidekick_slack: \"...\"\nfalco_sidekick_slack_priority: \"...\"\nfalco_sidekick_teams: \"...\"\nfalco_sidekick_teams_priority: \"...\"\n</code></pre>"},{"location":"reference/K3s/#usage","title":"Usage","text":"<p>The playbook and inventory are ready to use, just start from the example inventory <code>./inventory-k3s.yml</code> and fill your secrets and infrastructure IPs (metallb ranges, host IPs, interface naming etc...)</p>"},{"location":"reference/K3s/#final-result","title":"Final Result","text":"<p>This is the cluster layout at the end of the provisioning. Bear in mind that this is customizable in both versions and components</p> <p></p>"},{"location":"reference/K3s/Ansible/","title":"ansible-role-k3s","text":"<p>This document describes a number of ways of consuming this Ansible role for use in your own k3s deployments. It will not be able to cover every use case scenario but will provide some common example configurations.</p>"},{"location":"reference/K3s/Ansible/#requirements","title":"Requirements","text":"<p>Before you start you will need an Ansible controller. This can either be your workstation, or a dedicated system that you have access to. The instructions in this documentation assume you are using <code>ansible</code> CLI, there are no instructions available for Ansible Tower at this time.</p> <p>Follow the below guide to get Ansible installed.</p> <p>https://docs.ansible.com/ansible/latest/installation_guide/index.html</p>"},{"location":"reference/K3s/Ansible/#quickstart","title":"Quickstart","text":"<p>Below are quickstart examples for a single node k3s server, a k3s cluster with a single control node and HA k3s cluster. These represent the bare minimum configuration.</p> <ul> <li>Single node k3s</li> <li>Simple k3s cluster</li> <li>HA k3s cluster using embedded etcd</li> </ul>"},{"location":"reference/K3s/Ansible/#example-configurations-and-operations","title":"Example configurations and operations","text":""},{"location":"reference/K3s/Ansible/#configuration","title":"Configuration","text":"<ul> <li>Setting up 2-node HA control plane with external datastore</li> <li>Provision multiple standalone k3s nodes</li> <li>Set node labels and component arguments</li> <li>Use an alternate CNI</li> <li>IPv4/IPv6 Dual-Stack config</li> <li>Start K3S after another service</li> </ul>"},{"location":"reference/K3s/Ansible/#operations","title":"Operations","text":"<ul> <li>Stop/Start a cluster</li> <li>Updating k3s</li> <li>Extending a cluster</li> <li>Shrinking a cluster</li> </ul>"},{"location":"reference/K3s/quickstart-cluster/","title":"Quickstart: K3s cluster with a single control node","text":"<p>This is the quickstart guide to creating your own k3s cluster with one control plane node. This control plane node will also be a worker.</p> <p>:hand: This example requires your Ansible user to be able to connect to the servers over SSH using key-based authentication. The user is also has an entry in a sudoers file that allows privilege escalation without requiring a password.</p> <p>To test this is the case, run the following check replacing <code>&lt;ansible_user&gt;</code> and <code>&lt;server_name&gt;</code>. The expected output is <code>Works</code></p> <p><code>ssh &lt;ansible_user&gt;@&lt;server_name&gt; 'sudo cat /etc/shadow &gt;/dev/null &amp;&amp; echo \"Works\"'</code></p> <p>For example:</p> <pre><code>[ xmanning@dreadfort:~/git/kubernetes-playground ] (master) $ ssh ansible@kube-0 'sudo cat /etc/shadow &gt;/dev/null &amp;&amp; echo \"Works\"'\nWorks\n[ xmanning@dreadfort:~/git/kubernetes-playground ] (master) $\n</code></pre>"},{"location":"reference/K3s/quickstart-cluster/#directory-structure","title":"Directory structure","text":"<p>Our working directory will have the following files:</p> <pre><code>kubernetes-playground/\n  |_ inventory.yml\n  |_ cluster.yml\n</code></pre>"},{"location":"reference/K3s/quickstart-cluster/#inventory","title":"Inventory","text":"<p>Here's a YAML based example inventory for our servers called <code>inventory.yml</code>:</p> <pre><code>---\n\nk3s_cluster:\n  hosts:\n    kube-0:\n      ansible_user: ansible\n      ansible_host: 10.10.9.2\n      ansible_python_interpreter: /usr/bin/python3\n    kube-1:\n      ansible_user: ansible\n      ansible_host: 10.10.9.3\n      ansible_python_interpreter: /usr/bin/python3\n    kube-2:\n      ansible_user: ansible\n      ansible_host: 10.10.9.4\n      ansible_python_interpreter: /usr/bin/python3\n</code></pre> <p>We can test this works with <code>ansible -i inventory.yml -m ping all</code>, expected result:</p> <pre><code>kube-0 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nkube-1 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nkube-2 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"reference/K3s/quickstart-cluster/#playbook","title":"Playbook","text":"<p>Here is our playbook for the k3s cluster (<code>cluster.yml</code>):</p> <pre><code>---\n\n- name: Build a cluster with a single control node\n  hosts: k3s_cluster\n  vars:\n    k3s_become: true\n  roles:\n    - role: xanmanning.k3s\n</code></pre>"},{"location":"reference/K3s/quickstart-cluster/#execution","title":"Execution","text":"<p>To execute the playbook against our inventory file, we will run the following command:</p> <p><code>ansible-playbook -i inventory.yml cluster.yml</code></p> <p>The output we can expect is similar to the below, with no failed or unreachable nodes. The default behavior of this role is to delegate the first play host as the control node, so kube-0 will have more changed tasks than others:</p> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=56   changed=11   unreachable=0    failed=0    skipped=28   rescued=0    ignored=0\nkube-1                     : ok=43   changed=10   unreachable=0    failed=0    skipped=32   rescued=0    ignored=0\nkube-2                     : ok=43   changed=10   unreachable=0    failed=0    skipped=32   rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/quickstart-cluster/#testing","title":"Testing","text":"<p>After logging into kube-0, we can test that k3s is running across the cluster, that all nodes are ready and that everything is ready to execute our Kubernetes workloads by running the following:</p> <ul> <li><code>sudo kubectl get nodes -o wide</code></li> <li><code>sudo kubectl get pods -o wide --all-namespaces</code></li> </ul> <p>:hand: Note we are using <code>sudo</code> because we need to be root to access the kube config for this node. This behavior can be changed with specifying <code>write-kubeconfig-mode: '0644'</code> in <code>k3s_server</code>.</p> <p>Get Nodes:</p> <pre><code>ansible@kube-0:~$ sudo kubectl get nodes -o wide\nNAME     STATUS   ROLES    AGE   VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nkube-0   Ready    master   34s   v1.19.4+k3s1   10.0.2.15     &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.1-k3s1\nkube-2   Ready    &lt;none&gt;   14s   v1.19.4+k3s1   10.0.2.17     &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.1-k3s1\nkube-1   Ready    &lt;none&gt;   14s   v1.19.4+k3s1   10.0.2.16     &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.1-k3s1\nansible@kube-0:~$\n</code></pre> <p>Get Pods:</p> <pre><code>ansible@kube-0:~$ sudo kubectl get pods -o wide --all-namespaces\nNAMESPACE     NAME                                     READY   STATUS      RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES\nkube-system   local-path-provisioner-7ff9579c6-72j8x   1/1     Running     0          55s   10.42.2.2   kube-1   &lt;none&gt;           &lt;none&gt;\nkube-system   metrics-server-7b4f8b595-lkspj           1/1     Running     0          55s   10.42.1.2   kube-2   &lt;none&gt;           &lt;none&gt;\nkube-system   helm-install-traefik-b6vnt               0/1     Completed   0          55s   10.42.0.3   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-66c464876b-llsh7                 1/1     Running     0          55s   10.42.0.2   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   svclb-traefik-jrqg7                      2/2     Running     0          27s   10.42.1.3   kube-2   &lt;none&gt;           &lt;none&gt;\nkube-system   svclb-traefik-gh65q                      2/2     Running     0          27s   10.42.0.4   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   svclb-traefik-5z7zp                      2/2     Running     0          27s   10.42.2.3   kube-1   &lt;none&gt;           &lt;none&gt;\nkube-system   traefik-5dd496474-l2k74                  1/1     Running     0          27s   10.42.1.4   kube-2   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"reference/K3s/quickstart-ha-cluster/","title":"Quickstart: K3s cluster with a HA control plane using embedded etcd","text":"<p>This is the quickstart guide to creating your own 3 node k3s cluster with a highly available control plane using the embedded etcd datastore. The control plane will all be workers as well.</p> <p>:hand: This example requires your Ansible user to be able to connect to the servers over SSH using key-based authentication. The user is also has an entry in a sudoers file that allows privilege escalation without requiring a password.</p> <p>To test this is the case, run the following check replacing <code>&lt;ansible_user&gt;</code> and <code>&lt;server_name&gt;</code>. The expected output is <code>Works</code></p> <p><code>ssh &lt;ansible_user&gt;@&lt;server_name&gt; 'sudo cat /etc/shadow &gt;/dev/null &amp;&amp; echo \"Works\"'</code></p> <p>For example:</p> <pre><code>[ xmanning@dreadfort:~/git/kubernetes-playground ] (master) $ ssh ansible@kube-0 'sudo cat /etc/shadow &gt;/dev/null &amp;&amp; echo \"Works\"'\nWorks\n[ xmanning@dreadfort:~/git/kubernetes-playground ] (master) $\n</code></pre>"},{"location":"reference/K3s/quickstart-ha-cluster/#directory-structure","title":"Directory structure","text":"<p>Our working directory will have the following files:</p> <pre><code>kubernetes-playground/\n  |_ inventory.yml\n  |_ ha_cluster.yml\n</code></pre>"},{"location":"reference/K3s/quickstart-ha-cluster/#inventory","title":"Inventory","text":"<p>Here's a YAML based example inventory for our servers called <code>inventory.yml</code>:</p> <pre><code>---\n\n# We're adding k3s_control_node to each host, this can be done in host_vars/\n# or group_vars/ as well - but for simplicity we are setting it here.\nk3s_cluster:\n  hosts:\n    kube-0:\n      ansible_user: ansible\n      ansible_host: 10.10.9.2\n      ansible_python_interpreter: /usr/bin/python3\n      k3s_control_node: true\n    kube-1:\n      ansible_user: ansible\n      ansible_host: 10.10.9.3\n      ansible_python_interpreter: /usr/bin/python3\n      k3s_control_node: true\n    kube-2:\n      ansible_user: ansible\n      ansible_host: 10.10.9.4\n      ansible_python_interpreter: /usr/bin/python3\n      k3s_control_node: true\n</code></pre> <p>We can test this works with <code>ansible -i inventory.yml -m ping all</code>, expected result:</p> <pre><code>kube-0 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nkube-1 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nkube-2 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"reference/K3s/quickstart-ha-cluster/#playbook","title":"Playbook","text":"<p>Here is our playbook for the k3s cluster (<code>ha_cluster.yml</code>):</p> <pre><code>---\n\n- name: Build a cluster with HA control plane\n  hosts: k3s_cluster\n  vars:\n    k3s_become: true\n    k3s_etcd_datastore: true\n    k3s_use_experimental: true  # Note this is required for k3s &lt; v1.19.5+k3s1\n  roles:\n    - role: xanmanning.k3s\n</code></pre>"},{"location":"reference/K3s/quickstart-ha-cluster/#execution","title":"Execution","text":"<p>To execute the playbook against our inventory file, we will run the following command:</p> <p><code>ansible-playbook -i inventory.yml ha_cluster.yml</code></p> <p>The output we can expect is similar to the below, with no failed or unreachable nodes. The default behavior of this role is to delegate the first play host as the primary control node, so kube-0 will have more changed tasks than others:</p> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=53   changed=8    unreachable=0    failed=0    skipped=30   rescued=0    ignored=0\nkube-1                     : ok=47   changed=10   unreachable=0    failed=0    skipped=28   rescued=0    ignored=0\nkube-2                     : ok=47   changed=9    unreachable=0    failed=0    skipped=28   rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/quickstart-ha-cluster/#testing","title":"Testing","text":"<p>After logging into any of the servers (it doesn't matter), we can test that k3s is running across the cluster, that all nodes are ready and that everything is ready to execute our Kubernetes workloads by running the following:</p> <ul> <li><code>sudo kubectl get nodes -o wide</code></li> <li><code>sudo kubectl get pods -o wide --all-namespaces</code></li> </ul> <p>:hand: Note we are using <code>sudo</code> because we need to be root to access the kube config for this node. This behavior can be changed with specifying <code>write-kubeconfig-mode: '0644'</code> in <code>k3s_server</code>.</p> <p>Get Nodes:</p> <pre><code>ansible@kube-0:~$ sudo kubectl get nodes -o wide\nNAME     STATUS   ROLES         AGE     VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nkube-0   Ready    etcd,master   2m58s   v1.19.4+k3s1   10.10.9.2     10.10.9.2     Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.1-k3s1\nkube-1   Ready    etcd,master   2m22s   v1.19.4+k3s1   10.10.9.3     10.10.9.3     Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.1-k3s1\nkube-2   Ready    etcd,master   2m10s   v1.19.4+k3s1   10.10.9.4     10.10.9.4     Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.1-k3s1\n</code></pre> <p>Get Pods:</p> <pre><code>ansible@kube-0:~$ sudo kubectl get pods -o wide --all-namespaces\nNAMESPACE     NAME                                     READY   STATUS      RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES\nkube-system   coredns-66c464876b-rhgn6                 1/1     Running     0          3m38s   10.42.0.2   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   helm-install-traefik-vwglv               0/1     Completed   0          3m39s   10.42.0.3   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   local-path-provisioner-7ff9579c6-d5xpb   1/1     Running     0          3m38s   10.42.0.5   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   metrics-server-7b4f8b595-nhbt8           1/1     Running     0          3m38s   10.42.0.4   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   svclb-traefik-9lzcq                      2/2     Running     0          2m56s   10.42.1.2   kube-1   &lt;none&gt;           &lt;none&gt;\nkube-system   svclb-traefik-vq487                      2/2     Running     0          2m45s   10.42.2.2   kube-2   &lt;none&gt;           &lt;none&gt;\nkube-system   svclb-traefik-wkwkk                      2/2     Running     0          3m1s    10.42.0.7   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   traefik-5dd496474-lw6x8                  1/1     Running     0          3m1s    10.42.0.6   kube-0   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"reference/K3s/quickstart-single-node/","title":"Quickstart: K3s single node","text":"<p>This is the quickstart guide to creating your own single-node k3s \"cluster\".</p> <p>:hand: This example requires your Ansible user to be able to connect to the server over SSH using key-based authentication. The user is also has an entry in a sudoers file that allows privilege escalation without requiring a password.</p> <p>To test this is the case, run the following check replacing <code>&lt;ansible_user&gt;</code> and <code>&lt;server_name&gt;</code>. The expected output is <code>Works</code></p> <p><code>ssh &lt;ansible_user&gt;@&lt;server_name&gt; 'sudo cat /etc/shadow &gt;/dev/null &amp;&amp; echo \"Works\"'</code></p> <p>For example:</p> <pre><code>[ xmanning@dreadfort:~/git/kubernetes-playground ] (master) $ ssh ansible@kube-0 'sudo cat /etc/shadow &gt;/dev/null &amp;&amp; echo \"Works\"'\nWorks\n[ xmanning@dreadfort:~/git/kubernetes-playground ] (master) $\n</code></pre>"},{"location":"reference/K3s/quickstart-single-node/#directory-structure","title":"Directory structure","text":"<p>Our working directory will have the following files:</p> <pre><code>kubernetes-playground/\n  |_ inventory.yml\n  |_ single_node.yml\n</code></pre>"},{"location":"reference/K3s/quickstart-single-node/#inventory","title":"Inventory","text":"<p>Here's a YAML based example inventory for our server called <code>inventory.yml</code>:</p> <pre><code>---\n\nk3s_cluster:\n  hosts:\n    kube-0:\n      ansible_user: ansible\n      ansible_host: 10.10.9.2\n      ansible_python_interpreter: /usr/bin/python3\n</code></pre> <p>We can test this works with <code>ansible -i inventory.yml -m ping all</code>, expected result:</p> <pre><code>kube-0 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"reference/K3s/quickstart-single-node/#playbook","title":"Playbook","text":"<p>Here is our playbook for a single node k3s cluster (<code>single_node.yml</code>):</p> <pre><code>---\n\n- name: Build a single node k3s cluster\n  hosts: kube-0\n  vars:\n    k3s_become: true\n  roles:\n    - role: xanmanning.k3s\n</code></pre>"},{"location":"reference/K3s/quickstart-single-node/#execution","title":"Execution","text":"<p>To execute the playbook against our inventory file, we will run the following command:</p> <p><code>ansible-playbook -i inventory.yml single_node.yml</code></p> <p>The output we can expect is similar to the below, with no failed or unreachable nodes:</p> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=39   changed=8    unreachable=0    failed=0    skipped=39   rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/quickstart-single-node/#testing","title":"Testing","text":"<p>After logging into the server, we can test that k3s is running and that it is ready to execute our Kubernetes workloads by running the following:</p> <ul> <li><code>sudo kubectl get nodes</code></li> <li><code>sudo kubectl get pods -o wide --all-namespaces</code></li> </ul> <p>:hand: Note we are using <code>sudo</code> because we need to be root to access the kube config for this node. This behavior can be changed with specifying <code>write-kubeconfig-mode: '0644'</code> in <code>k3s_server</code>.</p> <p>Get Nodes:</p> <pre><code>ansible@kube-0:~$ sudo kubectl get nodes\nNAME     STATUS   ROLES    AGE     VERSION\nkube-0   Ready    master   5m27s   v1.19.4+k3s\nansible@kube-0:~$\n</code></pre> <p>Get Pods:</p> <pre><code>ansible@kube-0:~$ sudo kubectl get pods --all-namespaces -o wide\nNAMESPACE     NAME                                     READY   STATUS      RESTARTS   AGE     IP          NODE     NOMINATED NODE   READINESS GATES\nkube-system   metrics-server-7b4f8b595-k692h           1/1     Running     0          9m38s   10.42.0.2   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   local-path-provisioner-7ff9579c6-5lgzb   1/1     Running     0          9m38s   10.42.0.3   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-66c464876b-xg42q                 1/1     Running     0          9m38s   10.42.0.5   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   helm-install-traefik-tdpcs               0/1     Completed   0          9m38s   10.42.0.4   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   svclb-traefik-hk248                      2/2     Running     0          9m4s    10.42.0.7   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system   traefik-5dd496474-bf4kv                  1/1     Running     0          9m4s    10.42.0.6   kube-0   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"reference/K3s/configuration/2-node-ha-ext-datastore/","title":"2 Node HA Control Plane with external database","text":"<p>For this configuration we are deploying a highly available control plane composed of two control nodes. This can be achieved with embedded etcd, however etcd ideally has an odd number of nodes.</p> <p>The example below will use an external PostgreSQL datastore to store the cluster state information.</p> <p>Main guide: https://rancher.com/docs/k3s/latest/en/installation/ha/</p>"},{"location":"reference/K3s/configuration/2-node-ha-ext-datastore/#architecture","title":"Architecture","text":"<pre><code>                   +-------------------+\n                   | Load Balancer/VIP |\n                   +---------+---------+\n                             |\n                             |\n                             |\n                             |\n         +------------+      |      +------------+\n         |            |      |      |            |\n+--------+ control-01 +&lt;-----+-----&gt;+ control-02 |\n|        |            |             |            |\n|        +-----+------+             +------+-----+\n|              |                           |\n|              +-------------+-------------+\n|              |             |             |\n|       +------v----+  +-----v-----+  +----v------+\n|       |           |  |           |  |           |\n|       | worker-01 |  | worker-02 |  | worker-03 |\n|       |           |  |           |  |           |\n|       +-----------+  +-----------+  +-----------+\n|\n|                   +-------+  +-------+\n|                   |       |  |       |\n+-------------------&gt; db-01 +--+ db-02 |\n                    |       |  |       |\n                    +-------+  +-------+\n</code></pre>"},{"location":"reference/K3s/configuration/2-node-ha-ext-datastore/#required-components","title":"Required Components","text":"<ul> <li>Load balancer</li> <li>2 control plane nodes</li> <li>1 or more worker nodes</li> <li>PostgreSQL Database (replicated, or Linux HA Cluster).</li> </ul>"},{"location":"reference/K3s/configuration/2-node-ha-ext-datastore/#configuration","title":"Configuration","text":"<p>For your control nodes, you will need to instruct the control plane of the PostgreSQL datastore endpoint and set <code>k3s_registration_address</code> to be the hostname or IP of your load balancer or VIP.</p> <p>Below is the example for PostgreSQL, it is possible to use MySQL or an Etcd cluster as well. Consult the below guide for using alternative datastore endpoints.</p> <p>https://rancher.com/docs/k3s/latest/en/installation/datastore/#datastore-endpoint-format-and-functionality</p> <pre><code>---\n\nk3s_server:\n  datastore-endpoint: postgres://postgres:verybadpass@database:5432/postgres?sslmode=disable\n  node-taint:\n    - \"k3s-controlplane=true:NoExecute\"\n</code></pre> <p>Your worker nodes need to know how to connect to the control plane, this is defined by setting <code>k3s_registration_address</code> to the hostname or IP address of the load balancer.</p> <pre><code>---\n\nk3s_registration_address: control.examplek3s.com\n</code></pre>"},{"location":"reference/K3s/configuration/ipv4-ipv6-dual-stack/","title":"IPv4 and IPv6 Dual-stack config","text":"<p>If you need to run your K3S cluster with both IPv4 and IPv6 address ranges you will need to configure the <code>k3s_server.cluster-cidr</code> and <code>k3s_server.service-cidr</code> values specifying both ranges.</p> <p>:hand: if you are using <code>k3s&lt;1.23</code> you will need to use a different CNI as dual-stack support is not available in Flannel.</p> <p>Below is a noddy example:</p> <pre><code>---\n\nk3s_server:\n  # Using Calico on k3s&lt;1.23 so Flannel needs to be disabled.\n  flannel-backend: 'none'\n  # Format: ipv4/cidr,ipv6/cidr\n  cluster-cidr: 10.42.0.0/16,fc00:a0::/64\n  service-cidr: 10.43.0.0/16,fc00:a1::/64\n</code></pre>"},{"location":"reference/K3s/configuration/multiple-standalone-k3s-nodes/","title":"Multiple standalone K3s nodes","text":"<p>This is an example of when you might want to configure multiple standalone k3s nodes simultaneously. For this we will assume a hypothetical situation where we are configuring 25 Raspberry Pis to deploy to our shop floors.</p> <p>Each Rasperry Pi will be configured as a standalone IoT device hosting an application that will push data to head office.</p>"},{"location":"reference/K3s/configuration/multiple-standalone-k3s-nodes/#architecture","title":"Architecture","text":"<pre><code>+-------------+\n|             |\n|   Node-01   +-+\n|             | |\n+--+----------+ +-+\n   |            | |\n   +--+---------+ +-+\n      |           | |\n      +--+--------+ |\n         |          |  Node-N\n         +----------+\n</code></pre>"},{"location":"reference/K3s/configuration/multiple-standalone-k3s-nodes/#configuration","title":"Configuration","text":"<p>Below is our example inventory of 200 nodes (Truncated):</p> <pre><code>---\n\nk3s_workers:\n   hosts:\n     kube-0:\n       ansible_user: ansible\n       ansible_host: 10.10.9.2\n       ansible_python_interpreter: /usr/bin/python3\n     kube-1:\n       ansible_user: ansible\n       ansible_host: 10.10.9.3\n       ansible_python_interpreter: /usr/bin/python3\n     kube-2:\n       ansible_user: ansible\n       ansible_host: 10.10.9.4\n       ansible_python_interpreter: /usr/bin/python3\n\n     # ..... SNIP .....\n\n     kube-199:\n       ansible_user: ansible\n       ansible_host: 10.10.9.201\n       ansible_python_interpreter: /usr/bin/python3\n     kube-200:\n       ansible_user: ansible\n       ansible_host: 10.10.9.202\n       ansible_python_interpreter: /usr/bin/python3\n</code></pre> <p>In our <code>group_vars/</code> (or as <code>vars:</code> in our playbook), we will need to set the <code>k3s_build_cluster</code> variable to <code>false</code>. This will stop the role from attempting to cluster all 200 nodes, instead it will install k3s across each node as as 200 standalone servers.</p> <pre><code>---\n\nk3s_build_cluster: false\n</code></pre>"},{"location":"reference/K3s/configuration/node-labels-and-component-args/","title":"Configure node labels and component arguments","text":"<p>The following command line arguments can be specified multiple times with <code>key=value</code> pairs:</p> <ul> <li><code>--kube-kubelet-arg</code></li> <li><code>--kube-proxy-arg</code></li> <li><code>--kube-apiserver-arg</code></li> <li><code>--kube-scheduler-arg</code></li> <li><code>--kube-controller-manager-arg</code></li> <li><code>--kube-cloud-controller-manager-arg</code></li> <li><code>--node-label</code></li> <li><code>--node-taint</code></li> </ul> <p>In the config file, this is done by defining a list of values for each command like argument, for example:</p> <pre><code>---\n\nk3s_server:\n  # Set the plugins registry directory\n  kubelet-arg:\n    - \"volume-plugin-dir=/var/lib/rancher/k3s/agent/kubelet/plugins_registry\"\n  # Set the pod eviction timeout and node monitor grace period\n  kube-controller-manager-arg:\n    - \"pod-eviction-timeout=2m\"\n    - \"node-monitor-grace-period=30s\"\n  # Set API server feature gate\n  kube-apiserver-arg:\n    - \"feature-gates=RemoveSelfLink=false\"\n  # Laels to apply to a node\n  node-label:\n    - \"NodeTier=development\"\n    - \"NodeLocation=eu-west-2a\"\n  # Stop k3s control plane having workloads scheduled on them\n  node-taint:\n    - \"k3s-controlplane=true:NoExecute\"\n</code></pre>"},{"location":"reference/K3s/configuration/systemd-config/","title":"systemd config","text":"<p>Below are examples to tweak how and when K3S starts up.</p>"},{"location":"reference/K3s/configuration/systemd-config/#wanted-service-units","title":"Wanted service units","text":"<p>In this example, we're going to start K3S after Wireguard. Our example server has a Wireguard connection <code>wg0</code>. We are using \"wants\" rather than \"requires\" as it's a weaker requirement that Wireguard must be running. We then want K3S to start after Wireguard has started.</p> <pre><code>---\n\nk3s_service_wants:\n  - wg-quick@wg0.service\nk3s_service_after:\n  - wg-quick@wg0.service\n</code></pre>"},{"location":"reference/K3s/configuration/use-an-alternat-cni/","title":"Use an alternate CNI","text":"<p>K3S ships with Flannel, however sometimes you want an different CNI such as Calico, Canal or Weave Net. To do this you will need to disable Flannel with <code>flannel-backend: \"none\"</code>, specify a <code>cluster-cidr</code> and add your CNI manifests to the <code>k3s_server_manifests_templates</code>.</p>"},{"location":"reference/K3s/configuration/use-an-alternat-cni/#calico-example","title":"Calico example","text":"<p>The below is based on the Calico quickstart documentation.</p> <p>Steps:</p> <ol> <li>Download <code>tigera-operator.yaml</code> to the manifests directory.</li> <li>Download <code>custom-resources.yaml</code> to the manifests directory.</li> <li>Choose a <code>cluster-cidr</code> (we are using 192.168.0.0/16)</li> <li>Set <code>k3s_server</code> and <code>k3s_server_manifest_templates</code> as per the below,      ensure the paths to manifests are correct for your project repo.</li> </ol> <pre><code>---\n\n# K3S Server config, don't deploy flannel and set cluster pod CIDR.\nk3s_server:\n  cluster-cidr: 192.168.0.0/16\n  flannel-backend: \"none\"\n\n# Deploy the following k3s server templates.\nk3s_server_manifests_templates:\n  - \"manifests/calico/tigera-operator.yaml\"\n  - \"manifests/calico/custom-resources.yaml\"\n</code></pre> <p>All nodes should come up as \"Ready\", below is a 3-node cluster:</p> <pre><code> $ kubectl get nodes -o wide -w\nNAME     STATUS   ROLES                       AGE    VERSION        INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nkube-0   Ready    control-plane,etcd,master   114s   v1.20.2+k3s1   10.10.9.2     10.10.9.2     Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.3-k3s1\nkube-1   Ready    control-plane,etcd,master   80s    v1.20.2+k3s1   10.10.9.3     10.10.9.3     Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.3-k3s1\nkube-2   Ready    control-plane,etcd,master   73s    v1.20.2+k3s1   10.10.9.4     10.10.9.4     Ubuntu 20.04.1 LTS   5.4.0-56-generic   containerd://1.4.3-k3s1\n</code></pre> <p>Pods should be deployed with deployed within the CIDR specified in our config file.</p> <pre><code>$ kubectl get pods -o wide -A\nNAMESPACE         NAME                                      READY   STATUS      RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES\ncalico-system     calico-kube-controllers-cfb4ff54b-8rp8r   1/1     Running     0          5m4s    192.168.145.65   kube-0   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-node-2cm2m                         1/1     Running     0          5m4s    10.10.9.2        kube-0   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-node-2s6lx                         1/1     Running     0          4m42s   10.10.9.4        kube-2   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-node-zwqjz                         1/1     Running     0          4m49s   10.10.9.3        kube-1   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-typha-7b6747d665-78swq             1/1     Running     0          3m5s    10.10.9.4        kube-2   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-typha-7b6747d665-8ff66             1/1     Running     0          3m5s    10.10.9.3        kube-1   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-typha-7b6747d665-hgplx             1/1     Running     0          5m5s    10.10.9.2        kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system       coredns-854c77959c-6qhgt                  1/1     Running     0          5m20s   192.168.145.66   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system       helm-install-traefik-4czr9                0/1     Completed   0          5m20s   192.168.145.67   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system       metrics-server-86cbb8457f-qcxf5           1/1     Running     0          5m20s   192.168.145.68   kube-0   &lt;none&gt;           &lt;none&gt;\nkube-system       traefik-6f9cbd9bd4-7h4rl                  1/1     Running     0          2m50s   192.168.126.65   kube-1   &lt;none&gt;           &lt;none&gt;\ntigera-operator   tigera-operator-b6c4bfdd9-29hhr           1/1     Running     0          5m20s   10.10.9.2        kube-0   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"reference/K3s/operations/extending-a-cluster/","title":"Extending a cluster","text":"<p>This document describes the method for extending an cluster with new worker nodes.</p>"},{"location":"reference/K3s/operations/extending-a-cluster/#assumptions","title":"Assumptions","text":"<p>It is assumed that you have already deployed a k3s cluster using this role, you have an appropriately configured inventory and playbook to create the cluster.</p> <p>Below, our example inventory and playbook are as follows:</p> <ul> <li>inventory: <code>inventory.yml</code></li> <li>playbook: <code>cluster.yml</code></li> </ul> <p>Currently your <code>inventory.yml</code> looks like this, it has two nodes defined, <code>kube-0</code> (control node) and <code>kube-1</code> (worker node).</p> <pre><code>---\n\nk3s_cluster:\n  hosts:\n    kube-0:\n      ansible_user: ansible\n      ansible_host: 10.10.9.2\n      ansible_python_interpreter: /usr/bin/python3\n    kube-1:\n      ansible_user: ansible\n      ansible_host: 10.10.9.3\n      ansible_python_interpreter: /usr/bin/python3\n</code></pre>"},{"location":"reference/K3s/operations/extending-a-cluster/#method","title":"Method","text":"<p>We have our two nodes, one control, one worker. The goal is to extend this to add capacity by adding a new worker node, <code>kube-2</code>. To do this we will add the new node to our inventory.</p> <pre><code>---\n\nk3s_cluster:\n  hosts:\n    kube-0:\n      ansible_user: ansible\n      ansible_host: 10.10.9.2\n      ansible_python_interpreter: /usr/bin/python3\n    kube-1:\n      ansible_user: ansible\n      ansible_host: 10.10.9.3\n      ansible_python_interpreter: /usr/bin/python3\n    kube-2:\n      ansible_user: ansible\n      ansible_host: 10.10.9.4\n      ansible_python_interpreter: /usr/bin/python3\n</code></pre> <p>Once the new node has been added, you can re-run the automation to join it to the cluster. You should expect the majority of changes to the worker node being introduced to the cluster.</p> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=53   changed=1    unreachable=0    failed=0    skipped=30   rescued=0    ignored=0\nkube-1                     : ok=40   changed=1    unreachable=0    failed=0    skipped=35   rescued=0    ignored=0\nkube-2                     : ok=42   changed=10   unreachable=0    failed=0    skipped=35   rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/operations/shrinking-a-cluster/","title":"Shrinking a cluster","text":"<p>This document describes the method for shrinking a cluster, by removing a worker nodes.</p>"},{"location":"reference/K3s/operations/shrinking-a-cluster/#assumptions","title":"Assumptions","text":"<p>It is assumed that you have already deployed a k3s cluster using this role, you have an appropriately configured inventory and playbook to create the cluster.</p> <p>Below, our example inventory and playbook are as follows:</p> <ul> <li>inventory: <code>inventory.yml</code></li> <li>playbook: <code>cluster.yml</code></li> </ul> <p>Currently your <code>inventory.yml</code> looks like this, it has three nodes defined, <code>kube-0</code> (control node) and <code>kube-1</code>, <code>kube-2</code> (worker nodes).</p> <pre><code>---\n\nk3s_cluster:\n  hosts:\n    kube-0:\n      ansible_user: ansible\n      ansible_host: 10.10.9.2\n      ansible_python_interpreter: /usr/bin/python3\n    kube-1:\n      ansible_user: ansible\n      ansible_host: 10.10.9.3\n      ansible_python_interpreter: /usr/bin/python3\n    kube-2:\n      ansible_user: ansible\n      ansible_host: 10.10.9.4\n      ansible_python_interpreter: /usr/bin/python3\n</code></pre>"},{"location":"reference/K3s/operations/shrinking-a-cluster/#method","title":"Method","text":"<p>We have our three nodes, one control, two workers. The goal is to shrink this to remove excess capacity by offboarding the worker node <code>kube-2</code>. To do this we will set <code>kube-2</code> node to <code>k3s_state: uninstalled</code> in our inventory.</p> <pre><code>---\n\nk3s_cluster:\n  hosts:\n    kube-0:\n      ansible_user: ansible\n      ansible_host: 10.10.9.2\n      ansible_python_interpreter: /usr/bin/python3\n    kube-1:\n      ansible_user: ansible\n      ansible_host: 10.10.9.3\n      ansible_python_interpreter: /usr/bin/python3\n    kube-2:\n      ansible_user: ansible\n      ansible_host: 10.10.9.4\n      ansible_python_interpreter: /usr/bin/python3\n      k3s_state: uninstalled\n</code></pre> <p>What you will typically see is changes to your control plane (<code>kube-0</code>) and the node being removed (<code>kube-2</code>). The role will register the removal of the node with the cluster by draining the node and removing it from the cluster.</p> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=55   changed=2    unreachable=0    failed=0    skipped=28   rescued=0    ignored=0\nkube-1                     : ok=40   changed=0    unreachable=0    failed=0    skipped=35   rescued=0    ignored=0\nkube-2                     : ok=23   changed=2    unreachable=0    failed=0    skipped=17   rescued=0    ignored=1\n</code></pre>"},{"location":"reference/K3s/operations/stop-start-cluster/","title":"Stopping and Starting a cluster","text":"<p>This document describes the Ansible method for restarting a k3s cluster deployed by this role.</p>"},{"location":"reference/K3s/operations/stop-start-cluster/#assumptions","title":"Assumptions","text":"<p>It is assumed that you have already deployed a k3s cluster using this role, you have an appropriately configured inventory and playbook to create the cluster.</p> <p>Below, our example inventory and playbook are as follows:</p> <ul> <li>inventory: <code>inventory.yml</code></li> <li>playbook: <code>cluster.yml</code></li> </ul>"},{"location":"reference/K3s/operations/stop-start-cluster/#method","title":"Method","text":""},{"location":"reference/K3s/operations/stop-start-cluster/#start-cluster","title":"Start cluster","text":"<p>You can start the cluster using either of the following commands:</p> <ul> <li>Using the playbook: <code>ansible-playbook -i inventory.yml cluster.yml --become -e 'k3s_state=started'</code></li> <li>Using an ad-hoc command: <code>ansible -i inventory.yml -m service -a 'name=k3s state=started' --become all</code></li> </ul> <p>Below is example output, remember that Ansible is idempotent so re-running a command may not necessarily change the state.</p> <p>Playbook method output:</p> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=6    changed=0    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nkube-1                     : ok=6    changed=0    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nkube-2                     : ok=6    changed=0    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/operations/stop-start-cluster/#stop-cluster","title":"Stop cluster","text":"<p>You can stop the cluster using either of the following commands:</p> <ul> <li>Using the playbook: <code>ansible-playbook -i inventory.yml cluster.yml --become -e 'k3s_state=stopped'</code></li> <li>Using an ad-hoc command: <code>ansible -i inventory.yml -m service -a 'name=k3s state=stopped' --become all</code></li> </ul> <p>Below is example output, remember that Ansible is idempotent so re-running a command may not necessarily change the state.</p> <p>Playbook method output:</p> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=6    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nkube-1                     : ok=6    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\nkube-2                     : ok=6    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/operations/stop-start-cluster/#restart-cluster","title":"Restart cluster","text":"<p>Just like the <code>service</code> module, you can also specify <code>restarted</code> as a state. This will do <code>stop</code> followed by <code>start</code>.</p> <ul> <li>Using the playbook: <code>ansible-playbook -i inventory.yml cluster.yml --become -e 'k3s_state=restarted'</code></li> <li>Using an ad-hoc command: <code>ansible -i inventory.yml -m service -a 'name=k3s state=restarted' --become all</code></li> </ul> <pre><code>PLAY RECAP *******************************************************************************************************\nkube-0                     : ok=7    changed=1    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0\nkube-1                     : ok=7    changed=1    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0\nkube-2                     : ok=7    changed=1    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/operations/stop-start-cluster/#tips","title":"Tips","text":"<p>You can limit the targets by adding the <code>-l</code> flag to your <code>ansible-playbook</code> command, or simply target your ad-hoc commands. For example, in a 3 node cluster (called <code>kube-0</code>, <code>kube-1</code> and <code>kube-2</code>) we can limit the restart to <code>kube-1</code> and <code>kube-2</code> with the following:</p> <ul> <li>Using the playbook: <code>ansible-playbook -i inventory.yml cluster.yml --become -e 'k3s_state=restarted' -l \"kube-1,kube-2\"</code></li> <li>Using an ad-hoc command: <code>ansible -i inventory.yml -m service -a 'name=k3s state=restarted' --become \"kube-1,kube-2\"</code></li> </ul> <pre><code>PLAY RECAP ********************************************************************************************************\nkube-1                     : ok=7    changed=2    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0\nkube-2                     : ok=7    changed=2    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0\n</code></pre>"},{"location":"reference/K3s/operations/stop-start-cluster/#faq","title":"FAQ","text":"<ol> <li>Why might I use the <code>ansible-playbook</code> command over an ad-hoc command?<ul> <li>The stop/start tasks will be aware of configuration. As the role    develops, there might be some pre-tasks added to change how a cluster    is stopped or started.</li> </ul> </li> </ol>"},{"location":"reference/concepts/bootstrap-bare-metal-provisioning/","title":"Bare metal provisioning (PXE boot)","text":"<pre><code>flowchart TD\n  subgraph controller[Initial controller]\n    Ansible\n    dhcp[DHCP server]\n    tftp[TFTP server]\n    http[HTTP server]\n  end\n\n  machine[Bare metal machine]\n\n  Ansible -. 1 .-&gt; machine\n  machine &lt;-. 2, 3 .-&gt; dhcp\n  machine &lt;-. 4, 5 .-&gt; tftp\n  machine &lt;-. 6, 7 .-&gt; http</code></pre> <ol> <li>Ansible: Hey MAC address <code>xx:xx:xx:xx:xx:xx</code>, wake up!</li> <li>Machine: Hello everyone, I just woke up in network mode, could someone please show me how to boot?</li> <li>DHCP server: I hear you, here's your IP address, proceed to the next server to obtain your bootloader.</li> <li>Machine: Hello, could you please send me my bootloader?</li> <li>TFTP server: Here you go. Grab your boot configuration, kernel, and initial ramdisk as well.</li> <li>Machine: Hi, I just booted into my bootloader, and my boot parameters instructed me to get the installation instructions, packages, etc. from this site.</li> <li>HTTP server: It's all yours.</li> <li>Machine: Great, now I can install the OS and reboot!</li> </ol>"},{"location":"reference/concepts/certificate-management/","title":"Certificate management","text":"<p>Certificates are generated and managed by cert-manager with Let's Encrypt. By default certificates are valid for 90 days and will be renewed after 60 days.</p> <p>cert-manager watches <code>Ingress</code> resources across the cluster. When you create an <code>Ingress</code> with a supported annotation:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  name: foo\nspec:\n  rules:\n    - host: foo.example.com\n      # ...\n  tls:\n    - hosts:\n        - foo.example.com\n      secretName: foo-tls-certificate\n</code></pre> <pre><code>flowchart LR\n  User -- 6 --&gt; Ingress\n\n  subgraph cluster[Homelab cluster]\n    Ingress --- Secret\n    Ingress -. 1 .-&gt; Certificate\n    Certificate -. 5 .-&gt; Secret\n    Certificate -- 2 --&gt; CertificateRequest -- 3 --&gt; Order -- 4 --&gt; Challenge\n  end\n\n  Order -.- ACMEServer[ACME server]\n\n  subgraph dnsprovider[DNS provider]\n    TXT\n  end\n\n  Challenge -- 4.a --&gt; TXT\n  ACMEServer -.- Challenge\n  ACMEServer -. 4.b .-&gt; TXT</code></pre> <ol> <li>cert-manager creates a corresponding <code>Certificate</code> resources</li> <li>Based on the <code>Certificate</code> resource, cert-manager creates a <code>CertificateRequest</code> resource to request a signed certificate from the configured <code>ClusterIssuer</code></li> <li>The <code>CertificateRequest</code> will create an order with an ACME server (we use Let's Encrypt), which is represented by the <code>Order</code> resource</li> <li>Then cert-manager will perform a DNS-01 <code>Challenge</code>:<ol> <li>Create a DNS TXT record (contains a computed key)</li> <li>The ACME server retrieve this key via a DNS lookup and validate that we own the domain for the requested certificate</li> </ol> </li> <li>cert-manager stores the certificate (typically <code>tls.crt</code> and <code>tls.key</code>) in the <code>Secret</code> specified in the <code>Ingress</code> configuration</li> <li>Now you can access the HTTPS website with a valid certificate</li> </ol> <p>A much more detailed diagram can be found in the official documentation under certificate lifecycle.</p>"},{"location":"reference/concepts/opus-container/","title":"Opus container","text":"<p>The Opus container makes it easy to get all of the dependencies needed to interact with Ubiquity, and deploy it (hence the name, Opus - Or creation).</p>"},{"location":"reference/concepts/opus-container/#how-to-open-it","title":"How to open it","text":"<p>You can use it by deploying the container via Docker::</p> Docker <pre><code>make tools\n</code></pre> <p>It will open a shell like this:</p> <pre><code>[cjcshadowsan@bootstrap:~/ubiquity]$ echo hello\nhello\n</code></pre>"},{"location":"reference/concepts/opus-container/#how-it-works","title":"How it works","text":"<ul> <li>All dependencies are defined in the <code>Dockerfile</code> and installed when the container is built</li> <li>The container is built using the <code>Makefile</code> in the root of the repository</li> <li>The container is run using the <code>Makefile</code> in the root of the repository</li> <li>It mounts the current directory into the container, so you can work on the files in the current directory</li> <li>It sets the current user as the user in the container, so files created in the container are owned by the current user</li> <li>It attaches SSH keys from the current user to the container, so you can use SSH to connect to other machines</li> <li>It provides you will all tools to manage or deploy a Ubiquity cluster including kubectl, helm, kustomize, terraform, ansible, K9s, clustershell, etc.</li> </ul>"},{"location":"reference/concepts/opus-container/#known-issues","title":"Known issues","text":"<ul> <li>If your Docker engine is not running in rootless mode, all files created by the tools container will be owned by <code>root</code></li> </ul>"},{"location":"reference/dns/duckdns/","title":"DuckDNS and Kubernetes","text":"<p>This guide pretends to explain a quick and easy way to get our applications in Kubernetes/AKS running exposed to internet with HTTPs using DuckDNS as domain name provider.</p>"},{"location":"reference/dns/duckdns/#pre-requisites-and-versions","title":"Pre-requisites and versions:","text":"<ul> <li>AKS cluster version: 1.21.7</li> <li>Helm 3</li> <li>Ingress-controller nginx chart version 4.0.16</li> <li>Ingress-controller nginx app version 1.1.1</li> <li>cert-manager version 1.2.0</li> <li>cert-manager DuckDNS webhook version 1.2.2</li> </ul> <p>(1) Add ingress-controller Helm repo helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx (2) Update repository helm repo update (3) Install ingress-controller with Helm With this command we will be installing the latest version from the repository. helm install nginx-ingress ingress-nginx/ingress-nginx --namespace ingress --create-namespace  (4) Verify the pods are running fine in our cluster kubectl get pods -n ingress You should see something like this: NAME                                                      READY   STATUS    RESTARTS   AGE nginx-ingress-ingress-nginx-controller-74fb55cbd5-hjvr9   1/1     Running   0          41m</p> <p>(5) We need to verify our ingress-controller has a public IP assigned kubectl get svc -n ingress We should see something similar to this, the key part here is to have an IP assigned in \"EXTERNAL-IP\", this might take a few seconds to show, it is expected as in the background what is happening is that Azure is spinning a \"Public IP\" resource for you and assigning it to the AKS cluster. NAME                                               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE nginx-ingress-ingress-nginx-controller             LoadBalancer   10.0.33.214    20.190.211.14   80:32321/TCP,443:30646/TCP   38m (6) Deploy a test application Now we will be deploying a testing application that will be running inside a pod with a service that we will use to access the pods. This might feel a bit overkill as we have only a single pod and having a service for a single pod seems a lot but keep in mind that pods can be rescheduled at any given moment and they can even change their IPs while a service doesnt, so reaching our pods using a service is the best (and the desired) option. This also scales better as if we have more pods we will still use the same service to reach them and the service will load balance between them.</p> <p>This is the yaml file for our test application: apiVersion: apps/v1 kind: Deployment metadata:   name: echo-app   namespace: default spec:   selector:     matchLabels:       app: echo-app   replicas: 2   template:     metadata:       labels:         app: echo-app     spec:       containers:       - name: echo-app         image: hashicorp/http-echo         args:         - \"-text=Test 123!\"         ports:         - containerPort: 5678 (7) Deploy our service apiVersion: v1 kind: Service metadata:   name: echo-svc   namespace: default spec:   ports:   - port: 80     targetPort: 5678   selector:     app: echo-app (8) Let's deploy an ingress resource Now we need to deploy an ingress resource, this will tell our ingress controller how to manage the traffic that will be arriving to the public IP of the ingress controller (the one from the step 5), basically we are telling it to forward the traffic from the \"/\" path to the service of our application. apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: ingress-echo   namespace: ingress   annotations:     kubernetes.io/ingress.class: nginx     nginx.ingress.kubernetes.io/ssl-redirect: \"false\"     nginx.ingress.kubernetes.io/use-regex: \"true\"     nginx.ingress.kubernetes.io/rewrite-target: /$1 spec:   rules:   - http:       paths:       - path: /(.*)         pathType: Prefix         backend:           service:             name: echo-svc             port:               number: 80</p> <p>We are telling the ingress controller to forward all the traffic of the port 80 to the service echo-svc on it's port 80.</p> <p>(9) Let's test it all together To test this we will be accessing the ingress using the public IP that we got in step 5:</p> <p>Using a web browser, go to https://IP</p> <p>Using the command line run curl https://IP</p> <p>Adding certificates with cert-manager for duckDNS So far all is good, the only (small :) ) detail is that our ingress has an IP and not a domain/subdomain which is a bit hard for humans to remember and our traffic is all going unencrypted over http, we don't have any security (yet). We will be adding cert-manager to generate TLS certificates for us in our DuckDNS subdomain, cert-manager not only allows us to get certificates, it also rotate them when they are about to expire (and we can configure how ofter we want to expire/rotate them).</p> <p>(10) Let's install cert-manager helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.2.0 --set 'extraArgs={--dns01-recursive-nameservers=8.8.8.8:53\\,1.1.1.1:53}' --create-namespace --set installCRDs=true After a moment it will be done creating the needed resources, we can verify this by checking the status of the pods in the cert-manager namespace: kubectl get pods -n cert-manager</p> <p>Something like the following should appear: NAME                                            READY   STATUS    RESTARTS   AGE cert-manager-6c9b44dd95-59b6n                   1/1     Running   0          47m cert-manager-cainjector-74459fcc56-6dfn8        1/1     Running   0          47m cert-manager-webhook-c45b7ff-hrcnx              1/1     Running   0          47m (11) Do you need a domain for free? DuckDNS to the rescue! With all this in place we are ready to request a TLS certificate for our site/application, but first we need to own a domain or a subdomain to point to our public IP (step 5) so we can reach our pods/service using a name instead of an IP. Another very important point to note is that cert-manager will only provide certificates if we can proove we own the domain/subdomain (this is to avoid the possibility of anyone requesting a certificate for a well known domain like google.com), to do this it has two methods http-01 and dns-01, we will focus this time in dns-01 which basically works like this: cert-manager requests us to provide credentials to access the domain/subdomain (in DuckDNS this is a token), with that cert-manager will generate a random string and make a TXT record in the domain provider with the value of the random string generated, will wait a moment and will use public DNSs to query that TXT record, if cert-manager finds the TXT record with the correct value it means we own that domain/subdomain and then will remove the TXT record and generate a certificate for us for that domain/subdomain. This will end with a secret in our K8s/AKS cluster containing the certificate and the key for that domain/subdomain, that secret is the one we will tell ingress-controller to use to validate the https traffic reaching our ingress.</p> <p>(11) Configuring our DuckDNS account We need to go to https://www.duckdns.org/ and log in with our account/credentials (you have multiple alternatives in the upper right part of the page). Once this is done you will see your token in the screen, that's the token we will need in step 12 of this guide.</p> <p>A bit below that we will see a text field where we need to enter the subdomain name we want (something.duckdns.org) and a place to assign an IP (IPv4), in there we can enter a name for our subdomain and for the IP enter the public IP of our ingess (the one from step 5), then click save/update.</p> <p>Now we are telling DuckDNS to redirect all the traffic that arrives to that subdomain to the IP we entered, wonderful!</p> <p>(12) Deploy a DuckDNS cert-manager webhook handler Now is time to deploy a DuckDNS webhook handler, this is what will add the functionality to cert-manager to manage records in DuckDNS. We can opt to use a helm chart or deploy by cloning the repository where this solution resides, the helm chart didn't work for me so I will be describing the approach using the code in the repo instead.</p> <p>Let's clone the repository first git clone https://github.com/ebrianne/cert-manager-webhook-duckdns.git Now we install it from the cloned repository cd cert-manager-webhook-duckdns</p> <p>helm install cert-manager-webhook-duckdns --namespace cert-manager --set duckdns.token='TOKEN_DE_DUCKDNS' --set clusterIssuer.production.create=true --set clusterIssuer.staging.create=true --set clusterIssuer.email='NUESTRO_MAIL' --set logLevel=2 ./deploy/cert-manager-webhook-duckdns Now we will see we have a new pod in our cert-manager namespace, we can check with the following command kubectl get pods -n cert-manager And you will see something like this NAME                                            READY   STATUS    RESTARTS   AGE cert-manager-webhook-duckdns-5cdbf66f47-kgt99   1/1     Running   0          56m (13) ClusterIssuers y detalles de cert-manager To generate certificates cert-manager has two certificate generators, one is called XXXX-Staging and the other one XXXX-Production. The main difference is that the Production one will provide a certificate that is valid for all web browsers, this is the one we want in our application, but if we are testing and learning we will make mistakes and too many mistakes in the Production one will cause cert-manager to ban us from using the service. To avoid this there is the Staging one which will provide a valid certifcate that our brownsers will take as \"valid buuuuuuut\" so you will see the padlock and the https but you will see in the certificate description that it is a Staging certificate. With this Staging one we can try and make as many mistakes as we need to fully understand how this works, once done, you simply change the ClusterIssuer to the Production one and you will get a new certifica but for Production and since it was working when you did your tests in Staging this one should not fail.</p> <p>When we installed the DuckDNS webhook we told the names to use for those ClusterIssuers, here is what I mean: --set clusterIssuer.production.create=true --set clusterIssuer.staging.create=true (14) Let's create an ingress resource using the Staging ClusterIssuer Create a file called staging-ingress.yaml with the following content: apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: echo-https-ingress   namespace: default   annotations:     kubernetes.io/ingress.class: nginx     cert-manager.io/cluster-issuer: cert-manager-webhook-duckdns-staging     nginx.ingress.kubernetes.io/rewrite-target: /$1     nginx.ingress.kubernetes.io/use-regex: \"true\" spec:   tls:   - hosts:     - superprueba.duckdns.org     secretName: superprueba-tls-secret-staging   rules:   - host: superprueba.duckdns.org     http:       paths:       - path: /(.*)         pathType: Prefix         backend:           service:             name: echo             port:               number: 80</p> <p>In this example the subdomain is called superprueba and I am defining to use the clusterissuer cert-manager-webhook-duckdns-staging and to store the certificate in a secret called superprueba-tls-secret, also all https traffic coming from superprueba.duckdns.org needs to be forwarded to service echo on port 80.</p> <p>The secret name can be anything we want, is not mandatory to contain the name of the subdomain/domain but is a good practice so we can quickly identify what the secret is for.</p> <p>Another important detail is that the ingress resource has to be defined in the same namespace as the service that will be forwarding traffic to, but the ingress CONTROLLER can be (and is normal to configure in this way) in another different namespace.</p> <p>Now we apply it with the following command: kubectil apply -f staging-ingress.yaml (15) Verify the creation process for our certificate Now if we run a kubectl get challenge in the same namespace where we deployed the ingress resource we should see something like this: NAME                                                        STATE     DOMAIN                       AGE superprueba-tls-secret-staging-6lmxj-668717679-4070204345   pending   superprueba.duckdns.org      4s This is the process that cert-manager uses to generate the TXT record in DuckDNS and confirm we own the subdomain/domain (basically we provided a valid token), once this process is done and cert-manager confirms we are the owner of the subdomain/domain this challenge is deleted and a certificate and a key are generated and stored in the secret we specified (superprueba-tls-secret-staging) in our case.</p> <p>If we check the status of our certificate while the challenge is still pending we will see something like this: NAME                             READY   SECRET                           AGE superprueba-tls-secret-staging   False    superprueba-tls-secret-staging   7m15s And once is done and the challenge is deleted we will see something like this: NAME                             READY   SECRET                           AGE superprueba-tls-secret-staging   True    superprueba-tls-secret-staging   7m15s At this point we can verify that we can access our subdomain superprueba-duckdns.org with a browser or using curl.</p> <p>With curl we would see something like this.  curl https://superprueba.duckdns.org/ curl: (60) schannel: SEC_E_UNTRUSTED_ROOT (0x80090325) - The certificate chain was issued by an authority that is not trusted. This is correct, we have a certificate but is not a production ready one, is just one to test the configuration for cert-manager is correct, now we can go and change the clusterissuer from staging to production to obtain a real and valid certificate.</p> <p>(16) Adjusting our ingress resource to request a production certificate Now let's crete a new file called production-ingress.yaml with the following content: apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: echo-https-ingress   namespace: default   annotations:     kubernetes.io/ingress.class: nginx     cert-manager.io/cluster-issuer: cert-manager-webhook-duckdns-production     nginx.ingress.kubernetes.io/rewrite-target: /$1     nginx.ingress.kubernetes.io/use-regex: \"true\" spec:   tls:   - hosts:     - superprueba.duckdns.org     secretName: superprueba-tls-secret-production   rules:   - host: superprueba.duckdns.org     http:       paths:       - path: /(.*)         pathType: Prefix         backend:           service:             name: echo             port:               number: 80 And then let's apply it with: kubectl apply -f production-ingress.yaml Once this is done, we can do the same verification steps as before to confirm that the production certificate is issued and stored in our secret and confirm by navigating to our site again curl https://superprueba.duckdns.org/ Test 123! (17) Troubleshooting Ok, this was the article on how to configure and make it work this solution without problems, some times we have a typo, or misconfigure an IP or a wrong name somewhere and is a pain in the neck to know what is wrong if you are just following this tutorial as a first approach to kubernetes. So here are a few things to check in case something is not working as expected.</p> <p>a) Check the cert-manager webhook logs, here you will find all the actions that the webhook performs to the duckdns service, if there is a problem with the token you are using, a failure to reach duckdns, etc. here is where you will find that.</p> <p>b) Check the logs of cert-manager (the core element), those are the pods called cert-manager-XXXX and in here you will find information on what is cert-manager doing, if is requesting a certificate, creating a secret, running a challenge, etc.</p> <p>c) Verify the logs for the ingress-controller pods, here we can see the requests reaching our cluster, if the requests can't reach our ingress they will not be able to be routed to any service, here we should see the request being ingested.</p> <p>d) Check the configuration in DuckDNS is pointing to the correct IP as we configured it, this can be done with https://digwebinterface.com/ which is a simple page that you input a domain name and it will return you the IP where it is pointing.</p>"},{"location":"reference/metal3/api/","title":"API and Resource Definitions","text":""},{"location":"reference/metal3/api/#baremetalhost","title":"BareMetalHost","text":"<p>Metal\u00b3 introduces the concept of BareMetalHost resource, which defines a physical host and its properties. The BareMetalHost embeds two well differentiated sections, the bare metal host specification and its current status.</p>"},{"location":"reference/metal3/api/#baremetalhost-spec","title":"BareMetalHost spec","text":"<p>The BareMetalHost's spec defines the desire state of the host. It contains mainly, but not only, provisioning details.</p>"},{"location":"reference/metal3/api/#bmc","title":"bmc","text":"<p>The <code>bmc</code> fields contain the connection information for the BMC (Baseboard Management Controller) on the host.</p> <p>The sub-fields are</p> <ul> <li>address -- The URL for communicating with the BMC controller, based   on the provider being used. See below for more details.</li> <li>credentialsName -- A reference to a secret containing the   username and password for the BMC.</li> <li>disableCertificateVerification -- A boolean to skip certificate     validation when true.</li> </ul> <p>BMC URLs vary based on the type of BMC and the protocol used to communicate with them.</p> Technology Protocol Boot method Format Notes Generic IPMI IPMI iPXE <code>ipmi://&lt;host&gt;:&lt;port&gt;</code> or just <code>&lt;host&gt;:&lt;port&gt;</code> Port is optional, defaults to 623 Generic Redfish Redfish iPXE <code>redfish://&lt;host&gt;:&lt;port&gt;/&lt;systemID&gt;</code> System ID is a path like <code>/redfish/v1/Systems/System.Embedded.1</code> Virtual media <code>redfish-virtualmedia://&lt;host&gt;:&lt;port&gt;/&lt;systemID&gt;</code> Virtual media support is vendor-dependent. Should not be used for Dell. Dell iDRAC WSMAN iPXE <code>idrac://&lt;host&gt;:&lt;port&gt;</code> Redfish iPXE <code>idrac-redfish://&lt;host&gt;:&lt;port&gt;/&lt;systemID&gt;</code> See above about system ID. Redfish Virtual media <code>idrac-virtualmedia://&lt;host&gt;:&lt;port&gt;/&lt;systemID&gt;</code> See above about system ID. Fujitsu iRMC iRMC iPXE <code>irmc://&lt;host&gt;:&lt;port&gt;</code> Port is optional, the default is 443. HUAWEI ibmc ibmc iPXE <code>ibmc://&lt;host&gt;:&lt;port&gt;</code> HPE iLO 4 iLO iPXE <code>ilo4://&lt;host&gt;:&lt;port&gt;</code> Port is optional, the default is 443. iLO Virtual media <code>ilo4-virtualmedia://&lt;host&gt;:&lt;port&gt;</code> HPE iLO 5 iLO iPXE <code>ilo5://&lt;host&gt;:&lt;port&gt;</code> Redfish iPXE <code>ilo5-redfish://&lt;host&gt;:&lt;port&gt;/&lt;systemID&gt;</code> Redfish Virtual media <code>ilo5-virtualmedia://&lt;host&gt;:&lt;port&gt;/&lt;systemID&gt;</code> <p>All protocols based on HTTPS (i.e. not IPMI) with an exception of iRMC allow optionally specifying the carrier protocol in the form of <code>+http</code> or <code>+https</code>, for example: <code>redfish+http://...</code> or <code>idrac-virtualmedia+https</code>. iLO (both versions) only support HTTPS. When not specified, HTTPS is used by default.</p>"},{"location":"reference/metal3/api/#online","title":"online","text":"<p>A boolean indicating whether the host should be powered on (true) or off (false). Changing this value will trigger a change in power state on the physical host.</p>"},{"location":"reference/metal3/api/#bootmacaddress","title":"bootMACAddress","text":"<p>The MAC address of the NIC used for provisioning the host.</p>"},{"location":"reference/metal3/api/#bootmode","title":"bootMode","text":"<p>The boot mode of the host, defaults to <code>UEFI</code>, can also be set to <code>legacy</code> for BIOS boot, or <code>UEFISecureBoot</code>.</p>"},{"location":"reference/metal3/api/#consumerref","title":"consumerRef","text":"<p>A reference to another resource that is using the host, it could be empty if the host is not being currently used.  For example, a Machine resource when the host is being used by the machine-api.</p>"},{"location":"reference/metal3/api/#externallyprovisioned","title":"externallyProvisioned","text":"<p>A boolean indicating whether the host provisioning and deprovisioning are managed externally. When set:</p> <ul> <li>Power status can still be managed using the <code>online</code> field.</li> <li>Hardware inventory will be monitored, but no provisioning or deprovisioning   operations are performed on the host.</li> </ul>"},{"location":"reference/metal3/api/#image","title":"image","text":"<p>Holds details for the image to be deployed on a given host.</p> <p>The sub-fields are</p> <ul> <li>url -- The URL of an image to deploy to the host.</li> <li>checksum -- The actual checksum or a URL to a file containing   the checksum for the image at image.url.</li> <li>checksumType -- Checksum algorithms can be specified. Currently   only <code>md5</code>, <code>sha256</code>, <code>sha512</code> are recognized. If nothing is specified   <code>md5</code> is assumed.</li> <li>format -- This is the disk format of the image. It can be one of <code>raw</code>,   <code>qcow2</code>, <code>vdi</code>, <code>vmdk</code>, <code>live-iso</code> or be left unset.   Setting it to raw enables raw image streaming in Ironic agent for that image.   Setting it to live-iso enables iso images to live boot without deploying   to disk, in this case the checksum fields are ignored.</li> </ul> <p>Even though the image sub-fields are required by Ironic, when the host provisioning is managed externally via <code>externallyProvisioned: true</code>, and power control isn't needed, the fields can be left empty.</p>"},{"location":"reference/metal3/api/#userdata","title":"userData","text":"<p>A reference to the Secret containing the cloudinit user data and its namespace, so it can be attached to the host before it boots for configuring different aspects of the OS (like networking, storage, ...).</p>"},{"location":"reference/metal3/api/#networkdata","title":"networkData","text":"<p>A reference to the Secret containing the network configuration data (e.g. network_data.json) and its namespace, so it can be attached to the host before it boots to set network up</p>"},{"location":"reference/metal3/api/#description","title":"description","text":"<p>A human-provided string to help identify the host.</p>"},{"location":"reference/metal3/api/#hardwareprofile","title":"hardwareProfile","text":"<p>This field is deprecated. See rootDeviceHints instead.</p> <p>The name of the hardware profile to use. The following are the current supported <code>hardwareProfile</code> settings and their corresponding root devices.</p> hardwareProfile Root Device <code>unknown</code> /dev/sda <code>libvirt</code> /dev/vda <code>dell</code> HCTL: 0:0:0:0 <code>dell-raid</code> HCTL: 0:2:0:0 <code>openstack</code> /dev/vdb <p>NOTE: These are subject to change.</p>"},{"location":"reference/metal3/api/#raid","title":"raid","text":"<p>This field contains the information about the RAID configuration for bare metal servers.</p> <p>The sub-fields are:</p> <ul> <li>hardwareRAIDVolumes -- It contains the list of logical disks for hardware    RAID. If rootDeviceHints isn't used, the first volume is the root volume.    Furthermore, it defines the desired configuration of volume in hardware RAID.</li> <li>level -- RAID level for the logical disk. The following levels are      supported: <code>0</code>,<code>1</code>,<code>2</code>,<code>5</code>,<code>6</code>,<code>1+0</code>,<code>5+0</code>,<code>6+0</code>.</li> <li>name -- Name of the volume. Should be unique within the server. If not      specified, volume name will be auto-generated.</li> <li>numberOfPhysicalDisks -- Integer, number of physical disks to use for the      logical disk. Defaults to minimum number of disks required for the      particular RAID level.</li> <li>physicalDisks -- List of names of physical disks (Strings). This is an      optional field. If specified, the <code>controller</code> field must be specified too.</li> <li>controller -- String, Name of the RAID controller to be used in the       hardware RAID volume. Optional field.</li> <li>rotational -- If true, select only rotational disks, if false - only      solid-state and NVMe. Any disk types are used by default.</li> <li>sizeGibibytes -- Size (Integer) of the logical disk to be created in GiB.      If unspecified or set to 0, the maximum capacity of disk will be used for      logical disk.</li> <li>softwareRAIDVolumes -- It contains the list of logical disks for software    RAID. If rootDeviceHints isn't used, the first volume is the root volume. If    HardwareRAIDVolumes is set this item will be invalid. The number of created    Software RAID devices must be 1 or 2. If there is only one Software RAID    device, it has to be a RAID-1. If there are two, the first one has to be a    RAID-1, while the RAID level for the second one can be 0, 1, or 1+0. As the    first RAID device will be the deployment device, enforcing a RAID-1 reduces    the risk of ending up with a non-booting node in case of a disk failure.    Furthermore, SoftwareRAIDVolume defines the desired configuration of volume    in software RAID.</li> <li>level -- RAID level for the logical disk. The following levels are      supported: <code>0</code>,<code>1</code>,<code>1+0</code>.</li> <li>physicalDisks -- A list of device hints, the number of items should be      greater than or equal to 2.</li> <li>sizeGibibytes -- Size (Integer) of the logical disk to be created in      GiB. If unspecified or set to 0, the maximum capacity of disk will be      used for logical disk.</li> </ul> <p>If you do not set the RAID field, we will keep the current RAID configuration.</p> <p>You can set the <code>hardwareRAIDVolume</code> as an empty slice to clear the hardware RAID configuration, for example:</p> <pre><code>spec:\n   raid:\n     hardwareRAIDVolume: []\n</code></pre> <p>NOTE: Currently the 'raid' field is only supported by ilo5/idrac/irmc.</p> <p>NOTE: Do not try to simultaneously delete hardware RAID volumes and add software RAID volumes. Delete hardware RAID first, wait for the host to settle down, add software volumes afterwards.</p> <p>NOTE: If you got following error message:</p> <ol> <li>raid settings are defined, but the node's driver %s does not support RAID.</li> <li>node's driver %s does not support hardware RAID.</li> </ol> <p>You can solve it by:</p> <ol> <li>Keep raid field is nil.</li> <li>Keep hardwareRAIDVolumes field is nil.</li> </ol> <p>If the error message you get isn't included the above, you may need to check whether the BM has a RAID controller and keep the raid field blank.</p>"},{"location":"reference/metal3/api/#firmware","title":"firmware","text":"<p>This field contains the information about the BIOS configuration for bare metal servers.</p> <p>The sub-fields are:</p> <ul> <li>simultaneousMultithreadingEnabled -- Allows a single physical processor   core to appear as several logical processors. This supports following   options: true, false.</li> <li>sriovEnabled -- SR-IOV support enables a hypervisor to create virtual   instances of a PCI-express device, potentially increasing performance.   This supports following options: true, false.</li> <li>virtualizationEnabled -- Supports the virtualization of platform   hardware. This supports following options: true, false.</li> </ul> <p>NOTE: Currently the <code>firmware</code> field is only supported by ilo4/ilo5/irmc /idrac.</p>"},{"location":"reference/metal3/api/#rootdevicehints","title":"rootDeviceHints","text":"<p>Guidance for how to choose the device to receive the image being provisioned. The storage devices are examined in the order they are discovered during inspection and the hint values are compared to the inspected values. The first discovered device that matches is used. Hints can be combined, and if multiple hints are provided then a device must match all hints in order to be selected.</p> <p>The sub-fields are</p> <ul> <li>deviceName -- A string containing a Linux device name like   <code>/dev/vda</code>. The hint must match the actual value exactly.</li> <li>hctl -- A string containing a SCSI bus address like   <code>0:0:0:0</code>. The hint must match the actual value exactly.</li> <li>model -- A string containing a vendor-specific device   identifier. The hint can be a substring of the actual value.</li> <li>vendor -- A string containing the name of the vendor or   manufacturer of the device. The hint can be a substring of the   actual value.</li> <li>serialNumber -- A string contianing the device serial   number. The hint must match the actual value exactly.</li> <li>minSizeGigabytes -- An integer representing the minimum size of the   device in Gigabytes.</li> <li>wwn -- A string containing the unique storage identifier. The   hint must match the actual value exactly.</li> <li>wwnWithExtension -- A string containing the unique storage   identifier with the vendor extension appended. The hint must match   the actual value exactly.</li> <li>wwnVendorExtension -- A string containing the unique vendor   storage indentifier. The hint must match the actual value exactly.</li> <li>rotational -- A boolean indicating whether the device should be   a rotating disk (<code>true</code>) or not (<code>false</code>).</li> </ul>"},{"location":"reference/metal3/api/#automatedcleaningmode","title":"automatedCleaningMode","text":"<p>An interface to enable/disable automated cleaning during provisioning and deprovisioning. When set to <code>disabled</code>, automated cleaning will be skipped, where <code>metadata</code>(default value) enables it.</p>"},{"location":"reference/metal3/api/#customdeploy","title":"customDeploy","text":"<p>An advanced alternative to using image. Set the subfield <code>method</code> to the name of a custom deploy step that is provided by your deployment ramdisk. Most users will want to use image instead.</p> <p>NOTE: setting either <code>customDeploy.method</code> or <code>image.url</code> triggers provisioning of the host.</p>"},{"location":"reference/metal3/api/#baremetalhost-status","title":"BareMetalHost status","text":"<p>Moving onto the next block, the BareMetalHost's status which represents the host's current state. Including tested credentials, current hardware details, etc.</p>"},{"location":"reference/metal3/api/#goodcredentials","title":"goodCredentials","text":"<p>A reference to the secret and its namespace holding the last set of BMC credentials the system was able to validate as working.</p>"},{"location":"reference/metal3/api/#triedcredentials","title":"triedCredentials","text":"<p>A reference to the secret and its namespace holding the last set of BMC credentials that were sent to the provisioning backend.</p>"},{"location":"reference/metal3/api/#lastupdated","title":"lastUpdated","text":"<p>The timestamp of the last time the status of the host was updated.</p>"},{"location":"reference/metal3/api/#operationalstatus","title":"operationalStatus","text":"<p>The status of the server. Value is one of the following:</p> <ul> <li>OK -- Indicates all the details for the host are known and working,   meaning the host is correctly configured and manageable.</li> <li>discovered -- Implies some of the host's details are either   not working correctly or missing. For example, the BMC address is known   but the login credentials are not.</li> <li>error -- Indicates the system found some sort of irrecuperable error.   Refer to the errorMessage field in the status section for more details.</li> </ul>"},{"location":"reference/metal3/api/#errormessage","title":"errorMessage","text":"<p>Details of the last error reported by the provisioning backend, if any.</p>"},{"location":"reference/metal3/api/#hardware","title":"hardware","text":"<p>The details for hardware capabilities discovered on the host. These are filled in by the provisioning agent when the host is registered.</p> <p>The sub-fields are</p> <ul> <li>nics -- List of network interfaces for the host.</li> <li>name -- A string identifying the network device,      e.g. nic-1.</li> <li>mac -- The MAC address of the NIC.</li> <li>ip -- The IP address of the NIC, if one was assigned      when the discovery agent ran.</li> <li>speedGbps -- The speed of the device in Gbps.</li> <li>vlans -- A list holding all the VLANs available for this NIC.</li> <li>vlanId -- The untagged VLAN ID.</li> <li>pxe -- Whether the NIC is able to boot using PXE.</li> <li>storage -- List of storage (disk, SSD, etc.) available to the host.</li> <li>name -- A string identifying the storage device,      e.g. disk 1 (boot).</li> <li>rotational -- Either true or false, indicates whether the disk      is rotational.</li> <li>sizeBytes -- Size of the storage device.</li> <li>serialNumber -- The device's serial number.</li> <li>cpu -- Details of the CPU(s) in the system.</li> <li>arch -- The architecture of the CPU.</li> <li>model -- The model string.</li> <li>clockMegahertz -- The speed in MHz of the CPU.</li> <li>flags -- List of CPU flags, e.g. 'mmx','sse','sse2','vmx', ...</li> <li>count -- Amount of these CPUs available in the system.</li> <li>firmware -- Contains BIOS information like for instance its vendor   and version.</li> <li>systemVendor -- Contains information about the host's manufacturer,   the productName and serialNumber.</li> <li>ramMebibytes -- The host's amount of memory in Mebibytes.</li> </ul>"},{"location":"reference/metal3/api/#hardwareprofile-status","title":"hardwareProfile (status)","text":"<p>This field is deprecated. See rootDeviceHints instead.</p> <p>The name of the hardware profile that matches the hardware discovered on the host based on the details saved to the Hardware section. If the hardware does not match any known profile, the value <code>unknown</code> will be set on this field and is used by default. In practice, this only affects which device the OS image will be written to. The following are the current supported <code>hardwareProfile</code> settings and their corresponding root devices.</p> hardwareProfile Root Device <code>unknown</code> /dev/sda <code>libvirt</code> /dev/vda <code>dell</code> HCTL: 0:0:0:0 <code>dell-raid</code> HCTL: 0:2:0:0 <code>openstack</code> /dev/vdb <p>NOTE: These are subject to change.</p>"},{"location":"reference/metal3/api/#poweredon","title":"poweredOn","text":"<p>Boolean indicating whether the host is powered on.</p> <p>See online on the BareMetalHost's Spec.</p>"},{"location":"reference/metal3/api/#provisioning","title":"provisioning","text":"<p>Settings related to deploying an image to the host.</p> <ul> <li>state -- The current state of any ongoing provisioning operation.    The following are the currently supported ones:</li> <li>\\&lt;empty string&gt; -- There is no provisioning happening, at the moment.</li> <li>unmanaged -- There is an insufficient information available to register      the host.</li> <li>registering -- The host's BMC details are being checked.</li> <li>match profile -- The discovered hardware details on the host      are being compared against known profiles.</li> <li>available -- The host is available to be consumed. (This state was      previously known as ready.)</li> <li>preparing -- The existing configuration will be removed, and the new      configuration will be set on the host.</li> <li>provisioning -- An image is being written to the host's disk(s).</li> <li>provisioned -- An image has been completely written to the host's      disk(s).</li> <li>externally provisioned -- Metal\u00b3 does not manage the image on the host.</li> <li>deprovisioning -- The image is being wiped from the host's disk(s).</li> <li>inspecting -- The hardware details for the host are being collected      by an agent.</li> <li>deleting -- The host is being deleted from the cluster.</li> <li>id -- The unique identifier for the service in the underlying   provisioning tool.</li> <li>image -- The image most recently provisioned to the host.</li> <li>raid -- The list of hardware or software RAID volumes recently set.</li> <li>firmware -- The BIOS configuration for bare metal server.</li> <li>rootDeviceHints -- The root device selection instructions used   for the most recent provisioning operation.</li> </ul>"},{"location":"reference/metal3/api/#baremetalhost-example","title":"BareMetalHost Example","text":"<p>The following is a complete example from a running cluster of a BareMetalHost resource (in YAML), it includes its specification and status sections:</p> <pre><code>apiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  creationTimestamp: \"2019-09-20T06:33:35Z\"\n  finalizers:\n  - baremetalhost.metal3.io\n  generation: 2\n  name: bmo-controlplane-0\n  namespace: bmo-project\n  resourceVersion: \"22642\"\n  selfLink: /apis/metal3.io/v1alpha1/namespaces/bmo-project/baremetalhosts/bmo-controlplane-0\n  uid: 92b2f77a-db70-11e9-9db1-525400764849\nspec:\n  bmc:\n    address: ipmi://10.10.57.19\n    credentialsName: bmo-controlplane-0-bmc-secret\n  bootMACAddress: 98:03:9b:61:80:48\n  consumerRef:\n    apiVersion: machine.openshift.io/v1beta1\n    kind: Machine\n    name: bmo-controlplane-0\n    namespace: bmo-project\n  externallyProvisioned: true\n  hardwareProfile: default\n  image:\n    checksum: http://172.16.1.100/images/myOSv1/myOS.qcow2.md5sum\n    url: http://172.16.1.100/images/myOSv1/myOS.qcow2\n  online: true\n  raid:\n    hardwareRAIDVolumes:\n    - level: \"1\"\n      sizeGibibytes: 200\n      rotational: true\n  firmware:\n    virtualizationEnabled: true\n  userData:\n    name: bmo-controlplane-user-data\n    namespace: bmo-project\n  networkData:\n    name: bmo-controlplane-network-data\n    namespace: bmo-project\n  metaData:\n    name: bmo-controlplane-meta-data\n    namespace: bmo-project\nstatus:\n  errorMessage: \"\"\n  goodCredentials:\n    credentials:\n      name: bmo-controlplane-0-bmc-secret\n      namespace: bmo-project\n    credentialsVersion: \"5562\"\n  hardware:\n    cpu:\n      arch: x86_64\n      clockMegahertz: 2000\n      count: 40\n      flags: []\n      model: Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz\n    firmware:\n      bios:\n        date: 12/17/2018\n        vendor: Dell Inc.\n        version: 1.6.13\n    hostname: bmo-controlplane-0.localdomain\n    nics:\n    - ip: 172.22.135.105\n      mac: \"00:00:00:00:00:00\"\n      model: unknown\n      name: eno1\n      pxe: true\n      speedGbps: 25\n      vlanId: 0\n    ramMebibytes: 0\n    storage: []\n    systemVendor:\n      manufacturer: Dell Inc.\n      productName: PowerEdge r460\n      serialNumber: \"\"\n  hardwareProfile: \"\"\n  lastUpdated: \"2019-09-20T07:03:23Z\"\n  operationalStatus: OK\n  poweredOn: true\n  provisioning:\n    ID: a4438010-3fc6-4c5c-b570-900bbe85da57\n    image:\n      checksum: \"\"\n      url: \"\"\n    state: externally provisioned\n  triedCredentials:\n    credentials:\n      name: bmo-controlplane-0-bmc-secret\n      namespace: bmo-project\n    credentialsVersion: \"5562\"\n</code></pre> <p>And here is the secret <code>bmo-controlplane-0-bmc-secret</code> holding the host's BMC credentials, base64 encoded:</p> <pre><code>$echo -n 'admin' | base64\n\nYWRtaW4=\n\n$echo -n 'password' | base64\n\ncGFzc3dvcmQ=\n</code></pre> <p>Copy the above base64 encoded username and password pair and paste it into the yaml as mentioned below.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: bmo-controlplane-0-bmc-secret\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: cGFzc3dvcmQ=\n</code></pre> <p>NOTE: After decoding the secret content, whitespace is stripped from the beginning and end before the username and password values are used.</p>"},{"location":"reference/metal3/api/#triggering-provisioning","title":"Triggering Provisioning","text":"<p>Several conditions must be met in order to initiate provisioning.</p> <ol> <li>The host <code>spec.image.url</code> field must contain a URL for a valid    image file that is visible from within the cluster and from the    host receiving the image.</li> <li>The host must have <code>online</code> set to <code>true</code> so that the operator will    keep the host powered on.</li> <li>The host must have all of the BMC details.</li> </ol> <p>To initiate deprovisioning, clear the image URL from the host spec.</p>"},{"location":"reference/metal3/api/#unmanaged-hosts","title":"Unmanaged Hosts","text":"<p>Hosts created without BMC details will be left in the <code>unmanaged</code> state until the details are provided. Unmanaged hosts cannot be provisioned and their power state is undefined.</p>"},{"location":"reference/metal3/api/#pausing-reconciliation","title":"Pausing reconciliation","text":"<p>It is possible to pause the reconciliation of a BareMetalHost object by adding an annotation <code>baremetalhost.metal3.io/paused</code>. Metal\u00b3  provider sets the value of this annotation as <code>metal3.io/capm3</code> when the cluster to which the BareMetalHost belongs, is paused and removes it when the cluster is not paused. If you want to pause the reconciliation of BareMetalHost you can put any value on this annotation other than <code>metal3.io/capm3</code>. Please make sure that you remove the annotation  only if the value of the annotation is not <code>metal3.io/capm3</code>, but another value that you have provided. Removing the annotation will enable the reconciliation again.</p>"},{"location":"reference/metal3/api/#detaching-hosts","title":"Detaching hosts","text":"<p>It is possible to prevent management of a BareMetalHost object by adding an annotation <code>baremetalhost.metal3.io/detached</code>. This removes the host from the provisioner, which prevents any management of the physical host (e.g changing power state, or deprovisioning), but still allows the BMH status to be updated unlike the <code>paused</code> annotation. While in this state the OperationalStatus field will be <code>detached</code> but the provisioning state will be unmodified.  This API only has any effect for BareMetalHost resources that are in either <code>Provisioned</code>, <code>ExternallyProvisioned</code> or <code>Ready</code>/<code>Available</code> state.</p> <p>Please note only the existence of the annotation is important to treat the BMH as detached and the value of the annotation is always ignored.</p>"},{"location":"reference/metal3/api/#hostfirmwaresettings","title":"HostFirmwareSettings","text":"<p>A HostFirmwareSettings resource is used to manage BIOS settings for a host, there is a one-to-one mapping with BareMetalHosts.  A HostFirmwareSettings resource is created when BIOS settings are read from Ironic as the host moves to the Ready state.  These settings are the complete actual BIOS configuration names returned from the BMC, typically 100-200 settings per host, as compared to the three vendor-independent fields stored in the BareMetalHosts <code>firmware</code> field.</p>"},{"location":"reference/metal3/api/#hostfirmwaresettings-spec","title":"HostFirmwareSettings spec","text":"<p>The HostFirmwareSettings's spec defines the desired BIOS settings. These settings will be sent to Ironic as part of clean-steps for update to the BMC when the host goes through cleaning, i.e. when the BareMetalHosts go through the Preparing state.</p>"},{"location":"reference/metal3/api/#spec-settings","title":"spec settings","text":"<p>The <code>settings</code> are an array of name/value pairs for the BIOS settings. The names must match the names read from Ironic and stored in the <code>status</code> field. The values must be within the limits as defined in the FirmwareSchema. Only settings which are not defined as <code>ReadOnly</code> or <code>Unique</code> (such as SerialNumbers) will be accepted.</p>"},{"location":"reference/metal3/api/#hostfirmwaresettings-status","title":"HostFirmwareSettings status","text":"<p>The HostFirmwareSettings's status defines the actual BIOS settings read from Ironic.</p>"},{"location":"reference/metal3/api/#status-schema","title":"status schema","text":"<p><code>Schema</code> is a reference to a FirmwareSchema resource that describes each BIOS setting by type and configurable limits for the type.</p>"},{"location":"reference/metal3/api/#status-settings","title":"status settings","text":"<p>The <code>settings</code> are an array of name/value pairs listing the complete set of BIOS settings retrieved from Ironic.</p>"},{"location":"reference/metal3/api/#status-conditions","title":"status conditions","text":"<p><code>conditions</code> reflects the status of the fields in the spec. Possible conditions are:</p> <ul> <li>Valid -- When set to True indicates that all settings in the spec are   correct for names and values. This is also set to True when nothing is set   in the spec. When set to False indicates that one or more names or values   in the spec are incorrect, the actual error will be shown in the Events   field.</li> <li>ChangeDetected -- Indicates whether or not settings in the spec are   different than settings in the status. When set to True the settings that   are different will be included in the clean-steps that are written to Ironic   as part of cleaning.</li> </ul>"},{"location":"reference/metal3/api/#hostfirmwaresettings-example","title":"HostFirmwareSettings Example","text":"<p>The following is a complete example from a running cluster of a HostFirmwareSettings resource (in YAML), it includes its spec and status sections:</p> <pre><code>apiVersion: metal3.io/v1alpha1\nkind: HostFirmwareSettings\nmetadata:\n  creationTimestamp: \"2021-11-03T21:21:02Z\"\n  generation: 1\n  name: ostest-worker-0\n  namespace: openshift-machine-api\n  ownerReferences:\n  - apiVersion: metal3.io/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: BareMetalHost\n    name: ostest-worker-0\n    uid: 87b08d19-b94c-4a29-901a-67890d9eb843\n  resourceVersion: \"24688\"\n  uid: 293a351b-1743-437d-abed-da53d41d8804\nspec:\n  settings: {}\nstatus:\n  conditions:\n  - lastTransitionTime: \"2021-11-03T21:21:54Z\"\n    message: \"\"\n    observedGeneration: 1\n    reason: Success\n    status: \"False\"\n    type: ChangeDetected\n  - lastTransitionTime: \"2021-11-03T21:21:54Z\"\n    message: \"\"\n    observedGeneration: 1\n    reason: Success\n    status: \"True\"\n    type: Valid\n  schema:\n    name: schema-dc98d7c8\n    namespace: openshift-machine-api\n  settings:\n    BootMode: Uefi\n    EmbeddedSata: Raid\n    L2Cache: 10x256 KB\n    NicBoot1: NetworkBoot\n    NumCores: \"10\"\n    ProcTurboMode: Enabled\n    QuietBoot: \"true\"\n    SecureBootStatus: Enabled\n    SerialNumber: QPX12345\n</code></pre>"},{"location":"reference/metal3/api/#firmwareschema","title":"FirmwareSchema","text":"<p>A FirmwareSchema resource contains the limits each setting, specific to each host.  This data comes directly from the BMC via Ironic. It can be used to prevent misconfiguration of the HostFirmwareSettings spec field so that invalid values are not sent to the host. The FirmwareSchema has a unique identifier derived from its settings and limits. Multiple hosts may therefore have the same FirmwareSchema identifier so its likely that more than one HostFirmwareSettings reference the same FirmwareSchema when hardware of the same vendor and model are used.</p>"},{"location":"reference/metal3/api/#firmwareschema-spec","title":"FirmwareSchema spec","text":""},{"location":"reference/metal3/api/#spec-schema","title":"spec schema","text":"<p>A map of settings names and limits. The values returned in the limits depend on the type of the setting. The following fields are included:</p> <ul> <li>attribute_type -- The type of setting - <code>Enumeration</code>, <code>Integer</code>, <code>String</code>,   <code>Boolean</code>, or <code>Password</code></li> <li>allowable_values -- A list of allowable values when the <code>attribute_type</code> is <code>Enumeration</code></li> <li>lower_bound -- The lowest allowed value when attribute_type is <code>Integer</code></li> <li>upper_bound -- The highest allowed value when attribute_type is <code>Integer</code></li> <li>min_length -- The shortest string length that the value can have when   attribute_type is <code>String</code></li> <li>max_length -- The longest string length that the value can have when   attribute_type is <code>String</code></li> <li>read_only -- The setting is ready only and cannot be modified</li> <li>unique -- The setting is specific to this host</li> </ul>"},{"location":"reference/metal3/api/#hardwarevendor","title":"hardwareVendor","text":"<p>The name of the vendor that this schema corresponds to.</p>"},{"location":"reference/metal3/api/#hardwaremodel","title":"hardwareModel","text":"<p>The hardware model that this schema corresponds to.</p>"},{"location":"reference/metal3/api/#firmwareschema-example","title":"FirmwareSchema Example","text":"<pre><code>apiVersion: metal3.io/v1alpha1\nkind: FirmwareSchema\nmetadata:\n  creationTimestamp: \"2021-11-03T21:21:54Z\"\n  generation: 1\n  name: schema-dc98d7c8\n  namespace: openshift-machine-api\n  ownerReferences:\n  - apiVersion: metal3.io/v1alpha1\n    kind: HostFirmwareSettings\n    name: ostest-controlplane-1\n    uid: a991875d-9897-49f1-9d86-16ea9ed6c84f\n  - apiVersion: metal3.io/v1alpha1\n    kind: HostFirmwareSettings\n    name: ostest-worker-0\n    uid: 650c291c-3da7-4902-be6e-61979daea254\n  - apiVersion: metal3.io/v1alpha1\n    kind: HostFirmwareSettings\n    name: ostest-controlplane-0\n    uid: afc8f76c-0200-431c-ace0-9a6195d16fcd\n  - apiVersion: metal3.io/v1alpha1\n    kind: HostFirmwareSettings\n    name: ostest-controlplane-2\n    uid: f0c49fec-d493-40ac-9a86-56504cd47a74\n  - apiVersion: metal3.io/v1alpha1\n    kind: HostFirmwareSettings\n    name: ostest-worker-1\n    uid: 5fe2c773-5499-4a37-a26d-6f17dc02382f\n  resourceVersion: \"19141\"\n  uid: b442e01f-3724-4f14-a578-f0b99d296c95\n  spec:\n    hardwareModel: KVM (8.2.0)\n    hardwareVendor: Red Hat\n    schema:\n      BootMode:\n        allowable_values:\n        - Bios\n        - Uefi\n        attribute_type: Enumeration\n        read_only: false\n      EmbeddedSata:\n        allowable_values:\n        - Ata\n        - Ahci\n        - Raid\n        - \"Off\"\n        attribute_type: Enumeration\n        read_only: false\n     L2Cache:\n        attribute_type: String\n        max_length: 16\n        min_length: 0\n        read_only: false\n        unique: false\n      NicBoot1:\n        allowable_values:\n        - NetworkBoot\n        - Disabled\n        attribute_type: Enumeration\n        read_only: false\n      NumCores:\n        attribute_type: Integer\n        lower_bound: 10\n        read_only: true\n        unique: false\n        upper_bound: 20\n      ProcTurboMode:\n        allowable_values:\n        - Enabled\n        - Disabled\n        attribute_type: Enumeration\n        read_only: false\n      QuietBoot:\n        attribute_type: Boolean\n        read_only: false\n        unique: false\n      SecureBootStatus:\n        allowable_values:\n        - Enabled\n        - Disabled\n        attribute_type: Enumeration\n        read_only: true\n      SerialNumber:\n        attribute_type: String\n        max_length: 16\n        min_length: 0\n        read_only: false\n        unique: true\n</code></pre>"},{"location":"reference/metal3/api/#hardwaredata","title":"HardwareData","text":"<p>A HardwareData resource contains hardware specifications data of a specific host and it is tightly coupled to its owner resource BareMetalHost. The data in the HardwareData comes from Ironic after a successful inspection phase. As such, operator will create HardwareData resource for a specific BareMetalHost during transitioning phase from inspecting into available state of the BareMetalHost. HardwareData gets deleted automatically by the operator whenever its BareMetalHost is deleted. Deprovisioning of the BareMetalHost should not trigger the deletion of HardwareData, but during next provisioning it can be re-created (with the same name and namespace) with the latest inspection data retrieved from Ironic. HardwareData holds the same name and namespace as its corresponding BareMetalHost resource. Currently, HardwareData doesn't have Status subresource but only the Spec, which we cover next.</p>"},{"location":"reference/metal3/api/#hardwaredata-spec","title":"HardwareData spec","text":"<p>As you probably already noticed, the Spec of HardwareData is the same as .Status.hardware of the BareMetalHost. However, this behaviour is temporary and eventually we will drop .Status.hardware from BareMetalHost and only rely on HardwareData Spec. The reason for having duplication of inspection data at the  moment is to avoid breaking the existing deployments.</p>"},{"location":"reference/metal3/api/#preprovisioningimage","title":"PreprovisioningImage","text":"<p>A PreprovisioningImage resource is automatically created by baremetal-operator for each BareMetalHost to ensure creation of a preprovisioning image for it. In this context, a preprovisioning image is an ISO or initramfs file that contains the Ironic agent. The relevant parts of BareMetalHost are copied to the PreprovisioningImage Spec, the resulting image is expected to appear in the Status.</p> <p>The baremetal-operator project contains a simple controller for PreprovisioningImages that uses images provided in the environment variables <code>DEPLOY_ISO_URL</code> and <code>DEPLOY_RAMDISK_URL</code>. More sophisticated controllers may be written downstream (for example, the OpenShift image-customization-controller).</p>"},{"location":"reference/metal3/api/#preprovisioningimage-spec","title":"PreprovisioningImage spec","text":"<p>The PreprovisioningImage's spec provides input for the image building process.</p> <ul> <li> <p><code>acceptFormats</code>: a list of accepted image formats: <code>iso</code> or <code>initrd</code>.</p> </li> <li> <p><code>architecture</code>: the CPU architecture to build the image for, e.g. <code>x86_64</code>.   The default PreprovisioningImage controller does not use this field.</p> </li> <li> <p><code>networkData</code>: the name of a Secret with the network configuration for the image.   The default PreprovisioningImage controller does not use this field.</p> </li> </ul>"},{"location":"reference/metal3/api/#preprovisioningimage-status","title":"PreprovisioningImage status","text":"<p>The PreprovisioningImage's status provides information about the resulting image.</p> <ul> <li> <p><code>architecture</code>: the CPU architecture of the image, e.g. <code>x86_64</code>.</p> </li> <li> <p><code>extraKernelParams</code>: additional kernel parameters that baremetal-operator will add to the node   when PXE booting the initramfs image. Has no meaning for ISO images.</p> </li> <li> <p><code>format</code>: format of the image: <code>iso</code> or <code>initrd</code>. Must be one of the formats provided in <code>acceptFormats</code>.</p> </li> <li> <p><code>imageUrl</code>: the URL of the resulting image.</p> </li> <li> <p><code>kernelUrl</code>: the URL of the kernel to use together with the initramfs image. If not provided,   baremetal-operator uses the default kernel image from the environment variables.   Has no meaning for ISO images.</p> </li> <li> <p><code>networkData</code>: the name of a Secret with the network configuration of the image.</p> </li> </ul>"},{"location":"reference/metal3/baremetalhost-states/","title":"BaremetalHost Provisioning States","text":"<p>The following diagram shows the possible Provisioning State transitions for the BaremetalHost object:</p> <p></p>"},{"location":"reference/metal3/baremetalhost-states/#created","title":"Created","text":"<p>Newly created hosts move immediately to Discovered or Registering. No host stays in the Created state while the operator is working properly.</p>"},{"location":"reference/metal3/baremetalhost-states/#unmanaged","title":"Unmanaged","text":"<p>An Unmanaged host is missing both the BMC address and credentials secret name, and does not have any information to access the BMC for registration.</p>"},{"location":"reference/metal3/baremetalhost-states/#externally-provisioned","title":"Externally Provisioned","text":"<p>An Externally Provisioned host was deployed using another tool and then a host object was created with the externallyProvisioned flag set. Hosts in this state are monitored, and only their power status is managed.</p>"},{"location":"reference/metal3/baremetalhost-states/#registering","title":"Registering","text":"<p>The host will stay in the Registering state while the BMC access details are being validated.</p>"},{"location":"reference/metal3/baremetalhost-states/#inspecting","title":"Inspecting","text":"<p>After the host is registered, an agent image will be booted on it using a ramdisk. The agent collects information about the available hardware components, and this process is called \"inspection.\" The host will stay in the Inspecting state until this process is completed.</p>"},{"location":"reference/metal3/baremetalhost-states/#preparing","title":"Preparing","text":"<p>When setting up RAID, BIOS and other similar configurations, the host will be in Preparing state. For ironic provisioner, we build and set up manual clean steps in Preparing state.</p>"},{"location":"reference/metal3/baremetalhost-states/#available","title":"Available","text":"<p>A host in the Available state is available to be provisioned. In older versions of the baremetal-operator, this state was called Ready.</p>"},{"location":"reference/metal3/baremetalhost-states/#provisioning","title":"Provisioning","text":"<p>While an image is being copied to the host and it is being configured to run the image the host will be in the Provisioning state.</p>"},{"location":"reference/metal3/baremetalhost-states/#provisioned","title":"Provisioned","text":"<p>After an image is copied to the host and the host is running the image, it will be in the Provisioned state.</p>"},{"location":"reference/metal3/baremetalhost-states/#deprovisioning","title":"Deprovisioning","text":"<p>When the previously provisioned image is being removed from the host, it will be in the Deprovisioning state.</p>"},{"location":"reference/metal3/baremetalhost-states/#error","title":"Error","text":"<p>If an error occurs during one of the processing states (Registering, Inspecting, Provisioning, Deprovisioning) the host will enter the Error state.</p>"},{"location":"reference/metal3/baremetalhost-states/#deleting","title":"Deleting","text":"<p>When the host is marked to be deleted, it will move from its current state to Deleting, at which point the resource record is deleted from kubernetes.</p>"},{"location":"reference/metal3/bmh_live_iso/","title":"Bmh live iso","text":""},{"location":"reference/metal3/bmh_live_iso/#add-boot-iso-api-to-baremetalhost","title":"Add boot-iso API to BareMetalHost","text":""},{"location":"reference/metal3/bmh_live_iso/#status","title":"Status","text":"<p>implemented</p>"},{"location":"reference/metal3/bmh_live_iso/#summary","title":"Summary","text":"<p>Add a new interface so that it is possible to boot arbitrary iso images via Ironic.</p>"},{"location":"reference/metal3/bmh_live_iso/#motivation","title":"Motivation","text":"<p>In some circumstances it is desirable to boot an existing iso image instead of having Ironic boot IPA, for example:</p> <p>To reduce boot time for ephemeral workloads, it may be possible to boot an iso and not deploy any image to disk (saving the time to write the image and reboot)</p> <p>Where an alternative installer exists that is distributed as a live-iso, it may be desirable to leverage that toolchain instead of the IPA deploy ramdisk, for example the fedora-coreos installer</p> <p>An Ironic API exists that supports this but it's not currently accessible via metal3.</p>"},{"location":"reference/metal3/bmh_live_iso/#goals","title":"Goals","text":"<p>Expose the Ironic API to boot iso images via metal3.</p>"},{"location":"reference/metal3/bmh_live_iso/#non-goals","title":"Non-Goals","text":"<p>This only considers the ability to boot an iso image (which Ironic supports via both redfish-virtualmedia and iPXE), not live kernel/ramdisk images.</p> <p>Ironic does support booting kernel/ramdisk images though, so the interface we decide on shouldn't preclude adding that capability in future.</p>"},{"location":"reference/metal3/bmh_live_iso/#proposal","title":"Proposal","text":"<p>Add a option to the BareMetalHost DiskFormat spec which indicates that instead of deploying an image to disk, a live-iso image will be booted:</p> <pre><code>  spec:\n    image:\n      url: http://1.2.3.4/image.iso\n      format: live-iso\n    online: true\n</code></pre> <p>Note that in this mode, <code>rootDeviceHints</code> and <code>userData</code> will not be used since the image won't be written to disk, and Ironic doesn't support passing user-data in addition to the iso attachment at present.</p> <p>At some future point it would be desirable for Ironic to add the ability to pass userData - then a generic iso could be booted with the ability to define customization via the userData field in the normal way.</p>"},{"location":"reference/metal3/bmh_live_iso/#design-details","title":"Design Details","text":"<p>When this mode is selected, we need to configure Ironic to use the ramdisk deploy interface which is not currently enabled in ironic-image.</p> <p>We will also need to write the provided ISO URL into the instance_info boot_iso field</p>"},{"location":"reference/metal3/bmh_live_iso/#risks-and-mitigations","title":"Risks and Mitigations","text":"<ul> <li> <p>Currently there is no detachment API in ironic, so in the case where an installer   iso is booted, we don't have a way to detach the virtualmedia and will have to   rely e.g on efibootmgr in whatever iso gets booted to ensure the correct boot device.</p> </li> <li> <p>The inspection of a BMH will still rely on booting IPA and there's no support for   fast-track provisioning in this workflow (since it's booting two different   iso images) so this adds a reboot into the process.  This can potentially be   avoided in some cases where we can provide a status annotation on creation of   the BMH such that inspection is not performed.</p> </li> <li> <p>Similarly cleaning of a BMH will rely on IPA, so will require a reboot which may   not be desirable in some situations.  This can potentially be avoided if cleaning   is made configurable in future.</p> </li> </ul>"},{"location":"reference/metal3/bmh_live_iso/#work-items","title":"Work Items","text":"<ul> <li>Enable ramdisk deploy interface in ironic-image</li> <li>Add <code>live-iso</code> image format option to the BMH schema</li> <li>Update BMO deploy logic to set the <code>deploy_interface</code> and <code>instance_info</code> appropriately</li> </ul>"},{"location":"reference/metal3/bmh_live_iso/#dependencies","title":"Dependencies","text":"<p>There are some Ironic roadmap items which can be considered soft-dependencies, these don't block the initial implementation but may be desirable to make this feature more flexible:</p> <p>Allow config-drive with virtualmedia ramdisk deploy</p> <p>Method to detach virtualmedia after initial boot</p>"},{"location":"reference/metal3/bmh_live_iso/#test-plan","title":"Test Plan","text":"<p>This should be tested in CI - do we have any metal3 coverage for redfish-virtualmedia atm?</p>"},{"location":"reference/metal3/bmh_live_iso/#upgrade-downgrade-strategy","title":"Upgrade / Downgrade Strategy","text":"<p>This new format is added to the BMH API as an optional new interface, all existing BMH interfaces should continue to work as before.</p> <p>On upgrade this new interface will become available, and once in use it will not be possible to downgrade, which given the expected use in net-new deployments is probably reasonable.</p>"},{"location":"reference/metal3/bmh_live_iso/#alternatives","title":"Alternatives","text":"<p>We could avoid exposing this interface and mandate all users rely on IPA to deploy disk images, but this doesn't provide a good solution for the emphemeral worload case, and requires testing/supporting two install paths where a platform (such as FCOS mentioned) provides existing iso based tooling to deploy to disk.</p>"},{"location":"reference/metal3/host-config-drive/","title":"Host config drive","text":""},{"location":"reference/metal3/host-config-drive/#host-config-drive","title":"host-config-drive","text":""},{"location":"reference/metal3/host-config-drive/#status","title":"Status","text":"<p>Implemented</p>"},{"location":"reference/metal3/host-config-drive/#summary","title":"Summary","text":"<p>Provisioning hosts requires two separate images. The first is the primary target image for the host, and contains the operating system and other software that the host will run. These are generally reusable across many hosts. Customization data can also be provided via a second \"config drive\" image, which contains configuration settings that are typically interpreted by a firstboot agent (cloud-init, ignition) in the primary target image.</p> <p>Customization data can be provided in several formats, but most commonly a \"user data\" blob is provided, with a format that depends on the specific firstboot agent.  This data can be  built into an ISO image, which is handled  by Ironic via writing an ISO to a separate partition with a predictable disk label, accessible to the primary target image when the host boots.</p> <p>Given use of Ironic, first boot agents must be configured to look for data in the OpenStack config drive format using the path <code>/openstack/latest/user_data</code>.</p> <p>User data contents are stored in a Secret within the kubernetes database because they can contain sensitive information.</p> <p>The baremetal operator can receive the Secret, extract the <code>userData</code> value, and pass the contents to Ironic as part of preparing the host for provisioning.</p>"},{"location":"reference/metal3/host-config-drive/#motivation","title":"Motivation","text":""},{"location":"reference/metal3/host-config-drive/#goals","title":"Goals","text":"<ul> <li>Avoid having the baremetal operator tightly coupled to provisioning   hosts to become nodes in the cluster.</li> <li>Avoid leaking secrets when passing the config drive to the baremetal   operator.</li> </ul>"},{"location":"reference/metal3/host-config-drive/#non-goals","title":"Non-Goals","text":"<p>N/A</p>"},{"location":"reference/metal3/host-config-drive/#proposal","title":"Proposal","text":""},{"location":"reference/metal3/host-config-drive/#implementation-detailsnotesconstraints","title":"Implementation Details/Notes/Constraints","text":"<p>User data settings come from the contents of a secret, is referenced via the BaremetalHost userData spec field.  The format of this data may differ depending on the firstboot tool in the primary OS image, so assumptions regarding the specific tool should be avoided in the BMO.</p> <p>Corresponding changes will be required in the Cluster/Machine API layer to ensure the required secret for the given host role is provided via the BMH userData field.</p>"},{"location":"reference/metal3/host-config-drive/#risks-and-mitigations","title":"Risks and Mitigations","text":"<p>Passing the user data to Ironic as a JSON string instead of an encoded ISO requires a recent version of Ironic (since the development cycle for Stein), an interim solution may be required until this is available in the metal3 images.</p>"},{"location":"reference/metal3/host-config-drive/#design-details","title":"Design Details","text":""},{"location":"reference/metal3/host-config-drive/#work-items","title":"Work Items","text":"<ul> <li>Add a <code>UserDataSecretRef</code> of type <code>SecretRef</code> to the   <code>BareMetalHostSpec</code> structure to hold the location of the Secret   containing the user data.</li> <li>We may want to define a new type to hold all of the provisioning   instructions, rather than adding individual fields to the host spec   directly.</li> <li>Update the cluster-api provider to find and pass the worker user data Secret to   the baremetal operator through the new field in the   <code>BareMetalHostSpec</code>.</li> <li>Update the baremetal operator to retrieve the user data Secret   content and pass it to Ironic, when it is present.</li> </ul>"},{"location":"reference/metal3/host-config-drive/#dependencies","title":"Dependencies","text":"<p>This will require work in both the actuator/provider and operator repositories.</p> <p>We will need to use version of Ironic from the Stein release series, which includes the user data support in the API.</p>"},{"location":"reference/metal3/host-config-drive/#test-plan","title":"Test Plan","text":"<p>Manual or automated integration tests for ensuring the config drive content is applied to the server during provisioning.</p>"},{"location":"reference/metal3/host-config-drive/#upgrade-downgrade-strategy","title":"Upgrade / Downgrade Strategy","text":"<p>N/A</p>"},{"location":"reference/metal3/host-config-drive/#version-skew-strategy","title":"Version Skew Strategy","text":"<p>N/A</p>"},{"location":"reference/metal3/host-config-drive/#drawbacks","title":"Drawbacks","text":"<p>N/A</p>"},{"location":"reference/metal3/host-config-drive/#alternatives","title":"Alternatives","text":"<p>N/A</p>"},{"location":"reference/metal3/host-config-drive/#references","title":"References","text":"<ul> <li>CoreOS setting for the config drive user data path</li> <li>golang config drive builder in gophercloud/utils</li> </ul>"},{"location":"reference/metal3/how-ironic-works/","title":"How ironic works","text":""},{"location":"reference/metal3/how-ironic-works/#how-ironic-works","title":"how-ironic-works","text":"<p>This document explains how to use ironic in order to achieve various tasks such as creating a node, recreating a node, unprovisioning a node, and deleting a node.</p> <p>This is not intended to be design specific documentation, but intends to help convey the mechanics of how, such that the reader does not have to become an expert in Ironic in order to learn to leverage it.</p>"},{"location":"reference/metal3/how-ironic-works/#how-ironic-controls-hardware","title":"How ironic controls hardware","text":"<p>Ironic is largely designed around the ability to issue commands to a remote Baseboard Management Controller (BMC) in order to control the desired next boot device and the system power state.</p> <p>Without BMC functionality, the power and potential boot mode changes need to be performed by an external entity such as a human or a network attached power distribution unit.</p>"},{"location":"reference/metal3/how-ironic-works/#how-ironic-boots-hardware","title":"How ironic boots hardware","text":"<p>Typically nodes are booted utilizing PXE. In the most ideal scenario this would be a hybrid PXE/iPXE configuration such that the deployment ramdisk for ironic is able to be quickly and efficently transferred to the node.</p> <p>In order to assert this configuration at boot time, a dedicated DHCP server on a semi-dedicated \"provisioning\" network should be leveraged. This network may be reused by other services and systems, but can also be leveraged in the case of hardware discovery. The most important factor is that this network does not have a second DHCP server attached.</p> <p>In the use case in which Ironic was developed, it would manage the DHCP server configuration. In this use case, we would rely upon a static configuration being provided by the DHCP server to signal where to find the initial components required to boot the ramdisk.</p> <p>Some specific hardware types in ironic do support use of virtual media to boot the deployment ramdisk, however this functionality is not available with the vendor neutral IPMI driver. Newer protocols such as Redfish may support virtual media, but not as of the time of this document having been composed. Virtual media should be considered not well enough supported at this time to be considered useful. Other methods such as booting directly from iSCSI should be considered out-of-scope in this use case as they are require an external block storage management system.</p>"},{"location":"reference/metal3/how-ironic-works/#how-ironic-writes-an-operating-system-image-to-baremetal","title":"How ironic writes an operating system image to baremetal","text":"<p>Ironic supports two fundamental types of disk images: whole-disk and partition (or filesystem) images. The Metal3 use cases will rely on whole disk images.</p> <p>The basic workflow consists of:</p> <ol> <li>Booting the deployment ramdisk</li> <li>The deployment ramdisk checks in with the ironic introspection    service which updates configuration information stored in ironic    about the baremetal machine.</li> <li>The deployment ramdisk checks in with the ironic service and leverages    the MAC addresses to help identify the physical machine in the hardware    inventory.</li> <li>Ironic initiates deployment by first identifying the root disk upon    which the disk image is to be written. By default this will be the    smallest storage device available, and can be overridden via explict    configuration of a root_device hint</li> <li>The deployment ramdisk downloads the image to be written and streams    that to the storage device.</li> <li>If defined as part of the deployment, ironic will add an additional    partition for a configuration drive. Ironic will then write the    configuraiton drive to disk</li> <li>Finally ironic reboots the machine.</li> </ol> <p>There are some additional steps that ironic performs, mainly fixing partition table data with GPT based partition tables in order to prevent issues after deployment, but these steps are incorporated as part of the deployment sequence to help ensure that the machine will deploy successfully without issues.</p>"},{"location":"reference/metal3/how-ironic-works/#what-connectivity-is-required","title":"What connectivity is required","text":"<p>Access to the BMC can be via a routed IP network, however this may be less desirable than having it on the same L2 network as Ironic from a security standpoint.</p> <p>When virtual media is used, the BMC needs to be on a network that allows it to reach the host serving the virtual media image.</p> <p>To boot the discovery and deployment image on the node, it will need access to the ironic host using:</p> <ul> <li>DHCP (for IP assignment and PXE instructions)</li> <li>TFTP (if iPXE is not natively supported by the network interfaces.)</li> <li>HTTP in order to download kernel/ramdisk images via HTTP over a TCP   connection.</li> </ul> <p>Connections from the ramdisk are to the host upon which ironic is executing.</p> <p>The discovery and deployment ramdisk image needs to be able to:</p> <ul> <li>DHCP (via the ironic host, for IP assignment and PXE instructions)</li> <li>Resolve DNS (FIXME - also via the ironic host?)</li> <li>Connect to the ironic inspector API endpoint, which operates   on port 5050/TCP by default.</li> <li>Connect to the ironic API endpoint, which operates on port   6385/TCP by default.</li> <li>The ramdisk needs to be able to reach an external   HTTP(s) endpoint in order to download the image files for   deployment.</li> <li>Be accessible on port 9999/TCP. This is used by ironic to issue   commands to the running ramdisk.</li> </ul> <p>Between ironic and ironic-inspector:</p> <ul> <li>Each service must be able to reach the API endpoint for   the other service.</li> </ul>"},{"location":"reference/metal3/how-ironic-works/#what-is-ironic-inspector","title":"What is ironic-inspector","text":"<p>Ironic-inspector is an auxiliary service that provides separate API to inspect and register hardware properties for a bare metal node (node introspection process). Ideally the node UUID must be already known by ironic, that means the node is enrolled and power management credentials should be already provided to allow sending reboot commands to the node, but this process may also be triggered during discovery and allow automatic node registration in ironic. The process of \"hardware introspection\" allows also to get the hardware parameters from a bare metal node and ready it for scheduling.</p> <p>Start, abort introspection, get introspection status, get introspection data are done through the <code>/v1/introspection</code> resource.</p> <p>To start the introspection process using ironic-inspector API send a POST request with an empty body using:</p> <pre><code>    POST /v1/introspection/node-id\n</code></pre> <p>The normal response code is 202 and if inspector can't find the node it will return a 404. The response is also an empty body.</p> <p>It's possible to monitor the status of a single introspection process using:</p> <pre><code>    GET /v1/introspection/node-id\n</code></pre> <p>This provides not only the state of the process, but also information on start and ending time, and more. An example of status answer:</p> <pre><code>{\n  \"error\": null,\n  \"finished\": true,\n  \"finished_at\": \"2017-08-16T12:24:30\",\n  \"links\": [\n    {\n      \"href\": \"http://127.0.0.1:5050/v1/introspection/c244557e-899f-46fa-a1ff-5b2c6718616b\",\n      \"rel\": \"self\"\n    }\n  ],\n  \"started_at\": \"2017-08-16T12:22:01\",\n  \"state\": \"finished\",\n  \"uuid\": \"c244557e-899f-46fa-a1ff-5b2c6718616b\"\n}\n</code></pre> <p>There can be multiple introspection processes running at the same time, it's possible to retrieve all the statuses using:</p> <pre><code>    GET /v1/introspection\n</code></pre> <p>The output will be a paginated list of single status as shown above.</p> <p>If it's needed, it's possible to interrupt the introspection process using:</p> <pre><code>    POST /v1/introspection/node-id/abort\n</code></pre> <p>At the end of the introspection process, to return all the data stored for a single node:</p> <pre><code>    GET /v1/introspection/node-id/data\n</code></pre> <p>The result will be a json body which content may vary based on the ramdisk used and the version of ironic-inspector itself.</p>"},{"location":"reference/metal3/how-ironic-works/#how-to","title":"How-to","text":""},{"location":"reference/metal3/how-ironic-works/#how-to-discover-hardware","title":"How to discover hardware","text":"<p>New hardware can be discovered by booting the deployment and discovery ramdisk with the \"ipa-inspection-callback-url\" kernel parameter. This URL is used by the agent on the deployment and discovery ramdisk as the location of the ironic-inspector service where it will post hardware profile information to.</p> <p>The ironic-inspector service then processes this data, and updates stored data or creates a new node and associated supporting records in ironic.</p>"},{"location":"reference/metal3/how-ironic-works/#how-to-add-hardware-to-ironic","title":"How to add hardware to ironic","text":"<p>The action of creating a node is part of the enrollment process and the first step to prepare a node to reach the \"available\" status At the end of the creation, the node status will be \"enroll\".</p> <p>All nodes must be created with a valid hardware type, or \"driver\". Valid and maintained in-tree hardware types, or drivers, as of ironic 12.0.0 is: idrac, ilo, ipmi, irmc, redfish, snmp, and xclarity.</p> <p>Usually more info are provided, at least a node name and parameters to initialize the drivers, such as username and password, if needed, passed through the \u201cdriver_info\u201d option.</p> <p>An example of a typical node create request in JSON format:</p> <pre><code>{\n\"name\": \"test_node_dynamic\",\n\"driver\": \"ipmi\",\n\"driver_info\": {\n    \"ipmi_username\": \"ADMIN\",\n    \"ipmi_password\": \"password\"\n},\n\"power_interface\": \"ipmitool\"\n}\n</code></pre> <p>The response, if successful, contains a complete record of the node in JSON format with provided or default ({}, \u201cnull\u201d, or \u201c\u201d) values.</p>"},{"location":"reference/metal3/how-ironic-works/#updating-information-about-a-hardware-node-in-ironic","title":"Updating information about a hardware node in ironic","text":"<p>All node information can be updated after the node has been created.</p> <p>Send a PATCH to <code>/v1/nodes/node-id</code> in the form of a JSON PATCH document.</p> <p>The normal response is a 200 and contains a complete record of the node in JSON format with the updated node.</p>"},{"location":"reference/metal3/how-ironic-works/#how-do-i-identify-the-current-state","title":"How do I identify the current state","text":"<p>All nodes in ironic are tied to a state which allows ironic to track what actions can be performed upon the node and convey its general disposition. This field is the \"provision_state\" field that can be retrieved via the API.</p> <pre><code>GET /v1/nodes/node-id\n</code></pre> <p>Inside the returned document, a \"provision_state\" field can be referenced. Further information can be found in ironic's state machine documentation.</p>"},{"location":"reference/metal3/how-ironic-works/#how-to-inspect-hardware","title":"How to inspect hardware","text":"<p>Inspection may be used if a baremetal node has not been already discovered or inspected previously in order to collect up to date details about the hardware. This is particullarly important in order to update the records of networking ports for identification of the baremetal node and creation of PXE/iPXE configuration files in order to help ensure that baremetal node is quickly booted for booting into the deployment and discovery ramdisk.</p> <pre><code>PUT /v1/nodes/node-id/provision/states\n{\"target\": \"inspect\"}\n</code></pre> <p>This operation can only be performed in the \"manageable\" ironic node state. If the node is already in the \"available\" state, the same requst can be used with a target of \"manage\", and then the target of \"inspect\" can be utilized to step through the state machine.</p> <p>After inspection, it is advisable to return nodes to an \"available\" state. This can be performed simillarly via the target \"provide\".</p>"},{"location":"reference/metal3/how-ironic-works/#how-to-deploy","title":"How to deploy","text":"<p>Starting with the bare metal node in the \"available\" provision_state:</p> <ol> <li> <p>First assert configuration to API to indicate the image written    to disk. This is performed as a HTTP PATCH request to the    <code>/v1/nodes/node-id</code> endpoint.</p> <p>{    {\u201cop\u201d: \u201creplace\u201d, \u201cpath\u201d: \u201c/instance_info\u201d, \u201cvalue\u201d: {        \u201cimage_source\u201d: \u201chttp://url-to-image\u201d,        \u201cimage_os_hash_algo\u201d: \u201csha256\u201d,        \u201cimage_os_hash_value\u201d: \u201cabcdefghi\u2026\u201d}},    {\u201cop\u201d: \u201creplace\u201d, \u201cpath\u201d: \u201c/instance_uuid\u201d, \u201cvalue\u201d: \u201canyuuidvalue\u201d}},    }</p> </li> </ol> <p>NOTE: Instead of defining the \"image_os_hash_*\" values, a MD5 based    image checksum can be set.</p> <p>This configuration does two things. First sets the image and checksum    to be utilized for image verification, and sets an \"instance_uuid\" value    which acts as a signal to any client that the node has been claimed by    an API cient. The instance_uuid can be set to any value, and is    ultimately not required.</p> <ol> <li> <p>Request ironic to perform a \"validate\" operation on the information    it is presently configured with. The expected response is a HTTP 200    return code, with a message body that consists of a list of \"driver\"    interfaces and any errors if applicable.</p> <p>GET /v1/nodes/node-id/validate</p> </li> </ol> <p>Reply:</p> <pre><code>   {\n   \"boot\": true,\n   ..\n   \"deploy\": \"configuration error message if applicable\"\n   }\n</code></pre> <p>The particular interfaces that would be important to pay attention to are    \u2018boot\u2019, \u2018deploy\u2019, \u2018power\u2019, \u2018management\u2019.</p> <p>More information can be found in the API documentation.</p> <ol> <li>Craft a configuration drive file</li> </ol> <p>Configuration drives are files that contain a small ISO9660 filesystem    which contains configuration metadata and user defined \"user-data\".</p> <p>1) Create a folder called \u201cTEMPDIR\u201d    2) In the case of ignition based configuration, that file would be       renamed \"user_data\" and placed in <code>TEMPDIR/openstack/latest/</code>       folder.    3) Metadata for networking configuration setup using \"cloud-init\" or       a similar application would also be written to the       <code>TEMPDIR/openstack/latest</code> as well. This is out of scope, but is well       documented in the OpenStack community.    4) Create an iso9660 image containing the contents of TEMPDIR using       a label of \u201cconfig-2\u201d.    5) Compress the resulting ISO9660 image file using the gzip       algorithm.    6) Encode the resulting gzip compressed image file in base64 for       storage and transport. Ironic does the needful to decode and uncompress       the configuration drive prior to deployment.</p> <ol> <li> <p>Send a HTTP POST to <code>/v1/nodes/node-id/states/provision</code> to initiate    the deployment</p> <p>{\u201ctarget\u201d: \u201cactive\u201d,     \u201cconfigdrive\u201d: \u201chttp://url-to-config-drive/node.iso.gz\u201d}</p> </li> </ol> <p>Once the request to make the node active has been received by ironic,    it will proceed with the deployment process and execute the required    steps to help ensure the baremetal node reboots into the requested    disk image.</p> <ol> <li>Monitor the provisioning operation by fetching the machine    state periodically, looking    for it to be set to <code>active</code>.</li> </ol> <p>The \"provision_state\" field will track the state of the node along    the state machine. A provision_state field with \"active\" means the    deployment has been completed.</p> <p>As the deployment is progressing, the \"provision_state\" may    alternate between \"deploying\" and \"deploy wait\" states. Deploying    indicates that the ironic server is actively working on the    deployment, where as \"deploy wait\" indicates that ironic is waiting    for the agent on the baremetal node to boot, write contents to    disk, or complete any other outstanding task issued by Ironic.</p> <p>A \"deploy failed\" state indicates that the deployment failed, and    additional details as to why can be retrieved from the \"last_error\"    fiend in the JSON document. With newer versions of ironic, greater    granularity can be observed by also refering the \"deploy_step\"    field, however this is a relatively new feature in ironic and the    information provided is fairly broad as of the time this document    was written.</p>"},{"location":"reference/metal3/how-ironic-works/#how-to-unprovision-a-baremetal-node","title":"How to unprovision a baremetal node","text":"<p>A provisioned, or \"active\" baremetal node can be unprovisioned by sending a state change request to the ironic api. This request will move the baremetal node through the \"cleaning\" process which ironic utilizes to erase the contents of the disks. This can be a time intensive process, and ultimately may only be useful for cleaning metadata except in limited circumstances.</p> <pre><code>PUT /v1/nodes/node-id/states/provision\n{\"target\": \"deleted\"}\n</code></pre>"},{"location":"reference/metal3/how-ironic-works/#how-to-delete-a-baremetal-node","title":"How to delete a baremetal node","text":"<p>Deletion of a node in ironic is removal from its inventory.</p> <p>As ironic is designed to manage the lifecycle of baremetal nodes, protections exist to prevent users of the ironic API from deleting nodes in states that may not be ideal. Mainly this restricts deletion to states where the node is not in use or is not actively performing a task.</p> <p>Safe states are:</p> <ul> <li>\"available\"</li> <li>\"manageable\"</li> <li>\"enroll\"</li> <li>\"adopt fail\"</li> </ul> <p>This may be overridden by putting the node into \"maintenance\", during which ironic will not attempt to perform any operations.</p> <pre><code>    PUT /v1/nodes/node-id/maintenance\n    DELETE /v1/nodes/node-id\n</code></pre> <p>NOTE: Care should be taken to avoid triggering the deletion of a node in a \"clean*\" states.</p> <p>Additional information on states and state transitions in ironic can be found in the ironic documentation</p>"},{"location":"reference/metal3/how-ironic-works/#how-to-create-the-record-of-an-active-node","title":"How to create the record of an active node","text":"<p>Ironic possesses the functionality to create a node and move it into an \"active\" state from the \"manageable\" state. This is useful to create the record of an \"active\" node without performing a deployment.</p> <p>Details on this functionality can be found in the ironic adoption documentation</p>"},{"location":"reference/metal3/how-ironic-works/#references","title":"References","text":"<ul> <li>Ironic Documentation</li> </ul>"},{"location":"reference/metal3/reboot-interface/","title":"Reboot interface","text":""},{"location":"reference/metal3/reboot-interface/#reboot-interface","title":"reboot-interface","text":""},{"location":"reference/metal3/reboot-interface/#status","title":"Status","text":"<p>implemented</p>"},{"location":"reference/metal3/reboot-interface/#summary","title":"Summary","text":"<p>A declarative API is proposed to request the baremetal operator to reboot a provisioned Host in an atomic step (i.e. without the need to make multiple sequential changes to the spec).</p>"},{"location":"reference/metal3/reboot-interface/#motivation","title":"Motivation","text":"<p>We require a non-destructive way to fence a Kubernetes node. Some nodes cannot be replaced (it is currently not possible for a new controlplane to join a cluster), or are expensive to replace (e.g. if this would require rebalancing Ceph data). A solution to this is for fencing to reboot the Host, thus ensuring that running processes are stopped to avoid a split-brain scenario while still allowing the node to rejoin the cluster with its data intact (albeit stale) after the reboot.</p> <p>The expected implementation of this on the fencing side is an annotation on the Machine resource that requests that it be remediated. The actual reboot will be effected either by the Machine Actuator itself (if the Cluster-API SIG can be persuaded that this should be part of the API in the long term), or some equivalent of the Machine Actuator limited to just this purpose.</p>"},{"location":"reference/metal3/reboot-interface/#goals","title":"Goals","text":"<p>The Machine (Reboot) Actuator will require from the BareMetalHost:</p> <ul> <li>A declarative API to perform the reboot</li> <li>The ability to determine a point where all processes running on the Host at   the time of the fencing decision are guaranteed to have been stopped</li> <li>A guarantee that the Reboot Actuator can delete any Node associated with the   Machine before the Host is able to complete booting (including in testing   when the Host is simulated by a VM).</li> <li>Rapid convergence to a state where all running processes are stopped,   independent of other happenings in the system</li> </ul>"},{"location":"reference/metal3/reboot-interface/#non-goals","title":"Non-Goals","text":"<p>There is no requirement to implement a scheduled reboot. In the Kubernetes context, reboot decisions should generally be made in realtime by some higher level of the Node/Machine hierarchy, to take into account such questions as the overall health of the cluster and the effect of a reboot on that. The best implementation for this would be a RebootSchedule CRD that waits until the appointed time before issuing an immediate reboot request to the BareMetalHost. This allows multiple reboots to be scheduled, scheduled reboots to be manipulated or cancelled, and a record to be left behind of past scheduled reboots. The proposed design could easily be extended to accomodate this requirement should it arise in future.</p> <p>This API is not responsible for managing unprovisioned hosts, e.g. to recover from state inconsistencies in Ironic prior to provisioning. Any such inconsistencies that are not handled internally by the BareMetalHost will likely need manual intervention to recover anyway.</p>"},{"location":"reference/metal3/reboot-interface/#proposal","title":"Proposal","text":"<p>A new date-time field, <code>lastPoweredOn</code>, will be added to the <code>provisioning</code> section of the BareMetalHost status. This records a time after which the Host was last booted using the current image. Processes running prior to this time may be assumed to have been stopped.</p> <p>A new date-time field, <code>pendingRebootSince</code>, will be added to the <code>provisioning</code> section of the BareMetalHost status. This records a time before which the Host was last requested to reboot (because we cannot trust any value from the user, who even if well intentioned, may have created the timestamp on a machine that was not synchronised with the cluster or has a different timezone).</p> <p>Since the user interface requirements are still unclear, we will follow standard practices of using an annotation to trigger reboots.</p> <p>The basic annotation form (<code>reboot.metal3.io</code>) triggers the controller to power cycle the Host. This form has set-and-forget semantics and the controller removes the annotation once it restores power to the Host.</p> <p>An advanced form (<code>reboot.metal3.io/{key}</code>) instructs the controller hold the Host in a <code>PoweredOff</code> state so that the caller can perform any required actions while the node is in a known safe state. Callers indicate to the controller that they are ready to continue by removing the annotation with their unique <code>{key}</code> suffix.</p> <p>In the case of multiple clients, the controller will wait for all annotations of the form <code>reboot.metal3.io/{key}</code> to be removed before powering on the Host.</p> <p>If both <code>reboot.metal3.io</code> and <code>reboot.metal3.io/{key}</code> forms are in use, the <code>reboot.metal3.io/{key}</code> form will take priority.</p> <p>In all cases, the content of the annotation is ignored but preserved. This ensures that whatever placed the annotation can have a way of tracing it back to its source. In the case of remediation, the content will be the UID of the Machine resource being remediated.</p> <p>The actual power management will be performed by the Host controller. This is necessary to avoid race conditions by ensuring that the <code>Online</code> flag and any reboot requests are managed in the same place.</p> <p>If:</p> <ul> <li>there is one or more annotations with the <code>reboot.metal3.io</code> prefix   present, and</li> <li>the <code>Status.PoweredOn</code> field is true, and</li> <li>the value of <code>pendingRebootSince</code> is empty or earlier than the   <code>lastPoweredOn</code> time</li> </ul> <p>then the Host controller will update <code>pendingRebootSince</code> to the current time.</p> <p>Whenever <code>pendingRebootSince</code> is later than the <code>lastPoweredOn</code> time, the Host controller will attempt to power the host off regardless of the <code>Spec.Online</code> setting.</p> <p>Once the Host is powered off (<code>Status.PoweredOn</code> is false), if/when</p> <ul> <li>the <code>Spec.Online</code> field is true, and</li> <li>the <code>lastPoweredOn</code> time is before the <code>pendingRebootSince</code> time</li> </ul> <p>then the controller should remove the suffixless <code>reboot.metal3.io</code> annotation (if present). Once no reboot annotations are present (i.e. those of the form <code>reboot.metal3.io/{key}</code> have been removed by their originators), the existing logic for powering on the Host should execute and update the <code>lastPoweredOn</code> timestamp accordingly.</p> <p>The controller automatically removes all annotations with the <code>reboot.metal3.io</code> prefix if</p> <ul> <li>the Host is deprovisioned</li> </ul> <p>The annotation value should be a JSON map containing a key <code>'mode'</code> with values of <code>'hard'</code> or <code>'soft'</code> to specify the reboot mode. The default reboot mode is <code>'soft'</code>. These are a few examples of using the reboot API:</p> <ul> <li><code>reboot.metal.io</code> -- immediate reboot via soft shutdown first,   followed by a hard shutdown if the soft shutdown fails.</li> <li><code>reboot.metal3.io: {'mode':'hard'}</code> -- immediate reboot via hard   shutdown, potentially allowing for high-availability use-cases.</li> <li><code>reboot.metal3.io/{key}</code> -- phased reboot, issued and managed by the   client registered with the <code>key</code>, via soft shutdown first, followed   by a hard reboot if the soft reboot fails.</li> <li><code>reboot.metal3.io/{key}: {'mode':'hard'}</code> -- phased reboot, issued   and managed by the client registered with the <code>key</code>, via hard   shutdown.</li> </ul> <p>It's important to note that as the API supports multiple clients, a hard shutdown request takes priority over a soft shutdown request, allowing workload recovery to take precedence over a graceful shutdown of a node.</p>"},{"location":"reference/metal3/reboot-interface/#drawbacks","title":"Drawbacks","text":"<p>This requires clients to be somewhat well-behaved - for example, only deleting their own annotations and never deleting others' annotations.</p> <p>It would be difficult to apply RBAC to the reboot operation specifically, since it is triggered by simply adding an annotation (that doesn't even have a fixed key).</p>"},{"location":"reference/metal3/reboot-interface/#future-enhancements","title":"Future Enhancements","text":""},{"location":"reference/metal3/reboot-interface/#defining-a-formal-user-interface","title":"Defining a Formal User Interface","text":"<p>Once we have experience with using this, we will have more information about how to design a formal interface for it (rather than using annotations).</p>"},{"location":"reference/metal3/reboot-interface/#alternatives","title":"Alternatives","text":"<p>The Machine (Reboot) Actuator could perform the reboot in an imperative, rather than declarative, manner by manipulating the <code>Online</code> spec of the Host in sequence to <code>false</code> and then back to <code>true</code> again once the <code>poweredOn</code> status was seen to be <code>false</code>. However, in the presence of multiple actors this approach is prone to race conditions or controller fights.</p> <p>We could create a <code>HostRebootRequest</code> CRD and have the existing host controller check for pending reboot requests, perform reboots when necessary, and update the state of the request. This would be easier to apply RBAC to, and indeed does not require clients to co-operate in any way. It would be a flexible jumping off point for adding more advanced features should they become required. However, it adds significant complexity (another CRD to deploy) and would be a permanent, formal interface. It's also not clear how outdated requests should be cleaned up.</p> <p>The request could simply be a timestamp field in the Host spec. The Host would coalesce multiple requests into one at the time they are received (rather than at the point of action). This would be simpler, but requires the client to provide a meaningful timestamp (i.e. in the recent past), forcing us to make awkward decisions about what to do with bogus data. In contrast, allowing multiple annotations allows the system to record the time at which it first noticed them. Selecting this option would also make it more difficult to add further reboot-related API features in the future.</p> <p>The request could be made by adding to a list of client-chosen strings in the Host spec. This effectively formalises the annotation-based system proposed here and makes it a permanent part of the interface.</p>"},{"location":"reference/metal3/remove-host/","title":"Remove a Host from a Cluster","text":"<p>At some point you will need to remove a host from a cluster. You may be removing failed hardware, downsizing a healthy cluster, or have some other reason.</p> <p>Since removal involves a BareMetalHost, Machine, and MachineSet, it can be non-obvious how best to accomplish host removal. This document provides guidance on how to do so.</p>"},{"location":"reference/metal3/remove-host/#steps","title":"Steps","text":"<p>These steps are both safe and compatible with automation that scales MachineSets to match the number of BareMetalHosts.</p>"},{"location":"reference/metal3/remove-host/#annotate-the-machine","title":"Annotate the Machine","text":"<p>Find the Machine that corresponds to the BareMetalHost that you want to remove. Add the annotation <code>cluster.k8s.io/delete-machine</code> with any value that is not an empty string.</p> <p>This ensures that when you later scale down the MachineSet, this Machine is the one that will be removed.</p>"},{"location":"reference/metal3/remove-host/#delete-the-baremetalhost","title":"Delete the BareMetalHost","text":"<p>Delete the BareMetalHost resource. This may take some time.</p>"},{"location":"reference/metal3/remove-host/#scale-down-machineset","title":"Scale down MachineSet","text":"<p>Find the corresponding MachineSet and scale it down to the correct level. This will cause the host's Machine to be deleted.</p>"},{"location":"reference/metal3/remove-host/#other-approaches","title":"Other Approaches","text":""},{"location":"reference/metal3/remove-host/#delete-the-machine-first","title":"Delete the Machine First","text":"<p>If you delete the Machine first, that will cause the BareMetalHost to be deprovisioned. You would still need to issue a subsequent delete of the BareMetalHost. That opens the possibility that for some period of time, the BareMetalHost could be fully deprovisioned and show as \"available\"; another Machine without a host could claim it before it gets deleted.</p> <p>Additionally, by deleting the Machine before scaling down the MachineSet, the MachineSet will try to replace it with a new Machine resource. That new resourse could match a BareMetalHost if one is available and cause it to start provisioning. For this reason, it is better to not directly delete a Machine.</p>"},{"location":"reference/metal3/remove-host/#scale-down-the-machineset","title":"Scale down the MachineSet","text":"<p>You could annotate the Machine and then directly scale down the MachineSet without first deleting the BareMetalHost. This will cause the Machine to be deleted, but then the same downsides apply as described above; the BareMetalHost could be in an \"available\" state for some period of time.</p>"},{"location":"user-guide/","title":"User Guide","text":""},{"location":"user-guide/EasyBuild/","title":"EasyBuild (via EESSI)","text":"<p>As part of our installations, we include EasyBuild via EESSI. We do this by utilising CVMFS (the Cern VM FileSystem) within our environments to fuse-mount EESSI pre-built modulefiles (including EasyBuild) optimised for your architecture.</p>"},{"location":"user-guide/EasyBuild/#general-usage","title":"General usage","text":"<p>On a login system, to use the EESSI software firstly you have to source the environment:</p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash\n</code></pre> <p>Then you'll be presented with the ability to use lmod to either use the pre-compiled modules (that match against your architecture using archdetect) using <code>module avail</code>, <code>module load &lt;module&gt;</code> <code>module list</code>, and so on, or recompile yourself for your environment. <pre><code>[ubiqsupport@login ~]$ source /cvmfs/software.eessi.io/versions/2023.06/init/bash\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/intel/skylake_avx512\nUsing x86_64/intel/skylake_avx512 as software subdirectory.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/intel/skylake_avx512/.lmod/lmodrc.lua\nFound Lmod SitePackage.lua file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/intel/skylake_avx512/.lmod/SitePackage.lua\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/intel/skylake_avx512/modules/all as the directory to be added to MODULEPATH.\nUsing /cvmfs/software.eessi.io/host_injections/2023.06/software/linux/x86_64/intel/skylake_avx512/modules/all as the site extension directory to be added to MODULEPATH.\nFound libcurl CAs file at RHEL location, setting CURL_CA_BUNDLE\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/intel/skylake_avx512/modules/all to $MODULEPATH...\nPrepending site path /cvmfs/software.eessi.io/host_injections/2023.06/software/linux/x86_64/intel/skylake_avx512/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n{EESSI 2023.06} [ubiqsupport@login ~]$ module av\n\n--------------------------------------------------------------- /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/intel/skylake_avx512/modules/all ----------------------------------------------------------------\n   Abseil/20230125.2-GCCcore-12.2.0                 gfbf/2023a                                            libunwind/1.6.2-GCCcore-12.3.0                          PROJ/9.3.1-GCCcore-13.2.0                     (D)\n   Abseil/20230125.3-GCCcore-12.3.0          (D)    gfbf/2023b                                  (D)       libunwind/1.6.2-GCCcore-13.2.0                   (D)    protobuf-python/4.24.0-GCCcore-12.3.0\n   ALL/0.9.2-foss-2023a                             Ghostscript/10.0.0-GCCcore-12.2.0                     libvorbis/1.3.7-GCCcore-12.2.0                          protobuf/23.0-GCCcore-12.2.0\n   AOFlagger/3.4.0-foss-2023b                       Ghostscript/10.01.2-GCCcore-12.3.0          (D)       libvorbis/1.3.7-GCCcore-12.3.0                   (D)    protobuf/24.0-GCCcore-12.3.0                  (D)\n   archspec/0.2.1-GCCcore-12.3.0                    giflib/5.2.1-GCCcore-12.2.0                           libwebp/1.3.1-GCCcore-12.3.0                            PuLP/2.8.0-foss-2023a\n   Armadillo/11.4.3-foss-2022b                      giflib/5.2.1-GCCcore-12.3.0                           libwebp/1.3.2-GCCcore-13.2.0                     (D)    pybind11/2.10.3-GCCcore-12.2.0\n   Armadillo/12.6.2-foss-2023a                      giflib/5.2.1-GCCcore-13.2.0                 (D)       libxc/6.1.0-GCC-12.2.0                                  pybind11/2.11.1-GCCcore-12.3.0\n   Armadillo/12.8.0-foss-2023b               (D)    git/2.38.1-GCCcore-12.2.0-nodocs                      libxc/6.2.2-GCC-12.3.0                           (D)    pybind11/2.11.1-GCCcore-13.2.0                (D)\n   arpack-ng/3.8.0-foss-2022b                       git/2.41.0-GCCcore-12.3.0-nodocs                      libxml2/2.10.3-GCCcore-12.2.0                           pyfaidx/0.8.1.1-GCCcore-12.3.0\n   arpack-ng/3.9.0-foss-2023a                       git/2.42.0-GCCcore-13.2.0                   (D)       libxml2/2.11.4-GCCcore-12.3.0                           PyOpenGL/3.1.7-GCCcore-12.3.0\n   arpack-ng/3.9.0-foss-2023b                (D)    GitPython/3.1.40-GCCcore-12.3.0                       libxml2/2.11.5-GCCcore-13.2.0                    (D)    PyQt-builder/1.15.4-GCCcore-12.3.0\n   arrow-R/11.0.0.3-foss-2022b-R-4.2.2              GLib/2.75.0-GCCcore-12.2.0                            libxslt/1.1.37-GCCcore-12.2.0                           PyQt5/5.15.10-GCCcore-12.3.0\n   arrow-R/14.0.1-foss-2023a-R-4.3.2         (D)    GLib/2.77.1-GCCcore-12.3.0                            libxslt/1.1.38-GCCcore-12.3.0                           Pysam/0.22.0-GCC-12.3.0\n   Arrow/11.0.0-gfbf-2022b                          GLib/2.78.1-GCCcore-13.2.0                  (D)       libxslt/1.1.38-GCCcore-13.2.0                    (D)    pystencils/1.3.4-gfbf-2023b\n   Arrow/14.0.1-gfbf-2023a                   (D)    GLPK/5.0-GCCcore-12.2.0                               libyaml/0.2.5-GCCcore-12.3.0                            pytest-flakefinder/1.1.0-GCCcore-12.3.0\n   ASE/3.22.1-gfbf-2022b                            GLPK/5.0-GCCcore-12.3.0                     (D)       libyaml/0.2.5-GCCcore-13.2.0                     (D)    pytest-rerunfailures/12.0-GCCcore-12.3.0\n   at-spi2-atk/2.38.0-GCCcore-12.2.0                GMP/6.2.1-GCCcore-12.2.0                              LittleCMS/2.14-GCCcore-12.2.0                           pytest-shard/0.1.2-GCCcore-12.3.0\n   at-spi2-atk/2.38.0-GCCcore-12.3.0         (D)    GMP/6.2.1-GCCcore-12.3.0                              LittleCMS/2.15-GCCcore-12.3.0                           Python-bundle-PyPI/2023.06-GCCcore-12.3.0\n   at-spi2-core/2.46.0-GCCcore-12.2.0               GMP/6.3.0-GCCcore-13.2.0                    (D)       LittleCMS/2.15-GCCcore-13.2.0                    (D)    Python-bundle-PyPI/2023.10-GCCcore-13.2.0     (D)\n   at-spi2-core/2.49.91-GCCcore-12.3.0       (D)    gmpy2/2.1.5-GCC-12.3.0                                LLVM/15.0.5-GCCcore-12.2.0                              python-casacore/3.5.2-foss-2023b\n   ATK/2.38.0-GCCcore-12.2.0                        gmpy2/2.1.5-GCC-13.2.0                      (D)       LLVM/16.0.6-GCCcore-12.3.0                              python-isal/1.1.0-GCCcore-12.3.0\n   ATK/2.38.0-GCCcore-12.3.0                 (D)    gnuplot/5.4.8-GCCcore-12.3.0                          LLVM/16.0.6-GCCcore-13.2.0                       (D)    Python/2.7.18-GCCcore-12.2.0-bare\n</code></pre></p>"},{"location":"user-guide/EasyBuild/#compiling-local-modules-from-eessi","title":"Compiling Local Modules from EESSI","text":"<p>Sometimes you will need to compile modules and software locally - Maybe your operating system is different, your GLIBC libraries are different and so on.</p> <p>To do this, you can invoke EasyBuild from within EESSI, and build to a local directory.</p> <p>You do this by creating an EasyBuild template configuration file, and pointing that configuration file to a shared directory and building versions of the software locally using EasyBuild Commands, issued below.</p>"},{"location":"user-guide/EasyBuild/#generating-an-easybuild-template-configuration-file","title":"Generating an EasyBuild template configuration file","text":"<p>Since EasyBuild v1.10, a command line option --confighelp is available that prints out the help text as an annotated configuration file. This can be used as an empty template configuration file:</p> <pre><code>mkdir -p $HOME/.config/easybuild\neb --confighelp &gt; $HOME/.config/easybuild/config.cfg\n$ head $HOME/.easybuild/config.cfg\n[MAIN]\n# Enable debug log mode (def False)\n#debug=\n# Enable info log mode (def False)\n#info=\n# Enable info quiet/warning mode (def False)\n#quiet=\n\n[basic]\n# Print build overview incl. dependencies (full paths) (def False)\n</code></pre>"},{"location":"user-guide/EasyBuild/#environment-variables","title":"Environment variables","text":"<p>All configuration settings listed as long options in <code>eb --help</code> can also be specified via <code>EASYBUILD_</code>-prefixed environment variables.</p> <p>Configuration settings specified this way always override the corresponding setting specified in a configuration file.</p> <p>For example, to enable debug logging using an environment variable: <pre><code>export EASYBUILD_DEBUG=1\n</code></pre> More examples of using environment variables to configure EasyBuild are shown in the sections below.</p> <p>Tip</p> <p>Any configuration option of EasyBuild which can be tuned by command line or via the configuration file, can also be tuned via a corresponding environment variable.</p> <p>Note</p> <p>If any <code>$EASYBUILD_</code>-prefixed environment variables are defined that do not correspond to a known configuration option, EasyBuild will report an error message and exit.</p>"},{"location":"user-guide/EasyBuild/#command-line-arguments","title":"Command line arguments","text":"<p>The configuration type with the highest precedence are the eb command line arguments, which override settings specified through environment variables or in configuration files.</p> <p>For some configuration options, both short and long command line arguments are available (see <code>eb --help</code>); the long options indicate how the configuration setting should be specified in a configuration file or via an environment variable (<code>$EASYBUILD_&lt;LONGOPTION&gt;</code>).</p> <p>For boolean configuration settings, both the <code>--&lt;option&gt;</code> and <code>--disable-&lt;option&gt;</code> variants are always available.</p>"},{"location":"user-guide/EasyBuild/#examples-more-below","title":"Examples (more below):","text":"<p>enable debug logging (long option) and logging to stdout (short option) <pre><code>eb --debug -l ...\n</code></pre> use /dev/shm as build path, install to temporary install path, disable debug logging <pre><code>eb --buildpath=/dev/shm --installpath=/tmp/$USER --disable-debug ...\n</code></pre></p>"},{"location":"user-guide/EasyBuild/#overview-of-current-configuration","title":"Overview of current configuration","text":"<p>(<code>--show-config</code>, <code>--show-full-config</code>)</p> <p>To get an overview of the current EasyBuild configuration across all configuration types, you can use <code>eb --show-config</code>.</p> <p>The output will specify:</p> <ul> <li>any configuration setting for which the current value is different from the default value</li> <li>a couple of selected important configuration settings (even if they are still set to the default value), i.e.:</li> <li>build path</li> <li>install path</li> <li>path to easyconfigs repository</li> <li>the robot search path</li> <li>source path</li> <li>through which configuration type each setting was defined</li> <li>i.e., default value, configuration file, environment variable or command line argument</li> </ul>"},{"location":"user-guide/EasyBuild/#example-output","title":"Example output","text":"<p><pre><code>$ cat $HOME/.config/easybuild/config.cfg\n[config]\nbuildpath = /tmp/eb-build\n\n$ export EASYBUILD_MODULES_TOOL=Lmod\n$ export EASYBUILD_OPTARCH=''\n\n$ eb --show-config --installpath=$HOME/apps --job-cores=4\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath      (F) = /tmp/eb-build\ninstallpath    (C) = /Users/example/apps\njob-cores      (C) = 4\nmodules-tool   (E) = Lmod\noptarch        (E) = ''\nrepositorypath (D) = /Users/example/.local/easybuild/ebfiles_repo\nrobot-paths    (D) = /Users/example/easybuild-easyconfigs/easybuild/easyconfigs\nsourcepath     (D) = /Users/example/.local/easybuild/sources\n</code></pre> For a full overview of the current configuration, including all configuration settings, see <code>eb --show-full-config</code>.</p>"},{"location":"user-guide/EasyBuild/#available-configuration-settings","title":"Available configuration settings","text":"<p>To obtain a full and up-to-date list of available configuration settings, see <code>eb --help</code>. We refrain from listing all available configuration settings here, to avoid outdated documentation.</p> <p>A couple of selected configuration settings are discussed below, in particular the mandatory settings.</p>"},{"location":"user-guide/EasyBuild/#mandatory-configuration-settings","title":"Mandatory configuration settings","text":"<p>A handful of configuration settings are mandatory, and should be provided using one of the supported configuration types.</p> <p>The following configuration settings are currently mandatory (more details in the sections below):</p> <ul> <li>Source path (<code>--sourcepath</code>)</li> <li>Build path (<code>--buildpath</code>)</li> <li>Software and modules install path (<code>--installpath</code>, <code>--installpath-software</code>, <code>--installpath-modules</code>)</li> <li>Easyconfigs repository (<code>--repository</code>, <code>--repositorypath</code>)</li> <li>Logfile format (<code>--logfile-format</code>)</li> </ul> <p>If any of these configuration settings is not provided in one way or another, EasyBuild will complain and exit.</p> <p>In practice, all of these have reasonable defaults (see <code>eb --help</code> for the default settings).</p> <p>Note</p> <p>The mandatory path-related options can be tweaked collectively via --prefix, see Overall prefix path (--prefix) for more information.</p>"},{"location":"user-guide/EasyBuild/#source-path-sourcepath","title":"Source path (--sourcepath)","text":"<p>default: $HOME/.local/easybuild/sources/ (determined via Overall prefix path (--prefix))</p> <p>The sourcepath configuration setting specifies the parent path of the directory in which EasyBuild looks for software source and install files.</p> <p>Looking for the files specified via the sources parameter in the .eb easyconfig file is done in the following order of preference:</p> <p>/: a subdirectory determined by the name of the software package //: in the style of the easyblocks/easyconfigs directories: in a subdirectory determined by the first letter (in lower case) of the software package and by its full name : directly in the source path Note that these locations are also used when EasyBuild looks for patch files in addition to the various easybuild/easyconfigs directories that are listed in the $PYTHONPATH. <p>You can specify multiple paths, separated with :, in which EasyBuild will look for sources, but only the first one will be used for downloading, so one needs to make sure at least the first path is writable by the user invoking eb.</p>"},{"location":"user-guide/EasyBuild/#build-path-buildpath","title":"Build path (<code>--buildpath</code>)","text":"<p>default: <code>$HOME/.local/easybuild/build/</code> (determined via Overall prefix path (<code>--prefix</code>))</p> <p>The buildpath configuration setting specifies the parent path of the (temporary) directories in which EasyBuild builds its software packages.</p> <p>Each software package is (by default) built in a subdirectory of the specified buildpath under //. <p>Note that the build directories are emptied and removed by EasyBuild when the installation is completed (by default).</p> <p>Tip</p> <p>Using /dev/shm as build path can significantly speed up builds, if it is available and provides a sufficient amount of space. Setting up the variable EASYBUILD_BUILDPATH in your shell startup files makes this default. However be aware that, fi., two parallel GCC builds may fill up /dev/shm !</p>"},{"location":"user-guide/EasyBuild/#software-and-modules-install-path","title":"Software and modules install path","text":"<p>(<code>--installpath</code>, <code>--installpath-software</code>, <code>--installpath-modules</code>)</p> <p>defaults:</p> <ul> <li>software install path: <code>$HOME/.local/easybuild/software</code> (determined via Overall prefix path (<code>--prefix</code>) and <code>--subdir-software</code>)</li> <li>modules install path: <code>$HOME/.local/easybuild/modules/all</code> (determined via Overall prefix path (<code>--prefix</code>), <code>--subdir-modules</code> and <code>--suffix-modules-path</code>)</li> </ul> <p>There are several ways in which the software and modules install path used by EasyBuild can be configured:</p> <p>using the direct configuration options <code>--installpath-software</code> and <code>--installpath-modules</code> (see below) via the parent install path configuration option <code>--installpath</code> (see below) via the overall prefix path configuration option <code>--prefix</code> (see Overall prefix path (<code>--prefix</code>))</p>"},{"location":"user-guide/EasyBuild/#direct-options","title":"DIRECT OPTIONS","text":"<p>(<code>--installpath-software</code> and <code>--installpath-modules</code>)</p> <p>default: (no default specified)</p> <p>The <code>--installpath-software</code> and <code>--installpath-modules</code> configuration options (available since EasyBuild v2.1.0) allow to directly specify the software and modules install paths, respectively.</p> <p>These configuration options have precedence over all of the other configuration options that relate to specifying the install path for software and/or modules (see below).</p>"},{"location":"user-guide/EasyBuild/#parent-install-path-installpath","title":"PARENT INSTALL PATH: <code>--INSTALLPATH</code>","text":"<p>default: (no default specified)</p> <p>The <code>--installpath</code> configuration option specifies the parent path of the directories in which EasyBuild should install software packages and the corresponding module files.</p> <p>The install path for software and modules specifically is determined by combining <code>--installpath</code> with <code>--subdir-software</code>, and combining <code>--installpath</code> with <code>--subdir-modules</code> and <code>--suffix-modules-path</code>, respectively.</p> <p>For more information on these companion configuration options, see Software and modules install path subdirectories (<code>--subdir-software</code>, <code>--subdir-modules</code>, <code>--suffix-modules-path</code>).</p>"},{"location":"user-guide/EasyBuild/#full-install-path-for-software-and-module-file","title":"FULL INSTALL PATH FOR SOFTWARE AND MODULE FILE","text":"<p>The full software and module install paths for a particular software package are determined by the active module naming scheme along with the general software and modules install paths specified by the EasyBuild configuration.</p> <p>Both the software itself and the corresponding module file will be installed in a subdirectory of the corresponding install path named according to the active module naming scheme (default format: /-). Additionally, symlinks to the actual module file are installed in a subdirectory of the modules install path named according to the value of the moduleclass easyconfig parameter. <p>For more information on the module naming scheme used by EasyBuild, see Active module naming scheme (<code>--module-naming-scheme</code>).</p>"},{"location":"user-guide/EasyBuild/#updating-modulepath","title":"UPDATING $MODULEPATH\u00b6","text":"<p>To make the modules generated by EasyBuild available, the <code>$MODULEPATH</code> environment variable must be updated to include the modules install path.</p> <p>The recommended way to do this is to use the module use command. For example: <pre><code>eb --installpath=$HOME/easybuild\nmodule use $HOME/easybuild/modules/all\n</code></pre> It is probably a good idea to add this to your (favourite) shell .rc file, e.g., <code>~/.bashrc</code>, and/or the <code>~/.profile</code> login scripts, so you do not need to adjust <code>$MODULEPATH</code> every time you start a new session.</p> <p>Note</p> <p>Updating <code>$MODULEPATH</code> is not required for EasyBuild itself, since eb updates <code>$MODULEPATH</code> itself at runtime according to the modules install path it is configured with.</p>"},{"location":"user-guide/EasyBuild/#easyconfigs-repository-repository-repositorypath","title":"Easyconfigs repository (<code>--repository</code>, <code>--repositorypath</code>)","text":"<p>default: FileRepository at $HOME/.local/easybuild/ebfiles_repo (determined via Overall prefix path (--prefix))</p> <p>EasyBuild has support for archiving (tested) .eb easyconfig files. After successfully installing a software package using EasyBuild, the corresponding .eb file is uploaded to a repository defined by the repository and repositorypath configuration settings.</p> <p>Currently, EasyBuild supports the following repository types (see also eb --avail-repositories):</p> <ul> <li>FileRepository('path', 'subdir'): a plain flat file repository; path is the path where files will be stored, subdir is an optional subdirectory of that path where the files should be stored</li> <li>GitRepository('path', 'subdir/in/repo': a non-empty bare git repository (created with <code>git init --bare</code> or <code>git clone --bare</code>); path is the path to the git repository (can also be a URL); subdir/in/repo is optional, and specifies a subdirectory of the repository where files should be stored in</li> <li>SvnRepository('path', 'subdir/in/repo'): an SVN repository; path contains the subversion repository location (directory or URL), the optional second value specifies a subdirectory in the repository You need to set the repository setting inside a configuration file like this: <pre><code>[config]\nrepository = FileRepository\nrepositorypath = &lt;path&gt;\n</code></pre> Or, optionally an extra argument representing a subdirectory can be specified, e.g.: <pre><code>export EASYBUILD_REPOSITORY=GitRepository\nexport EASYBUILD_REPOSITORYPATH=&lt;path&gt;,&lt;subdir&gt;\n</code></pre> You do not have to worry about importing these classes, EasyBuild will make them available to the configuration file.</li> </ul> <p>Using git requires the GitPython Python modules, using svn requires the pysvn Python module (see Dependencies).</p> <p>If access to the easyconfigs repository fails for some reason (e.g., no network or a missing required Python module), EasyBuild will issue a warning. The software package will still be installed, but the (successful) easyconfig will not be automatically added to the archive (i.e., it is not considered a fatal error).</p>"},{"location":"user-guide/EasyBuild/#logfile-format-logfile-format","title":"Logfile format (<code>--logfile-format</code>)","text":"<p>default: easybuild, easybuild-%(name)s-%(version)s-%(date)s.%(time)s.log</p> <p>The logfile format configuration setting contains a tuple specifying a log directory name and a template log file name. In both of these values, using the following string templates is supported:</p> <ul> <li>%(name)s: the name of the software package to install</li> <li>%(version)s: the version of the software package to install</li> <li>%(date)s: the date on which the installation was performed (in YYYYMMDD format, e.g. 20120324)</li> <li>%(time)s: the time at which the installation was started (in HHMMSS format, e.g. 214359)</li> </ul> <p>Note</p> <p>Because templating is supported in configuration files themselves (see Templates and constants supported in configuration files), the '%' character in these template values must be escaped when used in a configuration file (and only then), e.g., '%%(name)s'. Without escaping, an error like InterpolationMissingOptionError: Bad value substitution will be thrown by ConfigParser.</p> <p>For example, configuring EasyBuild to generate a log file mentioning only the software name in a directory named easybuild can be done via the --logfile-format command line option: <pre><code>eb --logfile-format=\"easybuild,easybuild-%(name)s.log\" ...\n</code></pre> or the <code>$EASYBUILD_LOGFILE_FORMAT</code> environment variable:</p> <p><pre><code>export EASYBUILD_LOGFILE_FORMAT=\"easybuild,easybuild-%(name)s.log\"\n</code></pre> or by including the following in an EasyBuild configuration file (note the use of '%%' to escape the name template value here): <pre><code>logfile-format = easybuild,easybuild-%%(name)s.log\n</code></pre></p>"},{"location":"user-guide/EasyBuild/#optional-configuration-settings","title":"Optional configuration settings","text":"<p>The subsections below discuss a couple of commonly used optional configuration settings.</p>"},{"location":"user-guide/EasyBuild/#overall-prefix-path-prefix","title":"Overall prefix path (<code>--prefix</code>)","text":"<p>default: $HOME/.local/easybuild</p> <p>The overall prefix path used by EasyBuild can be specified using the --prefix configuration option.</p> <p>This affects the default value of several configuration options:</p> <ul> <li>source path</li> <li>build path</li> <li>software and modules install path</li> <li>easyconfigs repository path</li> <li>package path</li> <li>container path</li> </ul>"},{"location":"user-guide/EasyBuild/#software-and-modules-install-path-subdirectories","title":"Software and modules install path subdirectories","text":"<p>(<code>--subdir-software</code>, <code>--subdir-modules</code>, <code>--suffix-modules-path</code>)</p> <p>defaults:</p> <ul> <li>software install path subdirectory (<code>--subdir-software</code>): software</li> <li>modules install path subdirectory (<code>--subdir-modules</code>): modules</li> <li>modules install path suffix (<code>--suffix-modules-path</code>): all</li> </ul> <p>The subdirectories for the software and modules install paths (relative to --installpath, see install path) can be specified using the corresponding dedicated configuration options (available since EasyBuild v1.14.0).</p> <p>For example:</p> <pre><code>export EASYBUILD_SUBDIR_SOFTWARE=installs\neb --installpath=$HOME/easybuild --subdir-modules=module_files ...\n</code></pre>"},{"location":"user-guide/EasyBuild/#modules-tool-modules-tool","title":"Modules tool (<code>--modules-tool</code>)","text":"<p>default: Lmod</p> <p>Specifying the modules tool that should be used by EasyBuild can be done using the modules-tool configuration setting. A list of supported modules tools can be obtained using eb --avail-modules-tools.</p> <p>Currently, the following modules tools are supported:</p> <ul> <li>Lmod (default): Lmod, an modern alternative to environment modules, written in Lua (lmod)</li> <li>EnvironmentModules: modern Tcl-only version of environment modules (4.x) (modulecmd.tcl)</li> <li>EnvironmentModulesC: Tcl/C version of environment modules, usually version 3.2.10 (modulecmd)</li> <li>EnvironmentModulesTcl: (ancient) Tcl-only version of environment modules (modulecmd.tcl)</li> </ul> <p>You can determine which modules tool you are using by checking the output of type -f module (in a bash shell), or alias module (in a tcsh shell).</p> <p>The actual module command (i.e., modulecmd, modulecmd.tcl, lmod, ...) must be available via $PATH (which is not standard), except when using Lmod (in that case the lmod binary can also be located via $LMOD_CMD) or when using Environment Modules (in that case the modulecmd.tcl binary can also be located via $MODULES_CMD).</p> <p>For example, to indicate that EasyBuild should be using Lmod as modules tool: <pre><code>eb --modules-tool=Lmod ...\n</code></pre></p>"},{"location":"user-guide/EasyBuild/#active-module-naming-scheme-module-naming-scheme","title":"Active module naming scheme (<code>--module-naming-scheme</code>)","text":"<p>default: EasyBuildModuleNamingScheme</p> <p>The module naming scheme that should be used by EasyBuild can be specified using the module-naming-scheme configuration setting. <pre><code>eb --module-naming-scheme=HierarchicalMNS ...\n</code></pre> For more details, see the dedicated page on using a custom module naming scheme.</p>"},{"location":"user-guide/EasyBuild/#module-files-syntax-module-syntax","title":"Module files syntax (<code>--module-syntax</code>)","text":"<p>default: Lua</p> <p>supported since: EasyBuild v2.1</p> <p>The syntax to use for generated module files can be specified using the --module-syntax configuration setting.</p> <p>Possible values are:</p> <ul> <li>Lua: generate module files in Lua syntax: this requires the use of Lmod as a modules tool to consume the module files (see modules tool) module file names will have the .lua extension</li> <li>Tcl: generate module files in Tcl syntax: Tcl module files can be consumed by all supported modules tools. Module files will contain a header string #%Module indicating that they are composed in Tcl syntax</li> </ul> <p>Note</p> <p>Lmod is able to deal with having module files in place in both Tcl and Lua syntax. When a module file in Lua syntax (i.e., with a .lua file name extension) is available, a Tcl module file with the same name will be ignored. The Tcl-based environment modules tool will simply ignore module files in Lua syntax, since they do not contain the header string that is included in Tcl module files.</p> <p>Note</p> <p>Using module files in Lua syntax has the advantage that Lmod does not need to translate from Lua to Tcl internally when processing the module files, which benefits responsiveness of Lmod when used interactively by users. In terms of Lmod-specific aspects of module files, the syntax of the module file does not matter; Lmod-specific statements supported by EasyBuild can be included in Tcl module files as well, by guarding them by a condition that only evaluates positively when Lmod is consuming the module file, i.e. 'if { [ string match \"*tcl2lua.tcl\" $env(_) ] } { ... }'. Only conditional load statements like 'load(atleast(\"gcc\",\"4.8\"))' can only be used in Lua module files.</p>"},{"location":"user-guide/onboarding/","title":"Onboarding","text":""},{"location":"user-guide/onboarding/#create-user","title":"Create user","text":"<p>Ask an admin to create your account, provide the following information:</p> <ul> <li> Full name</li> <li> Select a username</li> <li> Email address</li> </ul>"},{"location":"user-guide/onboarding/#install-companion-apps","title":"Install companion apps","text":"<p>For all users:</p> <ul> <li> Password manager</li> <li> Matrix chat client (optional, you can use the web version)</li> </ul> <p>For technical users:</p> <ul> <li> Docker</li> <li> Lens (optional, you can use the included <code>kubectl</code> or <code>k9s</code> command in the tools container)</li> </ul>"},{"location":"user-guide/onboarding/#appendix","title":"Appendix","text":""},{"location":"user-guide/onboarding/#recommended-password-managers","title":"Recommended password managers","text":"<ul> <li>Bitwarden (easy to use, but requires an online account)</li> <li>KeePassXC (completely offline, but you'll need to sync manually)</li> </ul>"}]}